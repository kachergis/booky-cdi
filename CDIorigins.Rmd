---
title: "Where do CDI words come from?"
author: "George & Georgia"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
require(tidytext)
```


## Goal

Can we identify words on the CDI that are more bookish, speechy, or from TV? (Controlling for difficulty, as more bookish words are probably more difficult)

Using this distributional source information, can we find features of children (e.g., mother's education) that relate to knowledge of subsets of words (e.g. bookish words)?

To start, we will look at American English, before extending to British English and French.
We will begin with unlemmatized corpora.


```{r word-cdi-definitions, echo=F, message=F, warning=F}
# move to separate file?
bigram_definitions = c("a lot","all gone", "buttocks/bottom*", "yum yum", "turn around", "uh oh", "french fries", "gonna/going to", "night night", "hafta/have to", "green beans", "high chair", "ice cream", "inside/in", "lemme/let me", "next to", "on top of", "peanut butter", "quack quack", "rocking chair", "shh/shush/hush", "teddybear", "thank you", "wanna/want to", "woof woof")
bigrams_to_search = c("a lot", "all gone", "butt", "yum", "turn around", "uh oh", "french fries", "going to", "good night", "have to", "green beans", "high chair", "ice cream", "inside", "let me", "next to", "on top of", "peanut butter", "quack", "rocking chair", "hush", "teddy bear", "thank you", "want to", "woof")
```


```{r load-corpus-data, echo=F, message=F, warning=F}
cdi_items <- readRDS("data/CDI-WS-American-EN.rds")
adult_books <- readRDS("data/Google_books_frequencies.rds")

# ToDo: remove book-reading corpora from CHILDES data!
# CHILDES corpus (hapaxes already removed, but 6295 word_count==2)
child_speech <- read_csv(here("data/childes_english_word_freq_cleaned_noHapaxes.csv")) %>% 
  select(-X1) %>%
  mutate(source = "CHILDES") %>%
#         word = tolower(word)) %>% 
#  mutate(word = ifelse(word=="i", "I", ifelse(word=="i'll", "I'll", word))) %>%
  arrange(desc(word_count)) %>%
  mutate(word = ifelse(word=="Play_Doh", "play dough", word)) %>%
  mutate(rank = 1:n()) %>%
  left_join(cdi_items) %>%
  mutate(on_cdi = ifelse(!is.na(definition), 1, 0)) # %>%
#  mutate(Ncapitals = str_count(word, "[A-Z]"))
# names are capitalized -- remove them (but also: "I", "I'll", "I've" etc)

# Play_Doh is in CHILDES 268 times (word_count_norm=35.1)

# SUBTLEX-US subtitle word frequencies (adult speech standard), min freq = 2
adult_speech <- read_csv(here("data/SUBTLEX-US.csv")) %>% # 50.3 million words
  rename(word_count_norm = SUBTLWF, # frequency per million words
         word_count = SLUSfreq) %>%
  mutate(word = tolower(word)) %>%
  group_by(word) %>%
  summarise(word_count = sum(word_count)) %>%
  mutate(source = "SUBTLEX",
         word_count_norm = word_count * (1e6 / sum(word_count)), # count per million tokens
         prob = word_count / sum(word_count)) %>%
  mutate(word = ifelse(word=="i", "I", word)) %>%
  arrange(desc(word_count_norm)) %>%
  mutate(rank = 1:n()) %>%
  select(word, word_count, word_count_norm, prob, source, rank) %>%
  left_join(cdi_items) %>%
  mutate(on_cdi = ifelse(!is.na(definition), 1, 0)) %>% # 655 CDI words
  filter(!is.element(word, c("s","t","m","re","ll","d"))) # need to re-parse subtitles and keep apostrophized words...

# SUBTLEX is missing all of the apostrophized words...let's use the Charlesworth corpus instead
load("data/Charlesworth-unlemmatized-counts.Rdata") 

adult_dat <- adult_dat %>% 
  left_join(cdi_items) %>%
  mutate(on_cdi = ifelse(!is.na(definition), 1, 0)) 
table(adult_dat$on_cdi, adult_dat$source) 
# 646 CDI words in books, 627 in speech, 643 on TV

child_dat <- child_dat %>% 
  left_join(cdi_items) %>%
  mutate(on_cdi = ifelse(!is.na(definition), 1, 0)) 
table(child_dat$on_cdi, child_dat$source)
# 604 in books, 662 in speech, 659 on TV

# min freq = 1 (2548, but mostly real words)
Montag_counts <- read.csv(here("data/Montag-book-corpus/100out.txt"), sep=' ') %>%
  mutate(word = ifelse(word=="i", "I", word)) %>%
  filter(word!="â€“") %>%
  bind_rows(tibble(word=bigrams_to_search, 
                   word_count=c(21, 1, 4, 5, 1, 1, 1, 63, 59, 47, 1, 1, 9, 1003, 12, 12, 9, 4, 28, 1, 7, 3, 36, 65, 17))) %>%
  left_join(cdi_items) 
# e.g. "night night" = "goodnight" + "good night" (no actual "night night"s)

child_books <- Montag_counts %>% 
  mutate(word_count_norm = word_count * (1e6 / sum(word_count)), # count per million tokens
         prob = word_count / sum(word_count),
         source = "Montag"
  ) %>%
  arrange(desc(word_count)) %>%
  mutate(rank = 1:n(),
         on_cdi = ifelse(!is.na(definition), 1, 0)) 
# starred items not all counted in Montag, e.g. "mother"

# attempt to read in bigrams from raw text..
#tmp <- data_frame(read_file(here("data/Montag-book-corpus/100Books.txt")))
#tmp <- tokenize(read_file(here("data/Montag-book-corpus/100Books.txt")))
#Montag_bigrams <- data_frame(unlist(tmp)) %>%
#  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
#  count(bigram, sort = TRUE)

adult_aoas <- read_csv("data/AoA_51715_words.csv") %>% 
  rename(word = Word, word_freq_norm=Freq_pm) %>%
  mutate(lexical_class = case_when(Dom_PoS_SUBTLEX=="Noun" ~ "nouns",
                                   Dom_PoS_SUBTLEX=="Verb" ~ "verbs",
                                   Dom_PoS_SUBTLEX=="Adjective" ~ "adjectives",
                                   Dom_PoS_SUBTLEX=="Name" ~ "other", 
                                   is.element(Dom_PoS_SUBTLEX, 
                                              c("Adverb", "Article", "Determiner", "Conjunction", 
                                                "Preposition", "Pronoun")) ~ "function_words",
                                   TRUE ~ "bad")) %>%
  filter(lexical_class != "bad") 
# ToDo: a small number of values in Dom_PoS_SUBTLEX are numeric...looks like copy/paste errors from adjacent cells

```

Our data sources:

- Adult speech: `r nrow(adult_speech)` types, `r sum(adult_speech$word_count)` tokens from the SUBTLEX corpus (`r sum(adult_speech$on_cdi)` CDI words)
- Children speech: `r nrow(child_speech)` types, `r sum(child_speech$word_count)` tokens from the CHILDES corpus (`r sum(child_speech$on_cdi)` CDI words)
- Adult books: `r nrow(adult_books)` types, `r sum(adult_books$word_count)` tokens from the Google books corpus
- Children's books: `r nrow(child_books)` types, `r sum(child_books$word_count)` tokens from the CHILDES corpus (`r sum(child_books$on_cdi)` CDI words)


## Normalize Word Frequencies

We will calculate keyness scores for each word, i.e. the ratio of normalized frequency in focus corpus to normalized frequency in a reference corpus.
For now, we will use the subset of words that are found in all four corpora (N=4229), but we may consider following Dawson et al. (2021) and adding a constant (e.g., 10) to all normalized frequencies in every corpus in order to not eliminate the bulk of words that do not appear in the smaller corpora (which are the child-directed corpora).
<!-- "Given the problem of calculating ratios for words occurring in the focus corpus, but not at all in the reference corpus, we added a constant of 10 to all normalised frequencies before calculating keyness. We selected this value as the constant because it focuses the keyword analysis on the lower end of the frequency spectrum (Kilgarriff, 2009), which we considered to be important when identifying the words that children were unlikely to encounter in everyday conversation, but which they would experience through regular exposure to book language." --> 


### Child-directed Speech vs. Books

```{r keyness-childes-vs-montag, echo=F}
# CHILDES and Montag books
ch_freq_long <- rbind(child_speech, child_books) %>%
  select(word, definition, source, on_cdi, word_count_norm, word_count, lexical_class) %>%
  filter(!is.element(word, c("d","m","s","t", "xxx", "ve", "ll", "n't", "rrb", "unc", "re"))) 
# equivalent to child_dat, which is based on Charlesworth corpora

# wide - no smoothing, only keep subset of 4611 words in all corpora
ch_freq <- ch_freq_long %>% select(-word_count) %>%
  group_by(word, definition, source, on_cdi, lexical_class) %>%
  summarise(word_count_norm = sum(word_count_norm)) %>%
  pivot_wider(id_cols = c(word,definition,on_cdi,lexical_class), names_from = source, values_from = word_count_norm) %>%
  filter(!is.na(Montag), !is.na(CHILDES)) %>%
  mutate(ch_book_vs_speech = Montag / CHILDES,
         prop_booky = Montag / (Montag + CHILDES)) 

# Laplace smoothing, alpha=10, keep all 123060 words in any corpus
ch_freq_smooth <- ch_freq_long %>% 
  group_by(word, definition, source, on_cdi, lexical_class) %>%
  summarise(word_count_norm = sum(word_count_norm)) %>%
  mutate(word_count_norm_smooth = word_count_norm + 10) %>%
  pivot_wider(id_cols = c(word, definition, on_cdi, lexical_class), names_from = source, values_from = word_count_norm_smooth, values_fill=10) %>% 
  filter(!is.na(Montag), !is.na(CHILDES)) %>%
  mutate(ch_book_vs_speech = Montag / CHILDES,
         prop_booky = Montag / (Montag + CHILDES))
```

```{r keyness-charlesworth-corpora, echo=F}
# wide - no smoothing, only keep subset of 12855 words in all corpora
ch_freq_charles <- child_dat %>% select(-word_count) %>%
  group_by(word, definition, source, on_cdi, lexical_class) %>%
  summarise(word_count_norm = sum(word_count_norm)) %>%
  pivot_wider(id_cols = c(word, definition, on_cdi, lexical_class), names_from = source, values_from = word_count_norm) %>%
  filter(!is.na(books), !is.na(speech), !is.na(tv)) %>%
  mutate(ch_book_vs_speech = books / speech,
         prop_booky = books / (books + speech)) 

# Laplace smoothing, alpha=10, keep all 124036 words in any corpus
ch_freq_smooth_charles <- child_dat %>% 
  group_by(word, definition, source, on_cdi, lexical_class) %>%
  summarise(word_count_norm = sum(word_count_norm)) %>%
  mutate(word_count_norm_smooth = word_count_norm + 10) %>%
  pivot_wider(id_cols = c(word, definition, on_cdi, lexical_class), names_from = source, values_from = word_count_norm_smooth, values_fill=10) %>% 
  filter(!is.na(books), !is.na(speech), !is.na(tv)) %>%
  mutate(ch_book_vs_speech = books / speech,
         prop_booky = books / (books + speech))
```

### Most Child Booky Words

```{r}
#ch_freq %>% arrange(desc(ch_book_vs_speech)) %>% head(10) %>% kable(digits=2)
#ch_freq_smooth %>% arrange(desc(ch_book_vs_speech)) %>% head(10) %>% kable(digits=2)

p1 <- ch_freq %>% mutate(Word = ifelse(on_cdi==1, "CDI", "Non-CDI")) %>%
  ggplot(aes(x=CHILDES, y=Montag, color=Word)) + 
  geom_point(alpha=.3) + theme_classic() + geom_smooth(method='lm') +
  scale_x_log10() + scale_y_log10() + ggtitle("Raw Frequencies (intersection)")

p2 <- ch_freq_smooth %>% mutate(Word = ifelse(on_cdi==1, "CDI", "Non-CDI")) %>%
  ggplot(aes(x=CHILDES, y=Montag, color=Word)) + 
  geom_point(alpha=.3) + theme_classic() + geom_smooth(method='lm') +
  scale_x_log10() + scale_y_log10() + ggtitle("Smoothed Frequencies (union)")

ggpubr::ggarrange(p1, p2, nrow=1, common.legend = T)
```


```{r}
cor(subset(ch_freq, on_cdi==1)$CHILDES, subset(ch_freq, on_cdi==1)$Montag) # .76
cor(subset(ch_freq, on_cdi==0)$CHILDES, subset(ch_freq, on_cdi==0)$Montag) # .58
```

Same, but with Charlesworth corpora

```{r}
p1 <- ch_freq_charles %>% mutate(Word = ifelse(on_cdi==1, "CDI", "Non-CDI")) %>%
  ggplot(aes(x=speech, y=books, color=Word)) + 
  geom_point(alpha=.1) + theme_classic() + geom_smooth(method='lm') +
  scale_x_log10() + scale_y_log10() + ggtitle("Raw Frequencies (intersection)")

p2 <- ch_freq_smooth_charles %>% mutate(Word = ifelse(on_cdi==1, "CDI", "Non-CDI")) %>%
  ggplot(aes(x=speech, y=books, color=Word)) + 
  geom_point(alpha=.1) + theme_classic() + geom_smooth(method='lm') +
  scale_x_log10() + scale_y_log10() + ggtitle("Smoothed Frequencies (union)")

ggpubr::ggarrange(p1, p2, nrow=1, common.legend = T)
```


### Which CDI words are more booky/speechy?

```{r, fig.width=10, fig.height=5}
p1 <- ch_freq_smooth %>% filter(on_cdi==1) %>%
  ggplot(aes(x=CHILDES, y=Montag, color=lexical_class)) + 
  geom_point(alpha=.3) + theme_classic() + 
  scale_x_log10() + scale_y_log10() +
  geom_text_repel(aes(label=word)) +
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("CHILDES vs. Montag book corpus")
  #geom_text(data=subset(ch_freq_smooth, on_cdi==1 & (wt > 4 | mpg > 25)), aes(label=name))

p2 <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>%
  ggplot(aes(x=speech, y=books, color=lexical_class)) + 
  geom_point(alpha=.3) + theme_classic() + 
  scale_x_log10() + scale_y_log10() +
  geom_text_repel(aes(label=word)) + xlab("CHILDES") + 
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("CHILDES vs. FB Children's Books")

p3 <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>%
  ggplot(aes(x=speech, y=tv, color=lexical_class)) + 
  geom_point(alpha=.3) + theme_classic() + 
  scale_x_log10() + scale_y_log10() + xlab("CHILDES") + 
  geom_text_repel(aes(label=word)) + ylab("Children's TV and Movies") + 
  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
  ggtitle("CHILDES vs. Children's Movies")

# merge in lexical_class and color points by it?

ggpubr::ggarrange(p1, p2, p3, nrow=1, common.legend = T)
#ggsave("CHILDES_smoothed_norm_freqs_vs_books_TV.pdf", width=16, height=9)
```



```{r}
ch_freq <- ch_freq %>% 
  left_join(adult_aoas %>% select(word, lexical_class, Nletters), by="word") %>% # AoA_Kup_lem
  rename(lexical_class = lexical_class.x) %>%
  mutate(lexical_class = ifelse(is.na(lexical_class), lexical_class.y, lexical_class)) %>%
  select(-lexical_class.y)
# ToDo: look at disagreements on lexical class
  
ch_freq_smooth <- ch_freq_smooth %>% 
  left_join(adult_aoas %>% select(word, lexical_class, Nletters), by="word") %>%
  rename(lexical_class = lexical_class.x) %>%
  mutate(lexical_class = ifelse(is.na(lexical_class), lexical_class.y, lexical_class)) %>%
  select(-lexical_class.y)

# holds even if we filter out many low frequency words
ch_key <- ch_freq %>% #filter(CHILDES>1, Montag>1) %>%
  group_by(on_cdi, lexical_class) %>%
  tidyboot_mean(ch_book_vs_speech)

ch_key_smooth <- ch_freq_smooth %>% 
  group_by(on_cdi, lexical_class) %>%
  tidyboot_mean(ch_book_vs_speech)

p1 <- ch_key %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean, color=lexical_class)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper), position = position_jitter(w=0.2)) +
  theme_classic() + ylab("Children's Book Keyness") + xlab("")

p2 <- ch_key_smooth %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean, color=lexical_class)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper), position = position_jitter(w=0.2)) +
  theme_classic() + ylab("Children's Book Keyness") + xlab("")

ggpubr::ggarrange(p1, p2, nrow=1, common.legend = T)
```

```{r}
ch_booky <- ch_freq %>% group_by(on_cdi, lexical_class) %>%
  tidyboot_mean(prop_booky)

ch_booky_smooth <- ch_freq_smooth %>% group_by(on_cdi, lexical_class) %>%
  tidyboot_mean(prop_booky)

p1 <- ch_booky %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean, color=lexical_class)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper), position = position_jitter(w=0.2)) +
  theme_classic() + ylab("Proportion of Book Occurrences") + xlab("")

p2 <- ch_booky_smooth %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean, color=lexical_class)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper), position = position_jitter(w=0.2)) +
  theme_classic() + ylab("Proportion of Book Occurrences") + xlab("")

ggpubr::ggarrange(p1, p2, nrow=1, common.legend = T)
```

## Booky vs. Speechy vs. TV Words

Table based on Charlesworth corpora

```{r}
ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) %>%
  DT::datatable(options=list(columnDefs = list(list(visible=FALSE, targets=c(1,3))))) %>%
  DT::formatRound(columns=c('books','tv','speech'), digits=0) %>%
  DT::formatPercentage(columns=c('prop_booky'), digits=0)
```


## Predicting AoA

Which corpus best predicts words' AoA?
Which corpora contribute unique variance to predicting AoA?
(Does the book corpus interact with SES, since we expect high-SES parents read more to their children?)

```{r}
dd <- ch_freq_smooth %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

# let's drop prop_booky..
dd_novels <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

ddd <- dd %>% select(-prop_booky) %>%
  left_join(dd_novels %>% select(-prop_booky)) %>%
  mutate(Nletters = ifelse(is.na(Nletters), nchar(word), Nletters))

# Montag book freq vs. Facebook children's books (more novels)
#cor.test(dd$Montag, dd$books) # .98
#cor.test(dd$prop_booky, dd_novels$prop_booky) # uncorrelated!


#### add aoas
#walk(list.files(".", pattern = "aoa_functions.R$", full.names = TRUE), source)
#wb_data <- load_wb_data("English (American)")
#aoas <- fit_aoas(wb_data)
#saveRDS(aoas, file="data/english_(american)_aoa_bydefinition.rds")
aoas <- readRDS("data/english_(american)_aoa_bydefinition.rds") %>%
  filter(measure=="produces")

#aoas <- aoas %>% left_join(cdi_items)
#aoas[which(is.na(aoas$lexical_class)),]
# inside and in don't have category/LC defined - go back and fix

# doesn't have category...left_join(cdi_items) if wanted
ddd <- ddd %>% left_join(aoas %>% select(-measure)) %>%
  left_join(adult_books %>% 
              select(word, word_count_norm) %>% 
              rename(adult_books = word_count_norm))
# add Google book frequency to df -- use this as "base rate"

summary(ddd)


#summary(lm(mean_produces ~ prop_booky, data=ddd)) # r^2 = .21; more booky = harder

summary(lm(aoa ~ log(tv), data=ddd)) # .008
summary(lm(aoa ~ log(CHILDES), data=ddd)) # .002 CHILDES (Eva/George cleaned)
summary(lm(aoa ~ log(speech), data=ddd)) # .004 CHILDES (Charlesworth cleaned)
summary(lm(aoa ~ log(Montag), data=ddd)) # .003 Montag 
summary(lm(aoa ~ log(books), data=ddd)) # .054 Charlesworth
summary(lm(aoa ~ log(adult_books), data=ddd)) # .069 Google books

ddd$books = scale(log(ddd$books), center=F)[,1]
ddd$tv = scale(log(ddd$tv), center=F)[,1]
ddd$speech = scale(log(ddd$speech), center=F)[,1]

# we should tack on adult_books
cor.test(ddd$books, ddd$speech) # .73
cor.test(ddd$tv, ddd$speech) # .83
cor.test(ddd$tv, ddd$books) # .94
# Books and TV are most correlated; books and speech least correlated

# correlations change if we scale and log the frequencies...
cor.test(ddd$books, ddd$speech) # .81
cor.test(ddd$tv, ddd$speech) # .86
cor.test(ddd$tv, ddd$books) # .87

# with all 3 sources, only speech and books are significant
summary(lm(mean_produces ~ books + speech + tv, data=ddd)) # 

summary(lm(mean_produces ~ books + tv + speech, data=ddd))
summary(lm(mean_produces ~ books + speech, data=ddd))
```


## Predict Booky Vocabulary per child by Mother Education

(also try ses_group)

```{r}
# let's take the 200 bookiest CDI items and look at relation of subscore to mom_ed
booky_items <- itdat %>% arrange(desc(prop_booky)) %>% head(200)

samp <- d_demo %>% filter(!is.na(mom_ed)) # 2773

booky_scores <- d_en_ws %>% filter(is.element(definition, booky_items$definition)) %>%
  group_by(data_id) %>% summarise(booky_production = sum(produces))

# most children get 0 booky words (they're hard)
# hist(booky_scores$booky_production)
samp <- left_join(samp, booky_scores)

#AoA ~ book_freq * mom_ed + spoken_freq * mom_ed

summary(lm(production ~ age, data=samp)) # .465
summary(lm(production ~ age + mom_ed, data=samp)) # r^2 = .477
#summary(lm(production/age ~ mom_ed, data=samp)) 

summary(lm(booky_production ~ age , data=samp)) # r^2 = .452
summary(lm(booky_production ~ age + mom_ed, data=samp)) # .462

samp %>% 
  mutate(nonbooky_production = production - booky_production) %>%
  ggplot(aes(x=booky_production, y=nonbooky_production, color=mom_ed, shape=mom_ed)) +
  geom_point(alpha=.3) + theme_classic() + geom_smooth()

d_age <- d_en_ws %>% left_join(d_demo %>% select(data_id, age)) %>%
  filter(!is.na(age)) %>%
  group_by(definition, category, lexical_class, age) %>%
  summarise(mean_produces = mean(produces, na.rm=T))

d_age %>% ggplot(aes(x=age, y=mean_produces, group=definition, color=lexical_class)) + 
  geom_line(alpha=.3) + theme_classic()
#d50pct <- d_age %>% filter(mean_produces < .55, mean_produces > .45)
# ToDo: get AoA model fits
```


## Per-child, per-item mixed effects

produces ~ book_freq * mom_ed + speech_freq * mom_ed + (1|item) + (1|child)
produces ~ lexical_class * book_freq * mom_ed + lexical_class * speech_freq * mom_ed + (1|item) + (1|child)


```{r}
d_samp <- d_en_ws %>% filter(is.element(data_id, samp$data_id)) %>%
  left_join(samp %>% select(data_id, age, production, mom_ed, ses_group)) %>% # vocab? booky_production
  left_join(itdat %>% select(definition, lexical_class, books, tv, speech, prop_booky, adult_books))

d_samp <- d_samp %>% mutate(age_sc = scale(age, center=T, scale=F), 
                            books_sc = scale(log(books)),
                            tv_sc = scale(log(tv)),
                            speech_sc = scale(log(speech)),
                            prop_booky_sc = scale(log(prop_booky)),
                            mom_ed_sc = scale(as.numeric(mom_ed), center=T, scale=F))
```

```{r fit-big-models, eval=F}
m1 <- glmer(produces ~ age_sc + books_sc * mom_ed_sc + speech_sc * mom_ed_sc + (1|item_id) + (1|data_id), family=binomial, data=d_samp)

m2 <- glmer(produces ~ age_sc + lexical_class * books_sc * mom_ed_sc + lexical_class * speech_sc * mom_ed_sc + (1|item_id) + (1|data_id), family=binomial, data=d_samp)
#Model failed to converge with max|grad| = 0.0255532 (tol = 0.002, component 1)Model is nearly unidentifiable: large eigenvalue ratio
# - Rescale variables?

save(m1, m2, file="data/model_fits_scaled.Rdata")
```

# Unscaled

```{r}
# try just log-scaled...
m1 <- glmer(produces ~ age_sc + 
              log(books) * mom_ed_sc + 
              log(speech) * mom_ed_sc + 
              (1|item_id) + (1|data_id), family=binomial, data=d_samp)

m2 <- glmer(produces ~ age_sc + 
              lexical_class * log(books) * mom_ed_sc + 
              lexical_class * log(speech) * mom_ed_sc + 
              (1|item_id) + (1|data_id), family=binomial, data=d_samp)

# control for adult_book frequency
m3 <- glmer(produces ~ age_sc + log(adult_books) +
              log(books) * mom_ed_sc + 
              log(speech) * mom_ed_sc + 
              (1|item_id) + (1|data_id), family=binomial, data=d_samp)

m4 <- glmer(produces ~ age_sc + log(adult_books) +
              lexical_class * log(books) * mom_ed_sc + 
              lexical_class * log(speech) * mom_ed_sc + 
              (1|item_id) + (1|data_id), family=binomial, data=d_samp)
# Model failed to converge with max|grad| = 0.00295637 (tol = 0.002, component 1)Model is nearly unidentifiable: very large eigenvalue
# - Rescale variables?Model failed to converge with max|grad| = 0.148143 (tol = 0.002, component 1)Model is nearly unidentifiable: very large eigenvalue
# - Rescale variables?Model failed to converge with max|grad| = 0.0178983 (tol = 0.002, component 1)Model is nearly unidentifiable: very large eigenvalue
# - Rescale variables?Model failed to converge with max|grad| = 0.22543 (tol = 0.002, component 1)Model is nearly unidentifiable: very large eigenvalue
# - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
# - Rescale variables?

save(m1, m2, m3, m4, file="data/model_fits.Rdata")
```


```{r}
load("data/model_fits.Rdata")
```


```{r}
summary(m1)
summary(m2)
summary(m3)
summary(m4)

m1pred <- ggpredict(m1)
m1eff <- ggeffect(m1)

m1_age_mom_ed <- ggemmeans(m1, c("age_sc", "mom_ed_sc"))
m1_speech_mom_ed <- ggemmeans(m1, c("mom_ed_sc","speech_sc"))
m1_books_mom_ed <- ggemmeans(m1, c("mom_ed_sc","books_sc"))

ggarrange(plot(m1_age_mom_ed), plot(m1_speech_mom_ed), plot(m1_books_mom_ed), nrow=1)
```



```{r}
load("data/model_fits_scaled.Rdata")
```



```{r}
summary(m1)
anova(m1)

m1pred <- ggpredict(m1)
m1eff <- ggeffect(m1)

m1_age_mom_ed <- ggemmeans(m1, c("age_sc", "mom_ed_sc"))
m1_speech_mom_ed <- ggemmeans(m1, c("mom_ed_sc","speech_sc"))
m1_books_mom_ed <- ggemmeans(m1, c("mom_ed_sc","books_sc"))

ggarrange(plot(m1_age_mom_ed), plot(m1_speech_mom_ed), plot(m1_books_mom_ed), nrow=1)
```



```{r}
summary(m2)

m2book <- ggpredict(m2, terms=c("mom_ed_sc","lexical_class","books_sc"))
m2speech <- ggpredict(m2, terms=c("mom_ed_sc","lexical_class","speech_sc"))


ggarrange(plot(m2book), plot(m2speech), nrow=1, common.legend = T)
#m3 <- lmer(produces ~ age + mom_ed * prop_booky * lexical_class, data=d_samp)
# somehow control for word difficulty: 
# utterance length, word length, and/or morphological complexity?
```




## Also try same regressions controlling for Google books frequency (reference corpus) 


### Comparing to Adult Speech

For our reference corpus, we will use adult speech (movie subtitles), as this is the target language distribution that children will eventually learn.
(Although we could use child-directed speech, as that is what is given to children, or Google book frequency, which could be considered the epitome of an 'educated' distribution.)

```{r keyness-subtlex, echo=F, eval=F}
all_freq_long <- rbind(child_speech, adult_speech, child_books, adult_books) %>%
  select(word, source, on_cdi, word_count_norm) %>%
  filter(!is.element(word, c("d","m","s","t", "xxx"))) 

# wide - no smoothing, only keep subset of words in all corpora
all_freq <- all_freq_long %>%
  pivot_wider(names_from = source, values_from = word_count_norm) %>%
  filter(!is.na(Montag), !is.na(CHILDES), !is.na(SUBTLEX), !is.na(`Adult Books`)) %>%
  mutate(child_vs_adult_speech = CHILDES / SUBTLEX, 
         child_book_vs_adult_speech = Montag / SUBTLEX,
         adult_books_vs_speech = `Adult Books` / SUBTLEX)
         #ch_book_ad_book_ratio = Montag / `Adult Books`,
         #ch_book_ad_book_ratio = Montag / CHILDES) # are children's books like adult books, or like children's speech?

# Laplace smoothing, alpha=1, keep all 123060 words in any corpus
all_freq_smooth <- all_freq_long %>% 
  mutate(word_count_norm_smooth = word_count_norm + 1) %>%
  pivot_wider(id_cols = c(word, on_cdi), names_from = source, values_from = word_count_norm_smooth, values_fill=1) %>% 
  filter(!is.na(Montag), !is.na(CHILDES), !is.na(SUBTLEX), !is.na(`Adult Books`)) %>%
  mutate(child_vs_adult_speech = CHILDES / SUBTLEX, 
         child_book_vs_adult_speech = Montag / SUBTLEX,
         adult_books_vs_speech = `Adult Books` / SUBTLEX)
```

Non-SUBTLEX adult speech (Charlesworth corpus):

```{r keyness-charlesworth, echo=F, eval=F}
all_freq_long <- rbind(child_speech, adult_speech2, child_books, adult_books) %>%
  select(word, source, on_cdi, word_count_norm) %>%
  filter(!is.element(word, c("d","m","s","t", "xxx", "child's own name"))) 

# wide - no smoothing, only keep subset of 3977 words in all corpora
all_freq <- all_freq_long %>%
  pivot_wider(names_from = source, values_from = word_count_norm) %>%
  filter(!is.na(Montag), !is.na(CHILDES), !is.na(speech), !is.na(`Adult Books`)) %>%
  mutate(child_vs_adult_speech = CHILDES / speech, 
         child_book_vs_adult_speech = Montag / speech,
         adult_books_vs_speech = `Adult Books` / speech)

# Laplace smoothing, alpha=10, keep all 115597 words in any corpus
all_freq_smooth <- all_freq_long %>% 
  mutate(word_count_norm_smooth = word_count_norm + 10) %>%
  pivot_wider(id_cols = c(word, on_cdi), names_from = source, values_from = word_count_norm_smooth, values_fill=10) %>% 
  filter(!is.na(Montag), !is.na(CHILDES), !is.na(speech), !is.na(`Adult Books`)) %>%
  mutate(child_vs_adult_speech = CHILDES / speech, 
         child_book_vs_adult_speech = Montag / speech,
         adult_books_vs_speech = `Adult Books` / speech)
```

## Child-directed speech vs. Adult speech

We first examine the keyness of words in child-directed speech vs. that in adult speech.
Here are the 10 words most over-represented in child-directed speech compared to adult movies:

```{r, echo=F, eval=F}
# top 10
all_freq %>% arrange(desc(child_vs_adult_speech)) %>% head(10) %>% kable(digits=2)
all_freq_smooth %>% arrange(desc(child_vs_adult_speech)) %>% head(10) %>% kable(digits=2)
```


Here are the 10 words most under-represented in CHILDES, compared to adult movies:

```{r, echo=F, eval=F}
all_freq %>% arrange(desc(child_vs_adult_speech)) %>% tail(10) %>% kable(digits=2)
all_freq_smooth %>% arrange(desc(child_vs_adult_speech)) %>% tail(10) %>% kable(digits=2)
```


Below we show the average keyness of child-directed speech and children's books for CDI vs. non-CDI words, using adult speech as the reference corpus.
This is based on the words common to all corpora.

```{r, keyness-cdi-confint, echo=F, eval=F, fig.width=8, fig.height=3.5}
ch_book <- all_freq %>% group_by(on_cdi) %>%
  tidyboot_mean(child_book_vs_adult_speech)

ch_speech <- all_freq %>% group_by(on_cdi) %>%
  tidyboot_mean(child_vs_adult_speech)

ad_speech <- all_freq %>% group_by(on_cdi) %>%
  tidyboot_mean(adult_books_vs_speech)

p1 <- ch_book %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
  theme_classic() + ylab("Children's Books Keyness") #+ ylim(0,20)

p2 <- ch_speech %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
  theme_classic() + ylab("Child-directed Speech Keyness") #+ ylim(0,6)

p3 <- ad_speech %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
  theme_classic() + ylab("Adult Books Keyness") + ylim(0,5)

ggpubr::ggarrange(p1, p2, p3, nrow=1)
```

Now we do the same for the union of all words across the corpora (Laplace-smoothed).

```{r, keyness-cdi-confint-smooth, echo=F, eval=F, fig.width=8, fig.height=3.5}
ch_book <- all_freq_smooth %>% group_by(on_cdi) %>%
  tidyboot_mean(child_book_vs_adult_speech)

ch_speech <- all_freq_smooth %>% group_by(on_cdi) %>%
  tidyboot_mean(child_vs_adult_speech)

ad_speech <- all_freq_smooth %>% group_by(on_cdi) %>%
  tidyboot_mean(adult_books_vs_speech)

p1 <- ch_book %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
  theme_classic() + ylab("Children's Books Keyness") #+ ylim(0,20)

p2 <- ch_speech %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
  theme_classic() + ylab("Child-directed Speech Keyness") #+ ylim(0,6)

p3 <- ad_speech %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
  ggplot(aes(x=CDI, y=mean)) + 
  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
  theme_classic() + ylab("Adult Books Keyness") #+ ylim(0,5)

ggpubr::ggarrange(p1, p2, p3, nrow=1)
```


## Other Ideas

- Does the mean IRT difficulty of CDI/non-CDI words systematically vary with the source?
- Use adult books as reference corpus for keyness metric (instad of subtitles)
