---
title: "Trimmed from CogSci"
author: "George & Georgia"
date: "1/26/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r get-wordbank-data, eval=F}
require(wordbankr)
en_demo <- get_administration_data(language = "English (American)", form = "WS")

en_long_ws <- get_instrument_data(language="English (American)", form = "WS") %>%
  mutate(produces = as.numeric(value == "produces")) 

en_ws <- get_item_data(language="English (American)", form="WS") %>%
  filter(type=="word") %>% select(-complexity_category, -item_id)

en_long_ws <- en_long_ws %>% left_join(en_ws %>% select(num_item_id, definition)) %>%
  filter(!is.na(definition))

save(en_ws, en_demo, en_long_ws, file=here("data/en_ws.Rdata"))

# get French WS data
fr_demo <- get_administration_data(language = "French (French)", form = "WS")

fr_long_ws <- get_instrument_data(language="French (French)", form = "WS") %>%
  mutate(produces = as.numeric(value == "produces")) 

fr_ws <- get_item_data(language="French (French)", form="WS") %>%
  filter(type=="word") %>% select(-complexity_category, -item_id)

fr_long_ws <- fr_long_ws %>% left_join(fr_ws %>% select(num_item_id, definition)) %>%
  filter(!is.na(definition))

table(fr_demo$mom_ed) # small amount of French data with mom_ed: 46 secondary, 112 college, 64 grad

save(fr_ws, fr_demo, fr_long_ws, file=here("data/fr_ws.Rdata"))
```

Trimmed from ~line 400:


## Regularized Regression on PCA Loadings

Next we used the CDI items' PCA loadings in lieu of the frequency distributions to predict AoA.
First, we did a L1-regularized (i.e., LASSO) regression predicting AoA with all of the principal components, to test which of the PCs should be included in the regression with lexical class.
We used cross-validation to find the $\lambda$ value (penalty for outsized coefficients) that minimized mean-squared error of the test set, resulting in $\lambda=0.006$ for English, a small value which will result in coefficients that would be quite close to those obtained in ordinary least squares regression (i.e., when $\lambda=0$).
Using this best-fitting $\lambda$, this model had $R^2 = 0.32$ in English, and the estimated coefficients were non-zero for all of the principal components.
For English, there were negative coefficients for PC1 ($\beta=-0.17$), PC2 ($\beta=-1.13$), PC4 ($\beta=-1.32$), and PC6 ($\beta=-0.34$), indicating that higher values on these PCs predict earlier AoAs -- especially for PC2 and PC4, which mostly capture child-directed speech (PC2) and media (PC4). There were positive coefficients for PC3 ($\beta=0.88$) and PC5 ($\beta=1.95$), indicating that higher values of these PCs predict later AoAs, which is reasonable, as PC3 is associated with adult-directed speech, and PC5 captures child-directed books. This is still reasonable, since even child-directed books contain more function words than speech, which tend to be later-learned. Moreover, children receive a relatively small fraction of their daily input from books -- perhaps 20 minutes/day, in contrast with several hours of child-directed speech and overheard adult-directed speech. 

For French, this model had $R^2 = 0.12$, and the estimated coefficients were non-zero for all of the principal components. There were negative coefficients for PC1 ($\beta=-0.05$) and PC5 ($\beta=-0.13$), indicating that higher values on these PCs predict earlier AoAs. There were positive coefficients for PC2 ($\beta=0.14$), PC3 ($\beta=1.63$), PC4 ($\beta=0.18$) and PC6 ($\beta=0.82$) indicating that higher values of these PCs predict later AoAs.


```{r, pca-regression, include=F, results='asis'}
ddbsa$lexical_class <- fct_relevel(ddbsa$lexical_class, "nouns")
pca_reg1 <- lm(aoa ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data=ddbsa) 
#summary(pca_reg1)
# R^2 = .321
pca_tab1 <- data.frame(pca_reg1$coefficients) 

print(xtable(pca_tab1, digits=2,
             caption = "Regression predicting AoA with PCs.",
             table.placement = "H"), comment=F)
             
ddbsa_fr$lexical_class <- fct_relevel(ddbsa_fr$lexical_class, "nouns") 
pca_reg1_fr <- lm(aoa ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data=ddbsa_fr) 
#summary(pca_reg1)
# R^2 = .321
pca_tab1_fr <- data.frame(pca_reg1_fr$coefficients) 

print(xtable(pca_tab1_fr, digits=2,
             caption = "Regression predicting AoA with French PCs on function_words, verbs, adjectives, nouns)",
             table.placement = "H"), comment=F)             
```

```{r, lasso-pca-regression, include=F, results='asis'}
library(glmnet)

PCs <- data.matrix(ddbsa[,paste0("PC",1:6)])

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(PCs, ddbsa$aoa, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min # 0.006 (very close to linear regression)
#best_lambda <- cv_model$lambda.1se # 0.323 1SE above MSE

#find coefficients of best model
best_model <- glmnet(PCs, ddbsa$aoa, alpha = 1, lambda = best_lambda)
lg_tab <- as.matrix(coef(best_model))
print(xtable(lg_tab, digits=2,
             caption = "LASSO AoA regression coefficients.",
             table.placement = "H"), comment=F)
             
PCs_fr <- data.matrix(ddbsa_fr[,paste0("PC",1:6)])

#perform k-fold cross-validation to find optimal lambda value
#cv_model_fr <- cv.glmnet(PCs_fr, ddbsa_fr$aoa, alpha = 1)

#find optimal lambda value that minimizes test MSE
#best_lambda_fr <- cv_model_fr$lambda.min # 0.006 (very close to linear regression)
#best_lambda <- cv_model$lambda.1se # 0.323 1SE above MSE

#find coefficients of best model
#best_model_fr <- glmnet(PCs, ddbsa_fr$aoa, alpha = 1, lambda = best_lambda_fr)
#lg_tab_fr <- as.matrix(coef(best_model_fr))
#print(xtable(lg_tab_fr, digits=2,
#             caption = "LASSO AoA regression coefficients for French.",
#             table.placement = "H"), comment=F)
```


```{r, include=F, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.3, fig.height=3.3, set.cap.width=T, num.cols.cap=1, fig.cap = "Test MSE for different lambda values in the cross-validated lasso regression. The minimum MSE is achieved by the leftmost dotted line, while the rightmost dotted line shows the lambda that achieves MSE $<1$ standard error above this minimum."}

#produce plot of test MSE by lambda value
plot(cv_model) 
```

## English

Figure 2 shows the loadings of English CDI items on PC1-PC4 vs. the average age of acquisition (AoA; in months).

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=8.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "English principal components vs. age of acquisition, by lexical class."}
#knitr::include_graphics("../CHILDES_smoothed_norm_freqs_vs_books_TV.pdf")
txt_sz = 4

p1 <- ggplot(ddbsa, aes(x = PC1, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.2) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC1 < -5 | PC1 > 5), aes(label = word), size=txt_sz) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p2 <- ggplot(ddbsa, aes(x = PC2, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.2) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC2 < -2 | PC2 > 2), aes(label = word), size=txt_sz) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p3 <- ggplot(ddbsa, aes(x = PC3, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.2) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC3 < -1 | PC3 > 1), aes(label = word), size=txt_sz) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p4 <- ggplot(ddbsa, aes(x = PC4, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.2) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC4 < -1 | PC4 > 1), aes(label = word), size=txt_sz) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()


ggarrange(p1, p2, p3, p4, nrow=4, ncol=1, legend = "none")
ggsave("figs/EN_PC1-4_AoA_by_lexical_class.pdf", width=13, height=11)
```


## French

ToDo: adjust label constraints and re-save for OSF

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=8.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "French principal components vs. age of acquisition, by lexical class."}
txt_sz = 4


p1 <- ggplot(ddbsa_fr %>% filter(lexical_class!="adverbs", aoa<40), 
             aes(x = PC1, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.2) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa_fr, PC1 < -5 | PC1 > 5), aes(label = token), size=txt_sz) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p2 <- ggplot(ddbsa_fr %>% filter(lexical_class!="adverbs", aoa<40), 
             aes(x = PC2, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.2) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa_fr, PC2 < -2 | PC2 > 2), aes(label = token), size=txt_sz) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p3 <- ggplot(ddbsa_fr %>% filter(lexical_class!="adverbs", aoa<40), 
             aes(x = PC3, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.2) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa_fr, PC3 < -1 | PC3 > 1), aes(label = token), size=txt_sz) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p4 <- ggplot(ddbsa_fr %>% filter(lexical_class!="adverbs", aoa<40), 
             aes(x = PC4, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.2) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa_fr, PC4 < -1 | PC4 > 1), aes(label = token), size=txt_sz) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()


ggarrange(p1, p2, p3, p4, nrow=4, ncol=1, legend = "none")
ggsave("figs/FR_PC1-4_AoA_by_lexical_class.pdf", width=13, height=11)
```