---
title: "Identifying the distributional sources of children's early vocabulary"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis* (kachergis@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA
    \AND {\large \bf Georgia Loukatou (@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA}

abstract: >
    Children's early vocabulary learning must to a large extent be driven by the prevalence 
    of words: they can't learn a word if they haven't heard it. Indeed, previous research 
    has found that higher word frequency is a good predictor of earlier learning. However,
    despite considerable overlap, word frequency distributions also vary significantly 
    by source: child-directed speech, books, and television have distinct profiles. Children
    receive a mixture of these different frequency distributions, and here we attempt to 
    discern the varying impact of these input sources on children's early word learning.
    
keywords: >
    early language learning; CDI; vocabulary development.
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(here)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
require(tidytext)
require(GGally)
```

# Introduction

Previous studies have shown that frequency matters for children's word learning
[for a review, see @ambridge2015ubiquity], and have observed an association between word frequency in children's language environments and age of acquisition [@goodman2008]. However, input word frequency varies significantly depending on the context. For instance, word frequency in books is not the same as frequency in conversational speech [@montag2015words]. Some differences between frequency distributions are intuitive: "mommy" is quite frequent in child-directed speech (2,260 tokens per million; TPM), yet not so common in children's books (10 TPM), and even more rare in books meant for all ages (2 TPM). But other differences are less intuitive: "of" is frequent in books meant for all ages (41,630 TPM), and while still frequent in child-directed speech (5,900 TPM), relatively less so as compared to children's books (20,400 TPM).

Different language input sources, such as the use of child-directed register or book reading time, can lead to variance in input speech heard by child during their everyday lives. This input variance has often been interpreted as a function of the families' socioeconomic status (SES)[@rowe2018understanding]. Importantly, this variance has been found to relate to children's language development and could predict aspects of word learning [@hoff2003specificity]. 

The primary goal of this paper is to examine shared and unique variance in word frequency across different sources of input, ranging from children's books and movies to child-directed speech and adult books and movies, which we accomplish via principle components analysis (PCA). After characterizing the structure of the principle components of frequency from different sources, we investigate how well these components predict children's early word learning, using aggregate MacArthur-Bates Communicative Development Inventories (CDI) data from Wordbank [@frank2017wordbank].
Finally, we examine how well the frequency components predict individual children's word learning in combination with their mother's education, which may be related to how much children are read to at home.

# Method

## Datasets

```{r load-data}
load("word_freqs_fr.Rdata")
en<-load("data/merged_word_freqs.Rdata")


total_child_tokens <- ch_freq_new %>% 
  select(-adult_speech, -adult_tv, -adult_book) %>%
  pivot_longer(c(child_speech,  child_book, child_tv), names_to = "source", values_to = "word_count") %>%
    filter(!is.na(word_count)) %>%
    group_by(source) %>%
    summarise(word_count = sum(word_count, na.rm=TRUE),
              n = n())

total_adult_tokens <- ch_freq_new %>% 
  select(-child_speech, -child_tv, -child_book) %>%
  pivot_longer(c(adult_speech,  adult_book, adult_tv), names_to = "source", values_to = "word_count") %>%
    filter(!is.na(word_count)) %>%
    group_by(source) %>%
    summarise(word_count = sum(word_count, na.rm=TRUE),
              n = n())
```

### Child-directed Speech.
Utterances of child-directed speech (ChS) were extracted from the CHILDES corpus [@macwhinney2000childes], a collection of transcripts of dyadic interactions between caregivers and children of ages ranging from 0 to 12 years ($M=2.9$ years).
After cleaning, the CHILDES corpus yielded a total of `r subset(total_child_tokens, source=="speech")$word_count` tokens across `r subset(total_child_tokens, source=="speech")$n` word types.

### Child-directed books (ChdB).
After cleaning, this children's book corpus totals `r subset(total_child_tokens, source=="child_book")$word_count` tokens across `r subset(total_child_tokens, source=="child_book")$n` word types.

### Child-directed Media (ChM).
This children's media corpus totals `r subset(total_child_tokens, source=="child_tv")$word_count` tokens across `r subset(total_child_tokens, source=="child_tv")$n` word types.

### Adult-directed Speech (AdS).
The adult-directed speech corpus (AdS) yielded a total of `r subset(total_adult_tokens, source=="adult_speech")$word_count` tokens across `r subset(total_adult_tokens, source=="adult_speech")$n` word types.

### Adult-directed Books (AdB).
The adult-directed book corpus (AdB) is comprised of `r subset(total_adult_tokens, source=="adult_book")$word_count` tokens across `r subset(total_adult_tokens, source=="adult_book")$n` word types.


### Adult-directed Media (AdM).
The adult-directed media corpus (AdM) is comprised of `r subset(total_adult_tokens, source=="adult_tv")$word_count` tokens across `r subset(total_adult_tokens, source=="adult_tv")$n` word types.



## Merging the Corpora

```{r}
#load("../data/Charlesworth-unlemmatized-counts.Rdata") 
# Montag corpus
dd <- ch_freq_smooth_new %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech, -prop_booky) %>%
  mutate(Nletters = nchar(token)) %>%
  mutate(adult_book = ifelse(is.na(adult_book), 10, adult_book),
         adult_speech = ifelse(is.na(adult_speech), 10, adult_speech),
         child_book = ifelse(is.na(child_book), 10, child_book),
         adult_tv = ifelse(is.na(adult_tv), 10, adult_tv),
         child_tv = ifelse(is.na(child_tv), 10, child_tv),
         child_speech = ifelse(is.na(child_speech), 10, child_speech)) %>%
         distinct()

dddc <- dd %>% 
  rename(`AdB` = adult_book,  
         `ChB` = child_book, 
         `AdM` = adult_tv,
         `AdS` = adult_speech,
         `ChS` = child_speech,
         `ChM` = child_tv) 

```


For any words that failed to appear in a given corpus, we replaced the missing word's frequency with a normalized count of 10 TPM.
`r sum(is.na(ch_freq_smooth_new$child_book))` missing word was replaced in each of ChB, `r sum(is.na(ch_freq_smooth_new$child_tv))` for ChM, and `r sum(is.na(ch_freq_smooth_new$child_speech))` for ChS, while `r sum(is.na(ch_freq_smooth_new$adult_book))` missing words were replaced in AdB, `r sum(is.na(ch_freq_smooth_new$adult_tv))` for AdM and `r sum(is.na(ch_freq_smooth_new$child_speech))` for AdS.

# Results


## Cross-corpus Frequency Correlations 

Table 1 shows the cross-corpus word frequency correlations for the `r nrow(dddc)` CDI words.

```{r}
cor(dddc[,c("ChS","ChM","ChB","AdS","AdM","AdB")]) %>%
    papaja::apa_table(digits=2, placement='h',
                      caption="Correlation table of word frequency distributions for CDI words.")
```


## Age of Acquisition Regression

```{r, results='asis'}
lm1 <- lm(aoa ~ log(ChS) + log(ChM) + log(ChB) + log(AdS) + log(AdM) + log(AdB) + Nletters, data=dddc)
# R^2 = 0.3433
tab <- data.frame(summary(lm1)$coefficients)
tab$VIF = c(NA, car::vif(lm1))
names(tab) = c("Beta","SE","t-val","p-val","VIF")
#tab %>% kable(digits=2)
print(xtable(tab, digits=2,
             caption = "Coefficients for corpora frequencies predicting AoA.",
             table.placement = "H"), comment=F)
```


## Principal Components of Frequency

We use PCA to examine the principal components of the six log-scaled word frequency distributions ([adult- vs. child-directed] x [speech, books, media]).
Table 3 shows the standard deviation (Std Dev) and proportion of variance explained, both individually (Prop Var) and cumulatively (Cum Prop Var) by the principal components (PC1-PC6).
PC1 already explains the bulk the variance (89%), and PC2-PC4 each only capture an additional 2-4% of the variance.
In total, the first four components capture >98% of the variance.

```{r}
pc_bsa <- prcomp(log(dddc[,c("ChM","ChB","ChS",
                             "AdM","AdB","AdS")])) # add Montag separately?
#eigenvalues = pc_bsa$sdev^2 # only first 2 PCs have eigenvalues >1
pc_imp <- summary(pc_bsa)$importance 
row.names(pc_imp) = c("Std Dev","Prop Var","Cum Prop Var")
pc_imp %>%
    papaja::apa_table(digits=2, caption="Importance of components from PCA.", placement='h')
```

```{r}
#pc_bsa$rotation  * -1
ddbsa <- cbind(dddc, data.frame(pc_bsa$x))
pc_bsa$rotation %>%
    papaja::apa_table(digits=2, caption="Principal components' rotation in the original coordinate system.", placement='h')
```


```{r, include=F}
## Correlation of PCA with original Frequency Distributions
cor(ddbsa[,c("ChM","ChB","ChS","AdM","AdB","AdS")], ddbsa[,paste0("PC",1:6)]) %>%
  papaja::apa_table(digits=2, caption="Correlation of PC loadings with frequency distributions.", placement='h')
```


## PCA-based Prediction of Age of Acquisition 

Figure 1 shows the loadings of CDI items on PC1-PC4 vs. the average age of acquisition (AoA; in months).

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=8.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Principal components vs. age of acquisition, by lexical class."}
#knitr::include_graphics("../CHILDES_smoothed_norm_freqs_vs_books_TV.pdf")
ddbsa <- ddbsa %>%
  rename(word = token) %>%
  filter(!is.na(aoa))

p1 <- ggplot(ddbsa, aes(x = PC1, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC1 < -5 | PC1 > 5), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p2 <- ggplot(ddbsa, aes(x = PC2, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC2 < -2 | PC2 > 2), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p3 <- ggplot(ddbsa, aes(x = PC3, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC3 < -1 | PC3 > 1), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p4 <- ggplot(ddbsa, aes(x = PC4, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC4 < -1 | PC4 > 1), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

ggarrange(p1, p2, p3, p4, nrow=4, ncol=1, legend = "none")
```

## Regularized Regression on PCA Loadings

Next we used the CDI items' PCA loadings in lieu of the frequency distributions to predict AoA.
First, we did a L1-regularized (i.e., LASSO) regression predicting AoA with all of the principal components, to test which of the PCs should be included in the regression with lexical class.
We used cross-validation to find the $\lambda$ value (penalty for outsized coefficients) that minimized mean-squared error of the test set, resulting in $\lambda=0.006$, a small value which will result in coefficients that would be quite close to those obtained in ordinary least squares regression (i.e., when $\lambda=0$).
Table 5 displays the estimated coefficients using this best-fitting $\lambda$, showing that all of the PCs have non-zero values.
This model has $R^2 = 0.32$.

```{r, pca-regression, include=F, results='asis'}
ddbsa$lexical_class <- fct_relevel(ddbsa$lexical_class, "nouns")
pca_reg1 <- lm(aoa ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data=ddbsa) 
#summary(pca_reg1)
# R^2 = .321
pca_tab1 <- data.frame(pca_reg1$coefficients) 

print(xtable(pca_tab1, digits=2,
             caption = "Regression predicting AoA with PCs.",
             table.placement = "H"), comment=F)
```

```{r, lasso-pca-regression, results='asis'}
library(glmnet)

PCs <- data.matrix(ddbsa[,paste0("PC",1:6)])

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(PCs, ddbsa$aoa, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min # 0.006 (very close to linear regression)
#best_lambda <- cv_model$lambda.1se # 0.323 1SE above MSE

#find coefficients of best model
best_model <- glmnet(PCs, ddbsa$aoa, alpha = 1, lambda = best_lambda)
lg_tab <- as.matrix(coef(best_model))
print(xtable(lg_tab, digits=2,
             caption = "LASSO AoA regression coefficients.",
             table.placement = "H"), comment=F)
```


```{r,  fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.3, fig.height=3.3, set.cap.width=T, num.cols.cap=1, fig.cap = "Test MSE for different lambda values in the cross-validated lasso regression. The minimum MSE is achieved by the leftmost dotted line, while the rightmost dotted line shows the lambda that achieves MSE $<1$ standard error above this minimum."}

#produce plot of test MSE by lambda value
plot(cv_model) 
```

Given that past research has found that lexical class strongly modulates influences of word frequency, we next examine the interaction of lexical class (LC) with PC1 - PC6. 
To determine if the inclusion of all PCs was justified, we ran a series of ANOVAs building up from PC1 to PC6--in decreasing order of the variance they accounted for in the PCA.
Thus, the R syntax for the sequence of regressions was `AoA~PC1*LC`, `AoA~(PC1+PC2)*LC`, ..., `AoA~(PC1 + PC2+PC3+PC4+PC5+PC6)*LC`.
The more complex model was always significantly preferred, including up to the inclusion of PC6.
Table X shows the results of this final regression, which yielded $R^2 = .584$.

```{r, results='asis'}
# could try adding in PCs one by one..

ddbsa <- ddbsa %>%
  mutate(LC = case_when(lexical_class=="nouns" ~ "noun",
                        lexical_class=="verbs" ~ "verb",
                        lexical_class=="adjectives" ~ "adj",
                        lexical_class=="function_words" ~ "func",
                        lexical_class=="other" ~ "other",
                        TRUE ~ as.character(lexical_class)))
ddbsa$LC <- fct_relevel(ddbsa$LC, "noun")

pca_lc1 <- lm(aoa ~ PC1 * LC, data=ddbsa) # R^2=.26
pca_lc2 <- lm(aoa ~ (PC1 + PC2) * LC, data=ddbsa) # R^2=.47
pca_lc3 <- lm(aoa ~ (PC1 + PC2 + PC3) * LC, data=ddbsa)
pca_lc4 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4) * LC, data=ddbsa)
pca_lc5 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4 + PC5) * LC, data=ddbsa) # 0.576
pca_lc6 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4 + PC5 + PC6) * LC, data=ddbsa)

ttt <- anova(pca_lc1, pca_lc2) # 2***
ttt <- rbind(ttt, anova(pca_lc2, pca_lc3)[2,]) # 3***
ttt <- rbind(ttt, anova(pca_lc3, pca_lc4)[2,]) # 4***
ttt <- rbind(ttt, anova(pca_lc4, pca_lc5)[2,]) # 5***
ttt <- rbind(ttt, anova(pca_lc5, pca_lc6)[2,]) # 6*

#summary(pca_lc6) 
# R^2 = .584
pca_tab2 <- data.frame(summary(pca_lc6)$coefficients)
#car::vif(pca_lc_reg)
names(pca_tab2) = c("Beta","SE","t-val","p-val")

print(xtable(pca_tab2, digits=2,
             caption = "Regression predicting AoA with PCs and lexical class.",
             table.placement = "H"), comment=F)
```

## Combining distributions with demographic data

Past research has found that young children from higher-SES households tend to have larger vocabulary.
Parents with higher-SES tend to also report reading more to their young children than parents with lower SES. 
Together, this suggests that the vocabulary composition of children from higher-SES households may be better predicted by the word frequencies seen in child-directed books, rather than those from child-directed speech. 
To test this idea, we regressed 

```{r}
# Mike: PC1 + PC2 * mom_ed
# group_by(age, mom_ed, item)
# summaries successes and failures
# glmer(cbind(produces, doesn't produce) ~ age * PC1 + age * PC2 * mom_ed + (1|item))


```




# Discussion



# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
