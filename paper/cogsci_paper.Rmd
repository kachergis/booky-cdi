---
title: "Identifying the distributional sources of children's early vocabulary"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis* (kachergis@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA
    \AND {\large \bf Georgia Loukatou (@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA}
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA}

abstract: >
    Children's early vocabulary learning must to a large extent be driven by the prevalence 
    of words: they can't learn a word if they haven't heard it. Indeed, previous research 
    has found that higher word frequency is a good predictor of earlier learning. However,
    despite considerable overlap, word frequency distributions also vary significantly 
    by source: child-directed speech, books, and television have distinct profiles. Children
    receive a mixture of these different frequency distributions, and the ratios of the mixture
    may be predictive of their early word learning. The goal of this paper is to better understand
    the shared and unique variance in these sources of input--in both English and French--and 
    to evaluate how predictive these input frequencies are of children's early word learning.
    
keywords: >
    early language learning; CDI; vocabulary development; word frequency distributions.
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(here)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
require(tidytext)
require(GGally)
```

# Introduction

Previous studies have shown that frequency matters for children's word learning [for a review, see @ambridge2015ubiquity], and have observed an association between word frequency in children's language environments and age of acquisition [@goodman2008]. 
However, input word frequency varies significantly depending on the context. 
For instance, word frequency in books is not the same as frequency in conversational speech [@dawson2021features; @montag2015words]. 
Some differences between frequency distributions are intuitive: "mommy" is quite frequent in child-directed speech (2,260 tokens per million; TPM), yet not so common in children's books (10 TPM), and even more rare in books meant for all ages (2 TPM). 
But other differences are less intuitive: "of" is frequent in books meant for all ages (41,630 TPM), and while still frequent in child-directed speech (5,900 TPM), relatively less so as compared to children's books (20,400 TPM).
In general, speech--both directed to children, and to adults--contains relatively fewer function words, and tends to score lower on measures of lexical diversity than books [@dawson2021features].

Different language input sources, such as the use of child-directed register or book reading time, can lead to variance in input speech heard by child during their everyday lives. 
This input variance has often been interpreted as a function of the families' socioeconomic status [SES; @rowe2018understanding]. 
Importantly, this variance has been found to relate to children's language development and to be predictive of aspects of word learning [@hoff2003specificity]. 

The primary goal of this paper is to examine shared and unique variance in word frequency across different sources of input, ranging from children's books and movies to child-directed speech and adult-directed books, movies, and speech, which we accomplish via principle components analysis (PCA). 
After characterizing the structure of the principle components of frequency from different sources in both English and French, we investigate how well these components predict English- and French-learning children's early word learning, using aggregate MacArthur-Bates Communicative Development Inventories (CDI) data from Wordbank [@frank2017wordbank]. 
Finally, we examine how well the frequency components predict individual children's word learning in combination with their mother's education, which may be related to how much children are read to at home.

# Method

## Datasets

```{r load-data}
load("../data/merged_word_freqs.Rdata")
load("../data/merged_word_freqs_fr.Rdata") 

aoas <- readRDS("../data/english_(american)_aoa_bydefinition.rds") %>%
  filter(measure=="produces")

load("../data/Charlesworth-unlemmatized-counts.Rdata") 

total_child_tokens <- child_dat %>% group_by(source) %>%
    summarise(word_count = sum(word_count),
              n = n())

total_adult_tokens <- adult_dat %>% group_by(source) %>%
    summarise(word_count = sum(word_count),
              n = n())
              
              
total_child_tokens_fr <- ch_freq_fr %>% 
  select(-adult_speech, -adult_tv, -adult_book) %>%
  pivot_longer(c(child_speech,  child_book, child_tv), names_to = "source", values_to = "word_count") %>%
    filter(!is.na(word_count)) %>%
    group_by(source) %>%
    summarise(word_count = sum(word_count, na.rm=TRUE),
              n = n())

total_adult_tokens_fr <- ch_freq_fr %>% 
  select(-child_speech, -child_tv, -child_book) %>%
  pivot_longer(c(adult_speech,  adult_book, adult_tv), names_to = "source", values_to = "word_count") %>%
    filter(!is.na(word_count)) %>%
    group_by(source) %>%
    summarise(word_count = sum(word_count, na.rm=TRUE),
              n = n())
```


### Child-directed Speech.
Utterances of child-directed speech (ChS) were extracted from the CHILDES corpus [@macwhinney2000childes], a collection of transcripts of dyadic interactions between caregivers and children of ages ranging from 0 to 12 years ($M=2.9$ years).
After cleaning, the CHILDES corpus yielded a total of `r subset(total_child_tokens, source=="speech")$word_count` tokens across `r subset(total_child_tokens, source=="speech")$n` word types. The French CHILDES corpus yielded a total of `r subset(total_child_tokens_fr, source=="child_speech")$word_count` tokens across `r subset(total_child_tokens_fr, source=="child_speech")$n` word types.

### Child-directed books (ChdB).
We used a sample of 98 children's books from Project Gutenberg's open-source database of books that has been used in prior machine learning research on language comprehension [@hill2015goldilocks]. 
These books were published between 1820 and 1922, but include such well-known titles as *The Legend of Sleep Hollow*.
After cleaning, this children's book corpus totals `r subset(total_child_tokens, source=="books")$word_count` tokens across `r subset(total_child_tokens, source=="books")$n` word types. For French, we used 130 popular children stories openly accessible in parenting websites. After cleaning, this children's book corpus totals `r subset(total_child_tokens_fr, source=="child_book")$word_count` tokens across `r subset(total_child_tokens_fr, source=="child_books")$n` word types.

### Child-directed Media (ChM).
Transcripts from television shows (e.g., from PBS Kids and Nickelodeon) and movies (e.g., *Beauty and the Beast*), including 1,078 movies and 4,309 TV episodes were taken from @charlesworth2021gender (available here: [https://osf.io/kqux5/](https://osf.io/kqux5/).
After cleaning, this children's media corpus totals `r subset(total_child_tokens, source=="tv")$word_count` tokens across `r subset(total_child_tokens, source=="tv")$n` word types. For French, we also used openly accessible movie subtitles. After cleaning, this children's media corpus totals `r subset(total_child_tokens_fr, source=="child_tv")$word_count` tokens across `r subset(total_child_tokens_fr, source=="child_tv")$n` word types. 

### Adult-directed Speech (AdS).
Adult-directed speech was obtained from the Switchboard-1 Telephone Speech Corpus (Godfrey & Holliman, 1993), a corpus of transcripts from dyadic telephone conversations in which 543 adult speakers were assigned to discuss a randomly-assigned topic.
After cleaning, the adult-directed speech corpus (AdS) yielded a total of `r subset(total_adult_tokens, source=="speech")$word_count` tokens across `r subset(total_adult_tokens, source=="speech")$n` word types. For French, adult-directed speech was obtained from the TCOF corpus of adult speech transcripts collected during the 80s-90s for research [@andre2010mise]. The adult-directed speech corpus (AdS) yielded a total of `r subset(total_adult_tokens_fr, source=="adult_speech")$word_count` tokens across `r subset(total_adult_tokens_fr, source=="adult_speech")$n` word types.

### Adult-directed Books (AdB).
The adult-directed book corpus (AdB) is comprised of `r subset(total_adult_tokens, source=="books")$word_count` tokens across `r subset(total_adult_tokens, source=="books")$n` word types. For French, the corpus is comprised of books taken from the 1999 Association de Bibliophiles Universels, an open-source database of french books. It is comprised of `r subset(total_adult_tokens_fr, source=="adult_book")$word_count` tokens across `r subset(total_adult_tokens_fr, source=="adult_book")$n` word types.


### Adult-directed Media (AdM).
The adult-directed media corpus (AdM) is comprised of `r subset(total_adult_tokens, source=="TV")$word_count` tokens across `r subset(total_adult_tokens, source=="TV")$n` word types. For French, the corpus is comprised of `r subset(total_adult_tokens_fr, source=="adult_tv")$word_count` tokens across `r subset(total_adult_tokens_fr, source=="adult_tv")$n` word types.


## Merging the Corpora

```{r}
#load("../data/Charlesworth-unlemmatized-counts.Rdata") 
# Montag corpus
dd <- ch_freq_smooth %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

# Charlesworth (FB children's books)
dd_novels <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

ddd <- dd %>% select(-prop_booky) %>%
  left_join(dd_novels %>% select(-prop_booky)) %>%
  mutate(Nletters = ifelse(is.na(Nletters), nchar(word), Nletters)) %>%
  left_join(aoas %>% select(-measure))

dddc <- ddd %>% mutate(adult_books = ifelse(is.na(adult_books), 0.07322, adult_books),
                       speech = ifelse(is.na(speech), 10, speech),
                       books = ifelse(is.na(books), 10, books),
                       tv = ifelse(is.na(tv), 10, tv)) %>%
  distinct()

adult_dat <- adult_dat %>% filter(source!="books") %>%
  select(source, word, word_count_norm) %>%
  pivot_wider(id_col = word, names_from = source, values_from = word_count_norm) %>%
  rename(`AdS` = speech,
         `AdM` = TV)

dddc <- dddc %>% 
  rename(`AdB` = adult_books, # Google books
         `ChB` = books, # Facebooks children's books (novels..)
         `ChM` = tv,
         `ChS` = speech) %>% 
  select(-CHILDES) %>% # less well-cleaned version
  left_join(adult_dat)

dddc <- dddc %>% replace_na(list(`AdS` = 0.32,
                                 `AdM` = 0.32))
```
Children's early word learning data is drrawn from the CDIs [@fenson2007]. CDIs are parental reports on their children’s lexical development, proven to be reliable indicators of a child’s language. CDIs are survey instruments, where parents mark whether their child (age ranges 8-15 and 16-30 months old) understands or produces particular words out of a list of several hundred words.

All word frequencies were normalized to number of tokens per million (TPM).
We focus our analysis on the 674 words from the CDI that we were able to find in at least some of the corpora.
We were unable to match 6 CDI items in any of the corpora, including "babysitter's name", "child's own name", ...
For any CDI words that failed to appear in a given corpus, we replaced the missing word's frequency with a normalized count of 10 TPM, or the minimum normalized frequency for that distribution, whichever was smaller.
`r sum(is.na(ddd$books))` missing word was replaced in each of ChB, ChM, and ChS, while `r sum(is.na(ddd$adult_books))` missing words were replaced in AdB. ...

# Results


## Cross-corpus Frequency Correlations 

Table 1 shows the cross-corpus word frequency correlations for the `r nrow(dddc)` CDI words.

```{r}
cor(dddc[,c("ChS","ChM","ChB","AdS","AdM","AdB")]) %>%
    papaja::apa_table(digits=2, placement='h',
                      caption="Correlation table of word frequency distributions for CDI words.")
```


## Age of Acquisition Regression

Despite the strong correlations of word frequency across these different corpora, we will attempt a simple regression predicting each CDI word's mean Age of Acquisition (AoA) -- the mean age (in months) at which 50% of children are expected to know a given word [@braginsky2019consistency].
We also include the number of letters as a predictor (Nletters) to help control for the overall difficulty of each word.
Table 2 below displays the estimated coefficients of this regression, showing significant contributions of all child-directed distributions (speech, books, and media), and of adult speech frequencies, as well as the number of letters.
However, the variance inflation factor (VIF) values for all of the frequency distributions are all $>>1$ (and many $>5$ or $>10$), indicating that these variables show strong multicollinearity which may compromise the reliability of the regression results.
Thus, we turned to principal components analysis (PCA) to disentangle these correlated distributions and to understand their interrelations.

```{r, results='asis'}
lm1 <- lm(aoa ~ log(ChS) + log(ChM) + log(ChB) + log(AdS) + log(AdM) + log(AdB) + Nletters, data=dddc)
# R^2 = 0.3433
tab <- data.frame(summary(lm1)$coefficients)
tab$VIF = c(NA, car::vif(lm1))
names(tab) = c("Beta","SE","t-val","p-val","VIF")
#tab %>% kable(digits=2)
print(xtable(tab, digits=2,
             caption = "Coefficients for corpora frequencies predicting AoA.",
             table.placement = "H"), comment=F)
```


## Principal Components of Frequency

We use PCA to examine the principal components of the six log-scaled word frequency distributions ([adult- vs. child-directed] x [speech, books, media]).
Table 3 shows the standard deviation (Std Dev) and proportion of variance explained, both individually (Prop Var) and cumulatively (Cum Prop Var) by the principal components (PC1-PC6).
PC1 already explains the bulk the variance (89%), and PC2-PC4 each only capture an additional 2-4% of the variance.
In total, the first four components capture >98% of the variance.

```{r}
pc_bsa <- prcomp(log(dddc[,c("ChM","ChB","ChS",
                             "AdM","AdB","AdS")])) # add Montag separately?
#eigenvalues = pc_bsa$sdev^2 # only first 2 PCs have eigenvalues >1
pc_imp <- summary(pc_bsa)$importance 
row.names(pc_imp) = c("Std Dev","Prop Var","Cum Prop Var")
pc_imp %>%
    papaja::apa_table(digits=2, caption="Importance of components from PCA.", placement='h')
```

Table 4 shows the eigenvectors of the principal components (PC1-PC6) in relation to the original six frequency distributions.
Table 5 shows the correlation of each frequency distribution with the CDI items' loadings on each principal component [GK: cut one of these tables? which?].
It is clear that PC1 captures overall frequency, but loads more strongly on the adult distributions (-.47 to -.52).
PC2 (3.8% of variance) mostly captures child-directed speech (0.65), especially differentiating it from adult-directed books and media (-.47, -.45).
PC3 (3.1% of variance) captures adult-directed speech (.79), particularly from child-directed distributions.
PC4 (2.2% of variance) captures the similarity of child- and adult-directed media (.40, .48), distinguishing them from child- and adult-directed books (-.63, -.44).
PC5 (1.1% of variance) mostly captures child-directed books (0.53), distinguishing it from child-directed speech (-.60) and adult-directed books (-.50).
PC6 (<1% of variance) captures child-directed media (.73), distinguishing it in particular from adult-directed media (-.52).
In summary, the principal components align surprisingly well with particular dimensions of the frequency distributions: PC1 with overall adult-directed frequency, PC2 with child-directed speech, PC3 with adult-directed speech, PC4 with media vs. books, PC5 with child-directed books vs. speech, and PC6 with adult- vs. child-directed media.
Although even PC5 and PC6 seem interpretable, given limited space and the fact that they account for very little variance (~1%) we will focus on PC1-PC4 for the rest of our analyses.

```{r}
#pc_bsa$rotation  * -1
ddbsa <- cbind(dddc, data.frame(pc_bsa$x))
pc_bsa$rotation %>%
    papaja::apa_table(digits=2, caption="Principal components' rotation in the original coordinate system.", placement='h')
```


```{r, include=F}
## Correlation of PCA with original Frequency Distributions
cor(ddbsa[,c("ChM","ChB","ChS","AdM","AdB","AdS")], ddbsa[,paste0("PC",1:6)]) %>%
  papaja::apa_table(digits=2, caption="Correlation of PC loadings with frequency distributions.", placement='h')
```


## PCA-based Prediction of Age of Acquisition 

Figure 1 shows the loadings of CDI items on PC1-PC4 vs. the average age of acquisition (AoA; in months).

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=8.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Principal components vs. age of acquisition, by lexical class."}
#knitr::include_graphics("../CHILDES_smoothed_norm_freqs_vs_books_TV.pdf")

p1 <- ggplot(ddbsa, aes(x = PC1, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC1 < -5 | PC1 > 5), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p2 <- ggplot(ddbsa, aes(x = PC2, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC2 < -2 | PC2 > 2), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p3 <- ggplot(ddbsa, aes(x = PC3, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC3 < -1 | PC3 > 1), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p4 <- ggplot(ddbsa, aes(x = PC4, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC4 < -1 | PC4 > 1), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

ggarrange(p1, p2, p3, p4, nrow=4, ncol=1, legend = "none")
```

## Regularized Regression on PCA Loadings

Next we used the CDI items' PCA loadings in lieu of the frequency distributions to predict AoA.
First, we did a L1-regularized (i.e., LASSO) regression predicting AoA with all of the principal components, to test which of the PCs should be included in the regression with lexical class.
We used cross-validation to find the $\lambda$ value (penalty for outsized coefficients) that minimized mean-squared error of the test set, resulting in $\lambda=0.006$, a small value which will result in coefficients that would be quite close to those obtained in ordinary least squares regression (i.e., when $\lambda=0$).
Table 5 displays the estimated coefficients using this best-fitting $\lambda$, showing that all of the PCs have non-zero values.
This model has $R^2 = 0.32$.

```{r, pca-regression, include=F, results='asis'}
ddbsa$lexical_class <- fct_relevel(ddbsa$lexical_class, "nouns")
pca_reg1 <- lm(aoa ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data=ddbsa) 
#summary(pca_reg1)
# R^2 = .321
pca_tab1 <- data.frame(pca_reg1$coefficients) 

print(xtable(pca_tab1, digits=2,
             caption = "Regression predicting AoA with PCs.",
             table.placement = "H"), comment=F)
```

```{r, lasso-pca-regression, results='asis'}
library(glmnet)

PCs <- data.matrix(ddbsa[,paste0("PC",1:6)])

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(PCs, ddbsa$aoa, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min # 0.006 (very close to linear regression)
#best_lambda <- cv_model$lambda.1se # 0.323 1SE above MSE

#find coefficients of best model
best_model <- glmnet(PCs, ddbsa$aoa, alpha = 1, lambda = best_lambda)
lg_tab <- as.matrix(coef(best_model))
print(xtable(lg_tab, digits=2,
             caption = "LASSO AoA regression coefficients.",
             table.placement = "H"), comment=F)
```


```{r,  fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.3, fig.height=3.3, set.cap.width=T, num.cols.cap=1, fig.cap = "Test MSE for different lambda values in the cross-validated lasso regression. The minimum MSE is achieved by the leftmost dotted line, while the rightmost dotted line shows the lambda that achieves MSE $<1$ standard error above this minimum."}

#produce plot of test MSE by lambda value
plot(cv_model) 
```

Given that past research has found that lexical class strongly modulates influences of word frequency, we next examine the interaction of lexical class (LC) with PC1 - PC6. 
To determine if the inclusion of all PCs was justified, we ran a series of ANOVAs building up from PC1 to PC6--in decreasing order of the variance they accounted for in the PCA.
Thus, the R syntax for the sequence of regressions was `AoA~PC1*LC`, `AoA~(PC1+PC2)*LC`, ..., `AoA~(PC1 + PC2+PC3+PC4+PC5+PC6)*LC`.
The more complex model was always significantly preferred, including up to the inclusion of PC6.
Table X shows the results of this final regression, which yielded $R^2 = .584$.

```{r, results='asis'}
# could try adding in PCs one by one..

ddbsa <- ddbsa %>%
  mutate(LC = case_when(lexical_class=="nouns" ~ "noun",
                        lexical_class=="verbs" ~ "verb",
                        lexical_class=="adjectives" ~ "adj",
                        lexical_class=="function_words" ~ "func",
                        lexical_class=="other" ~ "other",
                        TRUE ~ as.character(lexical_class)))
ddbsa$LC <- fct_relevel(ddbsa$LC, "noun")

pca_lc1 <- lm(aoa ~ PC1 * LC, data=ddbsa) # R^2=.26
pca_lc2 <- lm(aoa ~ (PC1 + PC2) * LC, data=ddbsa) # R^2=.47
pca_lc3 <- lm(aoa ~ (PC1 + PC2 + PC3) * LC, data=ddbsa)
pca_lc4 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4) * LC, data=ddbsa)
pca_lc5 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4 + PC5) * LC, data=ddbsa) # 0.576
pca_lc6 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4 + PC5 + PC6) * LC, data=ddbsa)

ttt <- anova(pca_lc1, pca_lc2) # 2***
ttt <- rbind(ttt, anova(pca_lc2, pca_lc3)[2,]) # 3***
ttt <- rbind(ttt, anova(pca_lc3, pca_lc4)[2,]) # 4***
ttt <- rbind(ttt, anova(pca_lc4, pca_lc5)[2,]) # 5***
ttt <- rbind(ttt, anova(pca_lc5, pca_lc6)[2,]) # 6*

#summary(pca_lc6) 
# R^2 = .584
pca_tab2 <- data.frame(summary(pca_lc6)$coefficients)
#car::vif(pca_lc_reg)
names(pca_tab2) = c("Beta","SE","t-val","p-val")

print(xtable(pca_tab2, digits=2,
             caption = "Regression predicting AoA with PCs and lexical class.",
             table.placement = "H"), comment=F)
```

## Combining distributions with demographic data

Past research has found that young children from higher-SES households tend to have larger vocabulary.
Parents with higher-SES tend to also report reading more to their young children than parents with lower SES. 
Together, this suggests that the vocabulary composition of children from higher-SES households may be better predicted by the word frequencies seen in child-directed books, rather than those from child-directed speech. 
To test this idea, we regressed 

```{r get-wordbank-data, eval=F}
require(wordbankr)
en_demo <- get_administration_data(language = "English (American)", form = "WS")

en_long_ws <- get_instrument_data(language="English (American)", form = "WS") %>%
  mutate(produces = as.numeric(value == "produces")) 

en_ws <- get_item_data(language="English (American)", form="WS") %>%
  filter(type=="word") %>% select(-complexity_category, -item_id)

en_long_ws <- en_long_ws %>% left_join(en_ws %>% select(num_item_id, definition)) %>%
  filter(!is.na(definition))

save(en_ws, en_demo, en_long_ws, file=here("data/en_ws.Rdata"))
```


```{r}
load(here("data/en_ws.Rdata"))
# Mike: PC1 + PC2 * mom_ed
# group_by(age, mom_ed, item)
# summaries successes and failures
# glmer(cbind(produces, doesn't produce) ~ age * PC1 + age * PC2 * mom_ed + (1|item))

en_dat <- en_demo %>% filter(!is.na(mom_ed)) 

en_dat <- en_long_ws %>% 
```




# Discussion



# Acknowledgements

This research was funded in part by (Georgia's grant).
We thank members of the Language and Cognition lab for their feedback.


# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
