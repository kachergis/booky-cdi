---
title: "Identifying the distributional sources of children's early vocabulary"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis* (kachergis@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA
    \AND {\large \bf Georgia Loukatou (@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA}

abstract: >
    Children's early vocabulary learning must to a large extent be driven by the prevalence 
    of words: they can't learn a word if they haven't heard it. Indeed, previous research 
    has found that higher word frequency is a good predictor of earlier learning. However,
    despite considerable overlap, word frequency distributions also vary significantly 
    by source: child-directed speech, books, and television have distinct profiles. Children
    receive a mixture of these different frequency distributions, and here we attempt to 
    discern the varying impact of these input sources on children's early word learning.
    
keywords: >
    early language learning; CDI; vocabulary development.
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(here)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
require(tidytext)
require(GGally)
```

# Introduction

Word usage varies significantly by context: you don't speak to your boss the same way as you do to your dog.


Frequency matters for children's language learning
[for a review, see @ambridge2015ubiquity] 
[@goodman2008]

But - not all frequencies created equal
books vs. speech vs. etc. 
montag 
nation paper

Some of the large differences between frequency distributions are intuitive: for example, "mommy" is quite frequent in child-directed speech (2,260 tokens per million; TPM), yet not so common in children's books (10 TPM), and even more rare in books meant for all ages (2 TPM).
But some of the differences are less intuitive: "of" is quite frequent in books meant for all ages (41,630 TPM), and while still frequent in child-directed speech (5,900 TPM), relatively less so as compared to children's books (20,400 TPM).

SES variation in register, language source

The primary goal of this paper is to examine shared and unique variance in frequency across sources of input, ranging from children's books and movies to child-directed speech and adult books and movies, which we accomplish via principle components analysis (PCA).
After characterizing the structure of the principle components of frequency from different sources, we will investigate how well these components predict children's early word learning, using aggregate CDI data from Wordbank.
Finally, we also examine how well the frequency components predict individual children's word learning in combination with their mother's education, which may be related to how much children are read to at home.


# Method

## Datasets

```{r load-data}
load("../data/merged_word_freqs.Rdata")

aoas <- readRDS("../data/english_(american)_aoa_bydefinition.rds") %>%
  filter(measure=="produces")

load("../data/Charlesworth-unlemmatized-counts.Rdata") 

total_child_tokens <- child_dat %>% group_by(source) %>%
    summarise(word_count = sum(word_count),
              n = n())

total_adult_tokens <- adult_dat %>% group_by(source) %>%
    summarise(word_count = sum(word_count),
              n = n())
```

### Child-directed Speech.
Utterances of child-directed speech (ChS) were extracted from the CHILDES corpus [@macwhinney2000childes], a collection of transcripts of dyadic interactions between caregivers and children of ages ranging from 0 to 12 years ($M=2.9$ years).
After cleaning, the CHILDES corpus yielded a total of `r subset(total_child_tokens, source=="speech")$word_count` tokens across `r subset(total_child_tokens, source=="speech")$n` word types.

### Child-directed books (ChdB).
We used a sample of 98 children's books from Project Gutenberg's open-source database of books that has been used in prior machine learning research on language comprehension [@hill2015goldilocks]. 
These books were published between 1820 and 1922, but include such well-known titles as *The Legend of Sleep Hollow*.
After cleaning, this children's book corpus totals `r subset(total_child_tokens, source=="books")$word_count` tokens across `r subset(total_child_tokens, source=="books")$n` word types.

### Child-directed Media (ChM).
Transcripts from television shows (e.g., from PBS Kids and Nickelodeon) and movies (e.g., *Beauty and the Beast*), including 1,078 movies and 4,309 TV episodes were taken from @charlesworth2021gender (available here: [https://osf.io/kqux5/](https://osf.io/kqux5/).
After cleaning, this children's media corpus totals `r subset(total_child_tokens, source=="tv")$word_count` tokens across `r subset(total_child_tokens, source=="tv")$n` word types.

### Adult-directed Speech (AdS).

After cleaning, the adult-directed speech corpus (AdS) yielded a total of `r subset(total_adult_tokens, source=="speech")$word_count` tokens across `r subset(total_adult_tokens, source=="speech")$n` word types.

### Adult-directed Books (AdB).

The adult-directed book corpus (AdB) is comprised of `r subset(total_adult_tokens, source=="books")$word_count` tokens across `r subset(total_adult_tokens, source=="books")$n` word types.


### Adult-directed Media (AdM).

The adult-directed media corpus (AdM) is comprised of `r subset(total_adult_tokens, source=="TV")$word_count` tokens across `r subset(total_adult_tokens, source=="TV")$n` word types.


## Merging the Corpora

```{r}
#load("../data/Charlesworth-unlemmatized-counts.Rdata") 
# Montag corpus
dd <- ch_freq_smooth %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

# Charlesworth (FB children's books)
dd_novels <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

ddd <- dd %>% select(-prop_booky) %>%
  left_join(dd_novels %>% select(-prop_booky)) %>%
  mutate(Nletters = ifelse(is.na(Nletters), nchar(word), Nletters)) %>%
  left_join(aoas %>% select(-measure))

dddc <- ddd %>% mutate(adult_books = ifelse(is.na(adult_books), 0.07322, adult_books),
                       speech = ifelse(is.na(speech), 10, speech),
                       books = ifelse(is.na(books), 10, books),
                       tv = ifelse(is.na(tv), 10, tv)) %>%
  distinct()

adult_dat <- adult_dat %>% filter(source!="books") %>%
  select(source, word, word_count_norm) %>%
  pivot_wider(id_col = word, names_from = source, values_from = word_count_norm) %>%
  rename(`AdS` = speech,
         `AdM` = TV)

dddc <- dddc %>% 
  rename(`AdB` = adult_books, # Google books
         `ChB` = books, # Facebooks children's books (novels..)
         `ChM` = tv,
         `ChS` = speech) %>% 
  select(-CHILDES) %>% # less well-cleaned version
  left_join(adult_dat)

dddc <- dddc %>% replace_na(list(`AdS` = 0.32,
                                 `AdM` = 0.32))
```


# Results

We focus our analysis on the 674 words from the CDI that we were able to find in at least some of the corpora.

## Simple Age of Acquisition 

```{r, fig.cap="Correlation table of word frequency distributions for CDI words."}

cor(dddc[,c("ChS","ChM","ChB","AdS","AdM","AdB")]) %>%
    papaja::apa_table(digits=2)
```


Significant contributions of children's books, children's speech, adult book frequencies, and number of letters.

```{r, results='asis'}
lm1 <- lm(aoa ~ log(ChS) + log(ChM) + log(ChB) + log(AdS) + log(AdM) + log(AdB) + Nletters, data=dddc)
# R^2 = 0.3433
tab <- data.frame(summary(lm1)$coefficients)
tab$VIF = c(NA, car::vif(lm1))

print(xtable(tab, digits=2), comment=F)
```



But VIFs are >>1: the word frequency distributions are highly correlated, so we will use PCA to disentangle.

## Principal Components of Frequency
(french and english)
PCA, show words that pop out as relatively more speech/booky, kid/adult
predict kuperman aoa? (maybe)

# Results: Predicting Age of Acquisition 
(french and english)
aoa regressions - specific effects of bookiness/kidiness & interactions

# Analysis 3
(english) does relative proportion bookines interaction with momed

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=4.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}
#img <- png::readPNG("figs/walrus.png")
#grid::grid.raster(img)
knitr::include_graphics("../CHILDES_smoothed_norm_freqs_vs_books_TV.pdf")
```



```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "One column image."}

```


## R Plots

You can use R chunks directly to plot graphs. And you can use latex floats in the
fig.pos chunk option to have more control over the location of your plot on the page. For more information on latex placement specifiers see **[here](https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions)**


## Tables

Number tables consecutively; place the table number and title (in
10 point) above the table with one line space above the caption and
one line space below it, as in Table 1. You may float
tables to the top or bottom of a column, set wide tables across both
columns.

You can use the xtable function in the xtable package.

```{r xtable, results="asis"}
n <- 100
x <- rnorm(n)
y <- 2*x + rnorm(n)
out <- lm(y ~ x)

tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2), 
                       caption = "This table prints across one column.")

print(tab1, type="latex", comment = F, table.placement = "H")
```

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
