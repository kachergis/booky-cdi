---
title: "Identifying the distributional sources of children's early vocabulary"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis* (kachergis@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA
    \AND {\large \bf Georgia Loukatou (@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA}

abstract: >
    Children's early vocabulary learning must to a large extent be driven by the prevalence 
    of words: they can't learn a word if they haven't heard it. Indeed, previous research 
    has found that higher word frequency is a good predictor of earlier learning. However,
    despite considerable overlap, word frequency distributions also vary significantly 
    by source: child-directed speech, books, and television have distinct profiles. Children
    receive a mixture of these different frequency distributions, and the ratios of the mixture
    may be predictive of their early word learning. The goal of this paper is to better understand
    the shared and unique variance in these sources of input--in both English and French--and 
    to evaluate how predictive these input frequencies are of children's early word learning.
    
keywords: >
    early language learning; CDI; vocabulary development; word frequency distributions.
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(here)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
require(tidytext)
require(GGally)
library(corrplot)
```

# Introduction

How does speech addressed to children, heard on television, or read in books impact their word learning trajectory? How does this speech relate to adult-directed speech? And does this impact relate to parental socioeconomic status? Children learn language based on ambient linguistic input -- they cannot learn what they haven't heard. However, children's exposure to different words varies depending on how much child-directed speech they hear and how much they are read to. These different language input sources could lead to variance in input speech heard by child in their everyday lives. Input variance has often been interpreted as a function of the families' socioeconomic status [SES; @rowe2018understanding]. Importantly, this variance has been found to relate to children's language development and to be predictive of aspects of word learning [@hoff2003specificity]. 

Input word frequency varies significantly depending on the context. Previous studies have shown that frequency matters for children's word learning [for a review, see @ambridge2015ubiquity], and have observed an association between word frequency in children's language environments and age of acquisition [@goodman2008]. 
For instance, word frequency in books is not the same as frequency in conversational speech [@dawson2021features; @montag2015words]. 

Some differences between frequency distributions are intuitive: "mommy" is quite frequent in child-directed speech, yet not so common in children's books, and even more rare in books meant for all ages. But other differences are less intuitive: "of" is frequent in books meant for all ages, and while still frequent in child-directed speech, relatively less so as compared to children's books. In general, speech--both directed to children, and to adults--contains relatively fewer function words, and tends to score lower on measures of lexical diversity than books [@dawson2021features].

The primary questions of this paper are, first, to examine shared and unique variance in word frequency across different sources of English and French input, ranging from children's books and movies to child-directed speech and even comparing to adult-directed books, movies, and speech, which we accomplish using principle components analysis (PCA).

Second, we investigate how well these components predict English- and French-learning children's early word learning, using aggregate MacArthur-Bates Communicative Development Inventories (CDI) data from Wordbank [@frank2017wordbank]. The CDIs have proven to be reliable and valid indicators of child's language, with high internal consistency and predictive of later language outcomes [@fenson1994variability]. We approximate early word learning with the Age of Acquisition (AoA) prediction paradigm, which consists in predicting each CDI item's mean Age of Acquisition (AoA) -- the mean age (in months) at which 50% of children are expected to know a given word [@braginsky2019consistency; @goodman2008]. 

Third, we examine how well the frequency components predict individual differences in English-learning children's word learning in combination with their mother's education, which may be related to how much children are read to at home. The questions investigated in this paper aim to shed light on the importance of input sources on language development and its interaction with parental SES, would can be useful for informing future interventions.

# Method

## Datasets

```{r load-data}
load("../data/merged_word_freqs.Rdata")
load("../data/merged_word_freqs_fr.Rdata") 
# ToDo: try smoothing with +1 or dropping missing words, save new versions and re-run

aoas <- readRDS("../data/english_(american)_aoa_bydefinition.rds") %>%
  filter(measure=="produces")

load("../data/Charlesworth-unlemmatized-counts.Rdata") 

total_child_tokens <- child_dat %>% group_by(source) %>%
    summarise(word_count = sum(word_count),
              n = n())

total_adult_tokens <- adult_dat %>% group_by(source) %>%
    summarise(word_count = sum(word_count),
              n = n())
              
              
total_child_tokens_fr <- ch_freq_fr %>% 
  select(-adult_speech, -adult_tv, -adult_book) %>%
  pivot_longer(c(child_speech,  child_book, child_tv), names_to = "source", values_to = "word_count") %>%
    filter(!is.na(word_count)) %>%
    group_by(source) %>%
    summarise(word_count = sum(word_count, na.rm=TRUE),
              n = n())

total_adult_tokens_fr <- ch_freq_fr %>% 
  select(-child_speech, -child_tv, -child_book) %>%
  pivot_longer(c(adult_speech,  adult_book, adult_tv), names_to = "source", values_to = "word_count") %>%
    filter(!is.na(word_count)) %>%
    group_by(source) %>%
    summarise(word_count = sum(word_count, na.rm=TRUE),
              n = n())
```


Corpora from different sources are used to identify shared and distinct variance in frequencies, with the help of PCA. 

### Child-directed Speech (ChS).
Utterances of ChS were extracted from the CHILDES corpus [@macwhinney2000childes], a collection of transcripts of interactions between caregivers and children of ages ranging from 0 to 12 years ($M=2.9$ years). After cleaning, the CHILDES English corpus yielded `r format(round(subset(total_child_tokens, source=="speech")$word_count, -3), scientific=F)` tokens across `r format(subset(total_child_tokens, source=="speech")$n, scientific=F)` word types. The French ChS yielded `r format(round(subset(total_child_tokens_fr, source=="child_speech")$word_count, -3), scientific=F)` tokens across `r format(subset(total_child_tokens_fr, source=="child_speech")$n, scientific=F)` word types.

### Child-directed books (ChB).
We used a sample of 98 English children's books from Project Gutenberg's open-source database, previously used in machine learning research on language comprehension [@hill2015goldilocks]. The books were published between 1820 and 1922, but include well-known titles as *The Legend of Sleep Hollow*. We also used 130 popular French children stories accessible in parenting websites (https://fr.hellokids.com/) and 10 French children books from Project Gutenberg. After cleaning, the English ChB corpus totals `r format(round(subset(total_child_tokens, source=="books")$word_count, -3), scientific=F)` tokens across `r format(subset(total_child_tokens, source=="books")$n, scientific=F)` word types, and the French ChB totals `r format(round(subset(total_child_tokens_fr, source=="child_book")$word_count, -3), scientific=F)` tokens across `r format(subset(total_child_tokens_fr, source=="child_book")$n, scientific=F)` word types.

### Child-directed Media (ChM).
Transcripts were extracted from English television shows (e.g., from PBS Kids and Nickelodeon) and movies (e.g., *Beauty and the Beast*), including 1,078 movies and 4,309 TV episodes taken from @charlesworth2021gender (available here: [https://osf.io/kqux5/](https://osf.io/kqux5/). Openly accessible transcripts (https://www.subsynchro.com/) were also extracted from 100 French films directed to children. After cleaning, the English ChM totals `r format(subset(total_child_tokens, source=="tv")$word_count, scientific=F)` tokens across `r format(subset(total_child_tokens, source=="tv")$n, scientific=F)` word types.  The French ChM totals `r format(subset(total_child_tokens_fr, source=="child_tv")$word_count, scientific=F)` tokens across `r format(subset(total_child_tokens_fr, source=="child_tv")$n, scientific=F)` word types. 

### Adult-directed Speech (AdS).
English AdS was obtained from the Switchboard-1 Telephone Speech Corpus (Godfrey & Holliman, 1993), a corpus of transcripts from dyadic telephone conversations. French AdS was obtained from the TCOF corpus [@andre2010mise], the CLAPI corpus [@balthasar2005plateforme] and the CFPP corpus [@branca2012discours]. After cleaning, the English AdS yielded `r format(round(subset(total_adult_tokens, source=="speech")$word_count, -3), scientific=F)` tokens across `r format(subset(total_adult_tokens, source=="speech")$n, scientific=F)` word types. The French AdS yielded `r format(subset(total_adult_tokens_fr, source=="adult_speech")$word_count, scientific=F)` tokens across `r format(subset(total_adult_tokens_fr, source=="adult_speech")$n, scientific=F)` word types.

### Adult-directed Books (AdB).
The English AdB is comprised of `r format(round(subset(total_adult_tokens, source=="books")$word_count, -3), scientific=F)` tokens across `r format(subset(total_adult_tokens, source=="books")$n, scientific=F)` word types. The French AdB is comprised of books taken from the 1999 Association de Bibliophiles Universels, an open-source database of french books. After cleaning, it yielded `r format(subset(total_adult_tokens_fr, source=="adult_book")$word_count, scientific=F)` tokens across `r format(subset(total_adult_tokens_fr, source=="adult_book")$n, scientific=F)` word types.


### Adult-directed Media (AdM).
The French AdM is comprised of `r format(round(subset(total_adult_tokens, source=="TV")$word_count, -3), scientific=F)` tokens across `r format(subset(total_adult_tokens, source=="TV")$n, scientific=F)` word types. The French AdM corpus is comprised of `r format(subset(total_adult_tokens_fr, source=="adult_tv")$word_count, scientific=F)` tokens across `r format(subset(total_adult_tokens_fr, source=="adult_tv")$n, scientific=F)` word types, after cleaning openly accessible movie subtitles (https://www.subsynchro.com/) from 100 films.

## Merging the Corpora

```{r}
#load("../data/Charlesworth-unlemmatized-counts.Rdata") 
# Montag corpus
dd <- ch_freq_smooth %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

# Charlesworth (FB children's books)
dd_novels <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

ddd <- dd %>% select(-prop_booky) %>%
  left_join(dd_novels %>% select(-prop_booky)) %>%
  mutate(Nletters = ifelse(is.na(Nletters), nchar(word), Nletters)) %>%
  left_join(aoas %>% select(-measure))

dddc <- ddd %>% mutate(adult_books = ifelse(is.na(adult_books), 0.07322, adult_books),
                       speech = ifelse(is.na(speech), 10, speech),
                       books = ifelse(is.na(books), 10, books),
                       tv = ifelse(is.na(tv), 10, tv)) %>%
  distinct()

adult_dat <- adult_dat %>% filter(source!="books") %>%
  select(source, word, word_count_norm) %>%
  pivot_wider(id_col = word, names_from = source, values_from = word_count_norm) %>%
  rename(`AdS` = speech,
         `AdM` = TV)

dddc <- dddc %>% 
  rename(`AdB` = adult_books, # Google books
         `ChB` = books, # Facebooks children's books (novels..)
         `ChM` = tv,
         `ChS` = speech) %>% 
  select(-CHILDES) %>% # less well-cleaned version
  left_join(adult_dat)

dddc <- dddc %>% replace_na(list(`AdS` = 0.32,
                                 `AdM` = 0.32))
                                 
dd_fr <- ch_freq_smooth_fr %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech, -prop_booky) %>%
  mutate(Nletters = nchar(token)) %>%
  mutate(adult_book = ifelse(is.na(adult_book), 10, adult_book),
         adult_speech = ifelse(is.na(adult_speech), 10, adult_speech),
         child_book = ifelse(is.na(child_book), 10, child_book),
         adult_tv = ifelse(is.na(adult_tv), 10, adult_tv),
         child_tv = ifelse(is.na(child_tv), 10, child_tv),
         child_speech = ifelse(is.na(child_speech), 10, child_speech)) %>%
         distinct()%>% 
         filter(!lexical_class=="other")

dddc_fr <- dd_fr %>% 
  rename(`AdB` = adult_book,  
         `ChB` = child_book, 
         `AdM` = adult_tv,
         `AdS` = adult_speech,
         `ChS` = child_speech,
         `ChM` = child_tv)                                  
```

Children's early word learning data is drawn from the CDIs [@fenson2007], aggregated in the Wordbank database [@frank2017wordbank] (data from X kids for American English and X kids for French French). CDIs are parental reports on their children’s lexical development, proven to be reliable indicators of a child’s language. CDIs are survey instruments, where parents mark whether their child (age ranges 8-15 and 16-30 months old) understands or produces particular words out of a list of several hundred words. All word frequencies were normalized to number of tokens per million (TPM). We focus our analysis on the 674 words from the English CDI that we were able to find in at least some of the corpora, and 470 words from the French CDI (for French, words were matched to related words in corpora via a stemmer). For any CDI words that failed to appear in a given corpus, we replaced the missing word's frequency with a normalized count of 10 TPM, or the minimum normalized frequency for that distribution, whichever was smaller.


# Results


## Cross-corpus Frequency Correlations (Question 1)


Figure 1 shows the correlations in word frequency between different corpus sources for the matched CDI words (left: English, right: French). 

```{r, fig.env = "figure", fig.pos = "H", fig.align='center', out.width = "\\textwidth", set.cap.width=T, num.cols.cap=1, fig.cap = "Word frequency correlations between different corpus sources for the matched CDI words in English (left) and French (right)."}
res_en=cor(dddc[,c("ChS","ChM","ChB","AdS","AdM","AdB")])
res_fr=cor(dddc_fr[,c("ChS","ChM","ChB","AdS","AdM","AdB")])

# save figures
generate_fig1 <- function() {
pdf("figs/en_freq_cor.pdf", width=3.1, height=3.1)
corrplot(res_en, type="upper", method="circle", bg="snow", number.cex = .8, tl.cex=.9, #col=corcol,
              addCoef.col = "white", number.digits = 2, tl.col="black", tl.srt=20, cl.pos="n")
dev.off()

pdf("figs/fr_freq_cor.pdf", width=3.1, height=3.1)
corrplot(res_fr, type="upper", method="circle", bg="snow", number.cex = .8, tl.cex=.9, #col=corcol,
              addCoef.col = "white", number.digits = 2, tl.col="black", tl.srt=20, cl.pos="n") 
dev.off()
}

#generate_fig1()

knitr::include_graphics("figs/corpus_freq_cors.pdf")

#cor(dddc[,c("ChS","ChM","ChB","AdS","AdM","AdB")]) %>%
#    papaja::apa_table(digits=2, placement='h',
#                      caption="Correlations in word frequency between different sources for CDI words (English)")

#cor(dddc_fr[,c("ChS","ChM","ChB","AdS","AdM","AdB")]) %>%
#    papaja::apa_table(digits=2, placement='h',
#                      caption="Correlations in word frequency between different sources for CDI words (French).")
```

For both languages, there are both register and source effects. 
Unsurprisingly, there were strong correlations across these different corpora.
This multicollinearity makes it unwise to include these raw distributions in a regression, as the results would likely be unstable.[^1]
We thus turned to PCA to disentangle these correlated distributions and to understand their interrelations. 
We used PCA to examine the principal components of the six log-scaled word frequency distributions ([adult- vs. child-directed] x [speech, books, media]).

[^1]: We verified that this was the case by running a regression predicting AoA with log(word frequency) from each of the six distributions as predictors. The Variance Inflation Factor (VIF) for every distribution was $>>1$

Table 1 shows the standard deviation (Std Dev) and proportion of variance explained, both individually (Prop Var) and cumulatively (Cum Prop Var) by the principal components (PC1-PC6) for both languages. PC1 already explains the bulk the variance (89% for English and for French), and PC2-PC4 each only capture an additional 2-4% of the variance. In total, the first four components captured >98% of the variance for both languages.

Table 2 shows the eigenvectors of the principal components (PC1-PC6) in relation to the original six frequency distributions for English and French The first PC captures shared variance between all frequency sources, representing words that are high or low frequency across registers and sources. This component captures for example that 'the' is frequent in spoken and written text and in speech to adults and children.



```{r}
pc_bsa <- prcomp(log(dddc[,c("ChM","ChB","ChS",
                             "AdM","AdB","AdS")])) # add Montag separately?
#eigenvalues = pc_bsa$sdev^2 # only first 2 PCs have eigenvalues >1
pc_imp <- summary(pc_bsa)$importance 
row.names(pc_imp) = c("StdDev Eng","PropVar Eng","CumPropVar Eng")

pc_bsa_fr <- prcomp(log(dddc_fr[,c("ChM","ChB","ChS",
                             "AdM","AdB","AdS")])) # add Montag separately?
#eigenvalues = pc_bsa$sdev^2 # only first 2 PCs have eigenvalues >1
pc_imp_fr <- summary(pc_bsa_fr)$importance 
row.names(pc_imp_fr) = c("StdDev Fr","PropVar Fr","CumPropVar Fr")

table_1<-rbind(pc_imp, pc_imp_fr)

table_1 %>%
    papaja::apa_table(digits=2, caption="Importance of components from PCA for English (above) and French (below).", placement='h')
```





Tables 5 and 6 show the correlation of each frequency distribution with the CDI items' loadings on each principal component for English and French, respectively.

For English, it is clear that PC1 captures overall frequency, but loads more strongly on the adult distributions (-.47 to -.52).
PC2 (3.8% of variance) mostly captures child-directed speech (0.65), especially differentiating it from adult-directed books and media (-.47, -.45).
PC3 (3.1% of variance) captures adult-directed speech (.79), particularly from child-directed distributions.
PC4 (2.2% of variance) captures the similarity of child- and adult-directed media (.40, .48), distinguishing them from child- and adult-directed books (-.63, -.44).
PC5 (1.1% of variance) mostly captures child-directed books (0.53), distinguishing it from child-directed speech (-.60) and adult-directed books (-.50).
PC6 (<1% of variance) captures child-directed media (.73), distinguishing it in particular from adult-directed media (-.52).
In summary, the principal components align surprisingly well with particular dimensions of the English frequency distributions: PC1 with overall adult-directed frequency, PC2 with child-directed speech, PC3 with adult-directed speech, PC4 with media vs. books, PC5 with child-directed books vs. speech, and PC6 with adult- vs. child-directed media.
Although even PC5 and PC6 seem interpretable, given limited space and the fact that they account for very little variance ($\sim1$%) we will focus on PC1-PC4 for the rest of our analyses.

For French, we see a similar pattern of findings, although the order of the most important principal components is not quite the same.
As seen in Table 6, the first PC for the French corpora captures less of the variance than for the English corpora, and remaining PCs capture more -- even PC5 and PC6 each capture 3%, compared to 1% in the English PCA.





```{r}
#pc_bsa$rotation  * -1
ddbsa <- cbind(dddc, data.frame(pc_bsa$x))
pc_bsa$rotation %>%
    papaja::apa_table(digits=2, caption="Principal components' rotation in the original coordinate system.", placement='h')
    
#pc_bsa$rotation  * -1
ddbsa_fr <- cbind(dddc_fr, data.frame(pc_bsa_fr$x))
pc_bsa_fr$rotation %>%
    papaja::apa_table(digits=2, caption="Principal components' rotation in the original coordinate system. (French)", placement='h')    
```



## Age of Acquisition Regression (Question 2)


We attempted a simple regression predicting each CDI word's mean Age of Acquisition (AoA) -- the mean age (in months) at which 50% of children are expected to know a given word [@braginsky2019consistency]. We also included the number of letters as a predictor (Nletters) to help control for the overall difficulty of each word.
<!-- Table 2 below displays the estimated coefficients of this regression, showing significant contributions of all child-directed distributions (speech, books, and media), and of adult speech frequencies, as well as the number of letters.
However, the variance inflation factor (VIF) values for all of the frequency distributions are all $>>1$ (and many $>5$ or $>10$), indicating that these variables show strong multicollinearity which may compromise the reliability of the regression results. 
-->

```{r raw-logfreq-aoa-regression, include=F, results='asis'}
lm1 <- lm(aoa ~ log(ChS) + log(ChM) + log(ChB) + log(AdS) + log(AdM) + log(AdB) + Nletters, data=dddc)
# R^2 = 0.3433
tab <- data.frame(summary(lm1)$coefficients)
tab$VIF = c(NA, car::vif(lm1))
names(tab) = c("Beta","SE","t-val","p-val","VIF")
#tab %>% kable(digits=2)
print(xtable(tab, digits=2,
             caption = "Coefficients for English corpora frequencies predicting AoA.",
             table.placement = "H"), comment=F)

lm1_fr <- lm(aoa ~ log(ChS) + log(ChM) + log(ChB) + log(AdS) + log(AdM) + log(AdB) + Nletters, data=dddc_fr)
# R^2 = 0.3433
tab_fr <- data.frame(summary(lm1_fr)$coefficients)
tab_fr$VIF = c(NA, car::vif(lm1_fr))
names(tab_fr) = c("Beta","SE","t-val","p-val","VIF")
print(xtable(tab_fr, digits=2,
             caption = "Coefficients for French corpora frequencies predicting AoA.",
             table.placement = "H"), comment=F)

```





```{r, include=F}
## Correlation of PCA with original Frequency Distributions
cor(ddbsa[,c("ChM","ChB","ChS","AdM","AdB","AdS")], ddbsa[,paste0("PC",1:6)]) %>%
  papaja::apa_table(digits=2, caption="Correlation of PC loadings with frequency distributions.", placement='h')
```

## PCA-based Prediction of Age of Acquisition 

Figure 1 shows the loadings of English CDI items on PC1-PC4 vs. the average age of acquisition (AoA; in months).

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=8.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Principal components vs. age of acquisition, by lexical class."}
#knitr::include_graphics("../CHILDES_smoothed_norm_freqs_vs_books_TV.pdf")

p1 <- ggplot(ddbsa, aes(x = PC1, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC1 < -5 | PC1 > 5), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p2 <- ggplot(ddbsa, aes(x = PC2, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC2 < -2 | PC2 > 2), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p3 <- ggplot(ddbsa, aes(x = PC3, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC3 < -1 | PC3 > 1), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p4 <- ggplot(ddbsa, aes(x = PC4, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC4 < -1 | PC4 > 1), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

ggarrange(p1, p2, p3, p4, nrow=4, ncol=1, legend = "none")
```

<!--
## Regularized Regression on PCA Loadings

Next we used the CDI items' PCA loadings in lieu of the frequency distributions to predict AoA.
First, we did a L1-regularized (i.e., LASSO) regression predicting AoA with all of the principal components, to test which of the PCs should be included in the regression with lexical class.
We used cross-validation to find the $\lambda$ value (penalty for outsized coefficients) that minimized mean-squared error of the test set, resulting in $\lambda=0.006$ for English, a small value which will result in coefficients that would be quite close to those obtained in ordinary least squares regression (i.e., when $\lambda=0$).
Using this best-fitting $\lambda$, this model had $R^2 = 0.32$ in English, and the estimated coefficients were non-zero for all of the principal components.
For English, there were negative coefficients for PC1 ($\beta=-0.17$), PC2 ($\beta=-1.13$), PC4 ($\beta=-1.32$), and PC6 ($\beta=-0.34$), indicating that higher values on these PCs predict earlier AoAs -- especially for PC2 and PC4, which mostly capture child-directed speech (PC2) and media (PC4). There were positive coefficients for PC3 ($\beta=0.88$) and PC5 ($\beta=1.95$), indicating that higher values of these PCs predict later AoAs, which is reasonable, as PC3 is associated with adult-directed speech, and PC5 captures child-directed books. This is still reasonable, since even child-directed books contain more function words than speech, which tend to be later-learned. Moreover, children receive a relatively small fraction of their daily input from books -- perhaps 20 minutes/day, in contrast with several hours of child-directed speech and overheard adult-directed speech. 

For French, this model had $R^2 = 0.12$, and the estimated coefficients were non-zero for all of the principal components. There were negative coefficients for PC1 ($\beta=-0.05$) and PC5 ($\beta=-0.13$), indicating that higher values on these PCs predict earlier AoAs. There were positive coefficients for PC2 ($\beta=0.14$), PC3 ($\beta=1.63$), PC4 ($\beta=0.18$) and PC6 ($\beta=0.82$) indicating that higher values of these PCs predict later AoAs.
-->

```{r, pca-regression, include=F, results='asis'}
ddbsa$lexical_class <- fct_relevel(ddbsa$lexical_class, "nouns")
pca_reg1 <- lm(aoa ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data=ddbsa) 
#summary(pca_reg1)
# R^2 = .321
pca_tab1 <- data.frame(pca_reg1$coefficients) 

print(xtable(pca_tab1, digits=2,
             caption = "Regression predicting AoA with PCs.",
             table.placement = "H"), comment=F)
             
ddbsa_fr$lexical_class <- fct_relevel(ddbsa_fr$lexical_class, "nouns") 
pca_reg1_fr <- lm(aoa ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data=ddbsa_fr) 
#summary(pca_reg1)
# R^2 = .321
pca_tab1_fr <- data.frame(pca_reg1_fr$coefficients) 

print(xtable(pca_tab1_fr, digits=2,
             caption = "Regression predicting AoA with French PCs on function_words, verbs, adjectives, nouns)",
             table.placement = "H"), comment=F)             
```

```{r, lasso-pca-regression, include=F, results='asis'}
library(glmnet)

PCs <- data.matrix(ddbsa[,paste0("PC",1:6)])

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(PCs, ddbsa$aoa, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min # 0.006 (very close to linear regression)
#best_lambda <- cv_model$lambda.1se # 0.323 1SE above MSE

#find coefficients of best model
best_model <- glmnet(PCs, ddbsa$aoa, alpha = 1, lambda = best_lambda)
lg_tab <- as.matrix(coef(best_model))
print(xtable(lg_tab, digits=2,
             caption = "LASSO AoA regression coefficients.",
             table.placement = "H"), comment=F)
             
PCs_fr <- data.matrix(ddbsa_fr[,paste0("PC",1:6)])

#perform k-fold cross-validation to find optimal lambda value
#cv_model_fr <- cv.glmnet(PCs_fr, ddbsa_fr$aoa, alpha = 1)

#find optimal lambda value that minimizes test MSE
#best_lambda_fr <- cv_model_fr$lambda.min # 0.006 (very close to linear regression)
#best_lambda <- cv_model$lambda.1se # 0.323 1SE above MSE

#find coefficients of best model
#best_model_fr <- glmnet(PCs, ddbsa_fr$aoa, alpha = 1, lambda = best_lambda_fr)
#lg_tab_fr <- as.matrix(coef(best_model_fr))
#print(xtable(lg_tab_fr, digits=2,
#             caption = "LASSO AoA regression coefficients for French.",
#             table.placement = "H"), comment=F)
```


```{r, include=F, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.3, fig.height=3.3, set.cap.width=T, num.cols.cap=1, fig.cap = "Test MSE for different lambda values in the cross-validated lasso regression. The minimum MSE is achieved by the leftmost dotted line, while the rightmost dotted line shows the lambda that achieves MSE $<1$ standard error above this minimum."}

#produce plot of test MSE by lambda value
plot(cv_model) 
```

Given that past research has found that lexical class strongly modulates influences of word frequency, we next examined the interaction of lexical class (LC) with PC1 - PC6. 
To determine if the inclusion of all PCs was justified, we ran a series of ANOVAs building up from PC1 to PC6--in decreasing order of the variance they accounted for in the PCA.
Thus, the R syntax for the sequence of regressions was `AoA`$\sim$`PC1*LC`, `AoA`$\sim$`(PC1+PC2)*LC`, ..., `AoA`$\sim$`(PC1+PC2+PC3+PC4+PC5+PC6)*LC`, with noun as the baseline LC.
The more complex model was always significantly preferred, including up to the inclusion of PC6 ($R^2 = .584$), although for ease of presentation we only show the results of the regression with up to PC4.
Table 9 shows the results of this regression, which yielded $R^2 = .538$.
PC1 and PC3 had significant positive coefficients, while PC2 and PC4 had significant negative coefficients.
PC1 had significant interactions with all levels of lexical class, with negative coefficients.
PC2 had significant interactions with .. (positive), and with .. (negative).
Interactions of PC3 and PC4 with LC were not significant.
[better to just show a ggeffects graph?]

```{r, results='asis'}
# could try adding in PCs one by one..

ddbsa <- ddbsa %>%
  mutate(LC = case_when(lexical_class=="nouns" ~ "noun",
                        lexical_class=="verbs" ~ "verb",
                        lexical_class=="adjectives" ~ "adj",
                        lexical_class=="function_words" ~ "func",
                        lexical_class=="other" ~ "other",
                        TRUE ~ as.character(lexical_class)))
ddbsa$LC <- fct_relevel(ddbsa$LC, "noun")

pca_lc1 <- lm(aoa ~ PC1 * LC, data=ddbsa) # R^2=.26
pca_lc2 <- lm(aoa ~ (PC1 + PC2) * LC, data=ddbsa) # R^2=.47
pca_lc3 <- lm(aoa ~ (PC1 + PC2 + PC3) * LC, data=ddbsa)
pca_lc4 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4) * LC, data=ddbsa)
pca_lc5 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4 + PC5) * LC, data=ddbsa) # 0.576
pca_lc6 <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4 + PC5 + PC6) * LC, data=ddbsa)

ttt <- anova(pca_lc1, pca_lc2) # 2***
ttt <- rbind(ttt, anova(pca_lc2, pca_lc3)[2,]) # 3***
ttt <- rbind(ttt, anova(pca_lc3, pca_lc4)[2,]) # 4***
ttt <- rbind(ttt, anova(pca_lc4, pca_lc5)[2,]) # 5***
ttt <- rbind(ttt, anova(pca_lc5, pca_lc6)[2,]) # 6*

#summary(pca_lc6) # R^2 = .584
#summary(pca_lc4) # R^2 = .538
pca_tab2 <- data.frame(summary(pca_lc4)$coefficients)
#car::vif(pca_lc_reg)
names(pca_tab2) = c("Beta","SE","t-val","p-val")

print(xtable(pca_tab2, digits=2,
             caption = "Regression predicting English CDI AoAs with PCs and lexical class.",
             table.placement = "H"), comment=F)
```

## Combining distributions with demographic data

Past research has found that young children from higher-SES households tend to have larger vocabulary.
Parents with higher-SES tend to also report reading more to their young children than parents with lower SES. 
Together, this suggests that the vocabulary composition of children from higher-SES households may be better predicted by the word frequencies seen in child-directed books, rather than those from child-directed speech. 
To test this idea, we did an exploratory logistic regression using the first four PCs to predict the number of children in Wordbank who produce or don't produce each item, along with interactions of mother's education and children's age.
<!--The R syntax for the formula is `cbind(total_producing, total_not_producing) ~ age * mother_ed * (PC1 + PC2 + PC3 + PC4)`.-->
American English data from Wordbank contained 2,776 CDI:WS administrations with mother's education (coded: -1 for no more than secondary education (N=547), 0 for some/all college (N=1483), and 1 for at least some graduate school (N=746)).
There were significant main effects of age, mother's education, and PC1-PC4 (all $p<.001$).
There were significant interactions of age with mother's education ($p<.001$), PC2 ($p<.001$), and PC4 ($p=.009$).
There were significant interactions of mother's education with PC1-PC3 (all $p<.001$).
[how to show these effects?]

```{r get-wordbank-data, eval=F}
require(wordbankr)
en_demo <- get_administration_data(language = "English (American)", form = "WS")

en_long_ws <- get_instrument_data(language="English (American)", form = "WS") %>%
  mutate(produces = as.numeric(value == "produces")) 

en_ws <- get_item_data(language="English (American)", form="WS") %>%
  filter(type=="word") %>% select(-complexity_category, -item_id)

en_long_ws <- en_long_ws %>% left_join(en_ws %>% select(num_item_id, definition)) %>%
  filter(!is.na(definition))

save(en_ws, en_demo, en_long_ws, file=here("data/en_ws.Rdata"))

# get French WS data
fr_demo <- get_administration_data(language = "French (French)", form = "WS")

fr_long_ws <- get_instrument_data(language="French (French)", form = "WS") %>%
  mutate(produces = as.numeric(value == "produces")) 

fr_ws <- get_item_data(language="French (French)", form="WS") %>%
  filter(type=="word") %>% select(-complexity_category, -item_id)

fr_long_ws <- fr_long_ws %>% left_join(fr_ws %>% select(num_item_id, definition)) %>%
  filter(!is.na(definition))

table(fr_demo$mom_ed) # small amount of French data with mom_ed: 46 secondary, 112 college, 64 grad

save(fr_ws, fr_demo, fr_long_ws, file=here("data/fr_ws.Rdata"))
```


```{r en-momed-regression, results='asis'}
load(here("data/en_ws.Rdata"))

en_mom_ed = table(en_demo$mom_ed)
# None = 0, Primary = 8, Some Secondary = 123

en_dat <- en_long_ws %>% left_join(en_demo) %>%
  filter(!is.na(mom_ed)) %>%
  mutate(mom_ed_num = case_when(as.numeric(mom_ed)<5 ~ -1, # secondary or less
                                as.numeric(mom_ed)<7 ~ 0, # some college / college
                                TRUE ~ 1)) %>%  # some graduate / graduate
  mutate(MotherEd = case_when(as.numeric(mom_ed)<5 ~ "Secondary", # secondary or less
                                as.numeric(mom_ed)<7 ~ "College", # some college / college
                                TRUE ~ "Graduate")) %>%
  group_by(age, MotherEd, definition) %>%
  summarise(total_prod = sum(produces, na.omit=T), 
            total_not_prod = sum(produces==0, na.omit=T))

en_dat <- en_dat %>% left_join(ddbsa %>% select(definition, Nletters, PC1, PC2, PC3, PC4, PC5, PC6)) %>%
  mutate(age_sc = scale(age, center=T, scale=F)) %>% 
  filter(!is.na(PC1)) # get rid of the 10 CDI phrases we don't match (give me five!, pet's name etc.)
# unique(en_dat[is.na(en_dat$PC1),]$definition)

# scale/center the PC vars?

en_dat$MotherEd = factor(en_dat$MotherEd, levels=c("Secondary","College","Graduate"))

# whenever we center age (age_sc), matrix is rank deficient and age_sc main effect doesn't appear... 
en2 <- glmer(cbind(total_prod, total_not_prod) ~ age * MotherEd * (PC1 + PC2) + (1|definition), family = binomial(logit), nAGQ=0, data=en_dat) 
# PC1 +.03, PC2 +.19, mom_ed +.06, PC1:mom_ed +.003, PC2:mom_Ed +.008

en3 <- glmer(cbind(total_prod, total_not_prod) ~ age * MotherEd * (PC1 + PC2 + PC3) + (1|definition), family = binomial(logit), nAGQ=0, data=en_dat)
# PC1 +.03, PC2 +.19, PC3 -.19

en4 <- glmer(cbind(total_prod, total_not_prod) ~ age * MotherEd * (PC1 + PC2 + PC3 + PC4) + (1|definition), family = binomial(logit), nAGQ=0, data=en_dat)


#anova(en2, en3)
#anova(en3, en4)

#summary(en2)
#summary(en3)
mom_ed_tab <- summary(en4)$coefficients

colnames(mom_ed_tab) = c("Beta","SE","t-val","p-val")

print(xtable(mom_ed_tab, digits=2,
             caption = "Regression coefficients for predicting English CDI items using PCs and mother's education.",
             table.placement = "H"), comment=F)
```



# Discussion

We set out to investigate the sources of linguistic input that children may experience using word frequency distributions garnered from child-directed and adult-directed corpora of speech, books, and media (TV and movies).
In both English and French corpora, we found the principal components (PCs) of these distributions, and described how these PCs capture variation both in adult- vs. child-directedness, as well as between modalities (e.g., books and speech).
Moreover, in both English and French we found that multiple components are predictive of children's age of acquisition of words from the CDI. 

(Although a reason for the differences between the English and French findings may be that French corpora were composed by several smaller ones, due to the lack of big accessible corpora online, and the slight different use for the corpora e.g. small stories in French, large books in English).

Finally, we used English Wordbank data to examine how well the frequency components combine with mother's education--a measure of SES that has been found in the past to be positively related to early word learning, and to more child-directed reading--to predict children's early word learning.
This analysis revealed significant contributions of multiple PCs, as well as the interaction of mother's education with PC1 (overall frequency), PC2 (child-directed speech), and PC3 (adult-directed speech), but not of the child-directed books component (PC4).
A target for future research is to predict individual children's learning of particular words using these principal components, in combination with parent-reported measures of how much time their child spends daily receiving input from each of the input sources ([adult- vs. child-directed] x [books, media, and speech]).

In conclusion, despite the overwhelming similarity of word frequency distributions from different sources, we have shown that these distributions show systematic variation--at least in English and in French, which can predict some of the variation in children's early word learning.
By better understanding the similarity and differences between word frequencies children experience in different contexts, future research in this vein holds the promise to predict individual differences in children's early word learning on the basis of their daily routines.

# Acknowledgements

[Redacted for anonymous review.]
<!--This research was funded in part by (Georgia's grant).
We thank members of the Language and Cognition lab for their feedback.-->


# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
