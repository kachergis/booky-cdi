---
title: "Identifying the distributional sources of children's early vocabulary"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis* (kachergis@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA
    \AND {\large \bf Georgia Loukatou (@stanford.edu)} \\ Department of Psychology, Stanford University \\ Stanford, CA 94305 USA}

abstract: >
    Children's early vocabulary learning must to a large extent be driven by the prevalence 
    of words: they can't learn a word if they haven't heard it. Indeed, previous research 
    has found that higher word frequency is a good predictor of earlier learning. However,
    despite considerable overlap, word frequency distributions also vary significantly 
    by source: child-directed speech, books, and television have distinct profiles. Children
    receive a mixture of these different frequency distributions, and here we attempt to 
    discern the varying impact of these input sources on children's early word learning.
    
keywords: >
    early language learning; CDI; vocabulary development.
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(here)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
require(tidytext)
require(GGally)
```

# Introduction

Word usage varies significantly by context: you don't speak to your boss the same way as you do to your dog.


Frequency matters for children's language learning
[for a review, see @ambridge2015ubiquity] 
[@goodman2008]

But - not all frequencies created equal
books vs. speech vs. etc. 
montag 
nation paper

Some of the large differences between frequency distributions are intuitive: for example, "mommy" is quite frequent in child-directed speech (2,260 tokens per million; TPM), yet not so common in children's books (10 TPM), and even more rare in books meant for all ages (2 TPM).
But some of the differences are less intuitive: "of" is quite frequent in books meant for all ages (41,630 TPM), and while still frequent in child-directed speech (5,900 TPM), relatively less so as compared to children's books (20,400 TPM).

SES variation in register, language source

The primary goal of this paper is to examine shared and unique variance in frequency across sources of input, ranging from children's books and movies to child-directed speech and adult books and movies, which we accomplish via principle components analysis (PCA).
After characterizing the structure of the principle components of frequency from different sources, we will investigate how well these components predict children's early word learning, using aggregate CDI data from Wordbank.
Finally, we also examine how well the frequency components predict individual children's word learning in combination with their mother's education, which may be related to how much children are read to at home.

introduce CDI [@fenson2007] and AoA analysis [@braginsky2019consistency]

# Method

## Datasets

```{r load-data}
load("../data/merged_word_freqs.Rdata")

aoas <- readRDS("../data/english_(american)_aoa_bydefinition.rds") %>%
  filter(measure=="produces")

load("../data/Charlesworth-unlemmatized-counts.Rdata") 

total_child_tokens <- child_dat %>% group_by(source) %>%
    summarise(word_count = sum(word_count),
              n = n())

total_adult_tokens <- adult_dat %>% group_by(source) %>%
    summarise(word_count = sum(word_count),
              n = n())
```


### Child-directed Speech.
Utterances of child-directed speech (ChS) were extracted from the CHILDES corpus [@macwhinney2000childes], a collection of transcripts of dyadic interactions between caregivers and children of ages ranging from 0 to 12 years ($M=2.9$ years).
After cleaning, the CHILDES corpus yielded a total of `r subset(total_child_tokens, source=="speech")$word_count` tokens across `r subset(total_child_tokens, source=="speech")$n` word types.

### Child-directed books (ChdB).
We used a sample of 98 children's books from Project Gutenberg's open-source database of books that has been used in prior machine learning research on language comprehension [@hill2015goldilocks]. 
These books were published between 1820 and 1922, but include such well-known titles as *The Legend of Sleep Hollow*.
After cleaning, this children's book corpus totals `r subset(total_child_tokens, source=="books")$word_count` tokens across `r subset(total_child_tokens, source=="books")$n` word types.

### Child-directed Media (ChM).
Transcripts from television shows (e.g., from PBS Kids and Nickelodeon) and movies (e.g., *Beauty and the Beast*), including 1,078 movies and 4,309 TV episodes were taken from @charlesworth2021gender (available here: [https://osf.io/kqux5/](https://osf.io/kqux5/).
After cleaning, this children's media corpus totals `r subset(total_child_tokens, source=="tv")$word_count` tokens across `r subset(total_child_tokens, source=="tv")$n` word types.

### Adult-directed Speech (AdS).
Adult-directed speech was obtained from the Switchboard-1 Telephone Speech Corpus (Godfrey & Holliman, 1993), a corpus of transcripts from dyadic telephone conversations in which 543 adult speakers were assigned to discuss a randomly-assigned topic.
After cleaning, the adult-directed speech corpus (AdS) yielded a total of `r subset(total_adult_tokens, source=="speech")$word_count` tokens across `r subset(total_adult_tokens, source=="speech")$n` word types.

### Adult-directed Books (AdB).
The adult-directed book corpus (AdB) is comprised of `r subset(total_adult_tokens, source=="books")$word_count` tokens across `r subset(total_adult_tokens, source=="books")$n` word types.


### Adult-directed Media (AdM).
The adult-directed media corpus (AdM) is comprised of `r subset(total_adult_tokens, source=="TV")$word_count` tokens across `r subset(total_adult_tokens, source=="TV")$n` word types.


## Merging the Corpora

```{r}
#load("../data/Charlesworth-unlemmatized-counts.Rdata") 
# Montag corpus
dd <- ch_freq_smooth %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

# Charlesworth (FB children's books)
dd_novels <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

ddd <- dd %>% select(-prop_booky) %>%
  left_join(dd_novels %>% select(-prop_booky)) %>%
  mutate(Nletters = ifelse(is.na(Nletters), nchar(word), Nletters)) %>%
  left_join(aoas %>% select(-measure))

dddc <- ddd %>% mutate(adult_books = ifelse(is.na(adult_books), 0.07322, adult_books),
                       speech = ifelse(is.na(speech), 10, speech),
                       books = ifelse(is.na(books), 10, books),
                       tv = ifelse(is.na(tv), 10, tv)) %>%
  distinct()

adult_dat <- adult_dat %>% filter(source!="books") %>%
  select(source, word, word_count_norm) %>%
  pivot_wider(id_col = word, names_from = source, values_from = word_count_norm) %>%
  rename(`AdS` = speech,
         `AdM` = TV)

dddc <- dddc %>% 
  rename(`AdB` = adult_books, # Google books
         `ChB` = books, # Facebooks children's books (novels..)
         `ChM` = tv,
         `ChS` = speech) %>% 
  select(-CHILDES) %>% # less well-cleaned version
  left_join(adult_dat)

dddc <- dddc %>% replace_na(list(`AdS` = 0.32,
                                 `AdM` = 0.32))
```

All word frequencies were normalized to number of tokens per million (TPM).
We focus our analysis on the 674 words from the CDI that we were able to find in at least some of the corpora.
We were unable to match 6 CDI items in any of the corpora, including "babysitter's name", "child's own name", ...
For any CDI words that failed to appear in a given corpus, we replaced the missing word's frequency with a normalized count of 10 TPM, or the minimum normalized frequency for that distribution, whichever was smaller.
`r sum(is.na(ddd$books))` missing word was replaced in each of ChB, ChM, and ChS, while `r sum(is.na(ddd$adult_books))` missing words were replaced in AdB. ...

# Results


## Cross-corpus Frequency Correlations 

Table 1 shows the cross-corpus word frequency correlations for the `r nrow(dddc)` CDI words.

```{r}
cor(dddc[,c("ChS","ChM","ChB","AdS","AdM","AdB")]) %>%
    papaja::apa_table(digits=2, placement='h',
                      caption="Correlation table of word frequency distributions for CDI words.")
```


## Age of Acquisition Regression

Despite the strong correlations of word frequency across these different corpora, we will attempt a simple regression predicting each CDI word's mean Age of Acquisition (AoA) -- the mean age (in months) at which 50% of children are expected to know a given word [@braginsky2019consistency].
We also include the number of letters as a predictor (Nletters) to help control for the overall difficulty of each word.
Table 2 below displays the estimated coefficients of this regression, showing significant contributions of all child-directed distributions (speech, books, and media), and of adult speech frequencies, as well as the number of letters.
However, the variance inflation factor (VIF) values for all of the frequency distributions are all $>>1$ (and many $>5$ or $>10$), indicating that these variables show strong multicollinearity which may compromise the reliability of the regression results.
Thus, we turned to principal components analysis (PCA) to disentangle these correlated distributions and to understand their interrelations.

```{r, results='asis'}
lm1 <- lm(aoa ~ log(ChS) + log(ChM) + log(ChB) + log(AdS) + log(AdM) + log(AdB) + Nletters, data=dddc)
# R^2 = 0.3433
tab <- data.frame(summary(lm1)$coefficients)
tab$VIF = c(NA, car::vif(lm1))
names(tab) = c("Beta","SE","t-val","p-val","VIF")
#tab %>% kable(digits=2)
print(xtable(tab, digits=2,
             caption = "Coefficients for corpora frequencies predicting AoA.",
             table.placement = "H"), comment=F)
```


## Principal Components of Frequency

We use PCA to examine the principal components of the six word frequency distributions ([adult- vs. child-directed] x [speech, books, media]).
Table 3 shows the standard deviation (Std Dev) and proportion of variance explained, both individually (Prop Var) and cumulatively (Cum Prop Var) by the principal components (PC1-PC6).
PC1 already explains the bulk the variance (89%), and PC2-PC4 each only capture an additional 2-4% of the variance.
In total, the first four components capture >98% of the variance.

```{r}
pc_bsa <- prcomp(log(dddc[,c("ChM","ChB","ChS",
                             "AdM","AdB","AdS")])) # add Montag separately?
#eigenvalues = pc_bsa$sdev^2 # only first 2 PCs have eigenvalues >1
pc_imp <- summary(pc_bsa)$importance 
row.names(pc_imp) = c("Std Dev","Prop Var","Cum Prop Var")
pc_imp %>%
    papaja::apa_table(digits=2, caption="Importance of components from PCA.", placement='h')
```

Table 4 shows the rotation of each principal component (PC) with respect to the original six frequency distributions.
It is clear that PC1 captures overall frequency, but loads somewhat higher on adult distributions.
PC2 (3.8% of variance) differentiates adult books and TV, especially from children's speech.
PC3 (3.1% of variance) differentiates adult speech and to a small extent adult books from everything else.
PC4 (2.2% of variance) captures similarity of child and adult books.
PC5 adult books and child speech similarity?

```{r}
#pc_bsa$rotation  * -1
ddbsa <- cbind(dddc, data.frame(pc_bsa$x))
pc_bsa$rotation %>%
    papaja::apa_table(digits=2, caption="Principal component loadings.", placement='h')
```



```{r, include=F}
## Correlation of PCA with original Frequency Distributions
cor(ddbsa[,c("ChM","ChB","ChS","AdM","AdB","AdS")], ddbsa[,paste0("PC",1:6)]) %>%
  papaja::apa_table(digits=2, caption="Correlation of PCs with frequency distributions.", placement='h')
```


## PCA-based Prediction of Age of Acquisition 

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=8.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Principal components vs. age of acquisition, by lexical class."}
#knitr::include_graphics("../CHILDES_smoothed_norm_freqs_vs_books_TV.pdf")

p1 <- ggplot(ddbsa, aes(x = PC1, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC1 < -5 | PC1 > 5), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p2 <- ggplot(ddbsa, aes(x = PC2, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC2 < -2 | PC2 > 2), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p3 <- ggplot(ddbsa, aes(x = PC3, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC3 < -1 | PC3 > 1), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p4 <- ggplot(ddbsa, aes(x = PC4, y = aoa, col = lexical_class)) + 
  geom_point(alpha=.5) + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC4 < -1 | PC4 > 1), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

ggarrange(p1, p2, p3, p4, nrow=4, ncol=1, legend = "none")
```


```{r, include=F, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.2, fig.height=3.2, set.cap.width=T, num.cols.cap=1, fig.cap = "PC1 vs. AoA by lexical class."}


```

## Regression on PCA Loadings

```{r, pca-regressions, results='asis'}
ddbsa$lexical_class <- fct_relevel(ddbsa$lexical_class, "nouns")
pca_reg1 <- lm(aoa ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data=ddbsa) 
#summary(pca_reg1)
# R^2 = .321
pca_tab1 <- data.frame(pca_reg1$coefficients) 

print(xtable(pca_tab1, digits=2,
             caption = "Regression predicting AoA with PCs.",
             table.placement = "H"), comment=F)
```

Now with lexical class.

```{r, results='asis'}
pca_lc_reg <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4 + PC5) * lexical_class, data=ddbsa)
#summary(pca_lc_reg) 
# R^2 = .576
pca_tab2 <- data.frame(pca_lc_reg$coefficients) 

print(xtable(pca_tab2, digits=2,
             caption = "Regression predicting AoA with PCs and lexical class.",
             table.placement = "H"), comment=F)
```

# Analysis 3
(english) does relative proportion bookines interaction with momed





# Discussion



# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
