---
title: "English-cogsci-results"
author: "George & Georgia"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
require(tidytext)
require(GGally)
```

## Load Data

First we load in the frequency data from several corpora preprocessed in CDIorigins.Rmd.
The frequencies are already normalized to counts per million tokens.

```{r load-data}
load("data/merged_word_freqs.Rdata")

aoas <- readRDS("data/english_(american)_aoa_bydefinition.rds") %>%
  filter(measure=="produces")
```

## Word frequency table

Use to pick compelling examples.

```{r, echo=F}
ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) %>%
  DT::datatable(options=list(columnDefs = list(list(visible=FALSE, targets=c(1,3))))) %>%
  DT::formatRound(columns=c('books','tv','speech','adult_books'), digits=0) %>%
  DT::formatPercentage(columns=c('prop_booky'), digits=0)
```


## Prepare data

```{r}
# Montag corpus
dd <- ch_freq_smooth %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

# Charlesworth (FB children's books)
dd_novels <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

ddd <- dd %>% select(-prop_booky) %>%
  left_join(dd_novels %>% select(-prop_booky)) %>%
  mutate(Nletters = ifelse(is.na(Nletters), nchar(word), Nletters)) %>%
  left_join(aoas %>% select(-measure))

dddc <- ddd %>% mutate(adult_books = ifelse(is.na(adult_books), 0.07322, adult_books),
                       speech = ifelse(is.na(speech), 10, speech),
                       books = ifelse(is.na(books), 10, books),
                       tv = ifelse(is.na(tv), 10, tv)) %>%
  distinct()

summary(ddd)
```

## Correlations between word frequency distributions

```{r, fig.width=9, fig.height=9, warning=F}
dddc %>% mutate(CHILDES = log(CHILDES),
               Montag = log(Montag),
               books = log(books),
               tv = log(tv),
               adult_books = log(adult_books)) %>%
  ggpairs(columns = c("CHILDES","Montag","books","tv","adult_books"),
        ggplot2::aes(colour=lexical_class)) + 
  theme_classic()
#ggsave("CDIword_frequency_distro_comparisons.pdf", width=9, height=9)
```


## Basic AoA regression

Significant contributions of children's books, children's speech, adult book frequencies, and number of letters.

```{r}
lm1 <- lm(aoa ~ log(books) + log(speech) + log(tv) + log(adult_books) + Nletters, data=dddc)
summary(lm1) 
# R^2 = .31
```


```{r}
car::vif(lm1) %>%
  knitr::kable(digits = 3, format = "html", table.attr = "style='width:20%;'")
```

But VIFs are >>1: the word frequency distributions are highly correlated, so we will use PCA to disentangle.


## PCA 

```{r add-other-adult-freqs}
load("data/Charlesworth-unlemmatized-counts.Rdata") 

adult_dat <- adult_dat %>% filter(source!="books") %>%
  select(source, word, word_count_norm) %>%
  pivot_wider(id_col = word, names_from = source, values_from = word_count_norm) %>%
  rename(`AdS` = speech,
         `AdM` = TV)

dddc <- dddc %>% 
  rename(`AdB` = adult_books, # Google books
         `ChB` = books, # Facebooks children's books (novels..)
         `ChM` = tv,
         `ChS` = speech) %>% 
  select(-CHILDES) %>% # less well-cleaned version
  left_join(adult_dat)

dddc <- dddc %>% replace_na(list(`AdS` = 0.32,
                                 `AdM` = 0.32))
```


We will use all relevant frequency distributions and try to understand the composition of the principal components.

```{r, pca}
pc_bsa <- prcomp(log(dddc[,c("Adult Books","Child Books",
                             "Adult Speech","Child Speech",
                             "Adult TV", "Child TV")])) # add Montag separately?
summary(pc_bsa)$importance %>%
  knitr::kable(digits = 3, format = "html", table.attr = "style='width:50%;'")
```

The first component explains the bulk the variance (89%), and the first four components capture 98% of the variance.
What do these components look like?
PC1 captures overall frequency, but loads somewhat higher on adult distributions.
PC2 (3.8% of variance) differentiates adult books and TV, especially from children's speech.
PC3 (3.1% of variance) differentiates adult speech and to a small extent adult books from everything else.
PC4 (2.2% of variance) captures similarity of child and adult books.
PC5 adult books and child speech similarity?

```{r}
#pc_bsa$rotation  * -1
ddbsa <- cbind(dddc, data.frame(pc_bsa$x))
pc_bsa$rotation %>% 
  knitr::kable(digits = 2, format = "html", table.attr = "style='width:50%;'")
```


### Univariate correlations between PCs and AoA

```{r}
cor(ddbsa[,c("aoa",paste0("PC",1:6))])[2:7,"aoa"] %>% 
  kable(digits=3, col.names="AoA")
```


Can also look at delta residuals when adding PC2 to the model - see which things 

## Lasso Regression

Regularize to see which PCs survive.

```{r}
library(glmnet)

PCs <- data.matrix(ddbsa[,paste0("PC",1:5)])

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(PCs, ddbsa$aoa, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min # 0.007130127

#produce plot of test MSE by lambda value
plot(cv_model) 

#find coefficients of best model
best_model <- glmnet(PCs, ddbsa$aoa, alpha = 1, lambda = best_lambda)
coef(best_model)
```



```{r, plot-pca, fig.width=10, fig.height=10, warning=F}
outlier_thresh = 5

p1 <- ggplot(ddbsa, aes(x = PC1, y = aoa, col = lexical_class)) + 
  geom_point() + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC1 < -outlier_thresh | PC1 > outlier_thresh), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p2 <- ggplot(ddbsa, aes(x = PC2, y = aoa, col = lexical_class)) + 
  geom_point() + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC2 < -outlier_thresh | PC2 > outlier_thresh), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p3 <- ggplot(ddbsa, aes(x = PC3, y = aoa, col = lexical_class)) + 
  geom_point() + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC3 < -outlier_thresh | PC3 > outlier_thresh), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

p4 <- ggplot(ddbsa, aes(x = PC4, y = aoa, col = lexical_class)) + 
  geom_point() + geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC4 < -outlier_thresh | PC4 > outlier_thresh), aes(label = word)) + 
  facet_wrap(~lexical_class, nrow=1) + theme_classic()

ggarrange(p1, p2, p3, p4, nrow=4, ncol=1, common.legend = T)
#ggsave("AoA_vs_PCs1-4_by_lexical_class.pdf", width=8, height=8)
```


```{r, fig.width=8, fig.height=4, warning=F}
ggplot(ddbsa, aes(x = PC1, y = PC2, col = lexical_class)) + 
  geom_point() + theme_classic() +
  facet_wrap(~lexical_class) + 
  geom_text_repel(data = filter(ddbsa, PC1 < -outlier_thresh, PC1 > outlier_thresh, 
                                PC2 < -outlier_thresh, PC2 > outlier_thresh), 
                  aes(label = word), max.overlaps = 10, size =3)
```

## Regression on PCA Loadings

```{r, pca-regressions}
ddbsa$lexical_class <- fct_relevel(ddbsa$lexical_class, "nouns")
summary(lm(aoa ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data=ddbsa)) 
```

Now with lexical class.

```{r}
lexclass_mod <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4 + PC5) * lexical_class, data=ddbsa)
summary(lexclass_mod)
```


```{r}
ddbsa$hatvals <- hatvalues(lexclass_mod)

arrange(ddbsa,desc(hatvals)) %>%
  ungroup() %>%
  select(word, lexical_class, aoa, starts_with("PC"), hatvals) %>%
  slice(1:20) %>%
  knitr::kable(digits = 2, format = "html", table.attr = "style='width:80%;'")
```


# Item- and child-level data

Use mother's education to predict children's vocabulary.

```{r}
load("data/en_ws_production.Rdata")
samp <- d_demo %>% filter(!is.na(mom_ed)) # 2773
```


```{r}
d_samp <- d_en_ws %>% filter(is.element(data_id, samp$data_id)) %>%
  left_join(samp %>% select(data_id, age, production, mom_ed, ses_group)) %>% # vocab? booky_production
  left_join(ddbsa %>% select(definition, lexical_class, PC1, PC2, PC3, PC4, PC5, Nletters))

d_samp <- d_samp %>% mutate(age_sc = scale(age, center=T, scale=F), 
                            Nletters_sc = scale(Nletters),
                            mom_ed_sc = scale(as.numeric(mom_ed), center=T, scale=F))
```


```{r fit-big-models, eval=F}
m <- glmer(produces ~ age_sc + lexical_class * mom_ed_sc * (PC1 + PC2 + PC3 + PC4 + PC5) + Nletters + (1|item_id) + (1|data_id), family=binomial, data=d_samp)
# failure to converge in 10000 evaluationsconvergence code 4 from Nelder_Mead: failure to converge in 10000 evaluationsunable to evaluate scaled gradientModel failed to converge: degenerate  Hessian with 3 negative eigenvalues>
save(m, file="data/model_fits_PCA.Rdata")
```


```{r}
load("data/model_fits_PCA.Rdata")
summary(m)
```
Convergence issues...try only PC1-PC4? Or just the PCs involving child books / child-directed speech?

```{r}
m_pred <- ggpredict(m)
m_eff <- ggeffect(m)

m_age_mom_ed <- ggemmeans(m, c("age_sc", "mom_ed_sc"))
mom_ed_PC2 <- ggemmeans(m, c("mom_ed_sc","PC2"))
mom_ed_PC3 <- ggemmeans(m, c("mom_ed_sc","PC3"))
mom_ed_PC4 <- ggemmeans(m, c("mom_ed_sc","PC4"))
mom_ed_PC5 <- ggemmeans(m, c("mom_ed_sc","PC5"))

ggarrange(plot(m_age_mom_ed), 
          plot(mom_ed_PC2), 
          plot(mom_ed_PC3),
          plot(mom_ed_PC4),
          plot(mom_ed_PC5), nrow=1)
```


### OLD

Trouble fitting these due to collinearity.

produces ~ book_freq * mom_ed + speech_freq * mom_ed + (1|item) + (1|child)
produces ~ lexical_class * book_freq * mom_ed + lexical_class * speech_freq * mom_ed + (1|item) + (1|child)


```{r}
d_samp <- d_en_ws %>% filter(is.element(data_id, samp$data_id)) %>%
  left_join(samp %>% select(data_id, age, production, mom_ed, ses_group)) %>% # vocab? booky_production
  left_join(ddd %>% select(definition, lexical_class, books, tv, speech, #prop_booky, 
                           adult_books))

d_samp <- d_samp %>% mutate(age_sc = scale(age, center=T, scale=F), 
                            books_sc = scale(log(books)),
                            tv_sc = scale(log(tv)),
                            speech_sc = scale(log(speech)),
                            #prop_booky_sc = scale(log(prop_booky)),
                            mom_ed_sc = scale(as.numeric(mom_ed), center=T, scale=F))
```

ToDo: need to re-fit

```{r fit-big-models, eval=F}
m1 <- glmer(produces ~ age_sc + books_sc * mom_ed_sc + speech_sc * mom_ed_sc + (1|item_id) + (1|data_id), family=binomial, data=d_samp)

m2 <- glmer(produces ~ age_sc + lexical_class * books_sc * mom_ed_sc + lexical_class * speech_sc * mom_ed_sc + (1|item_id) + (1|data_id), family=binomial, data=d_samp)
#Model failed to converge with max|grad| = 0.0255532 (tol = 0.002, component 1)Model is nearly unidentifiable: large eigenvalue ratio
# - Rescale variables?

save(m1, m2, file="data/model_fits_scaled.Rdata")
```