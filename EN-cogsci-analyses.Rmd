---
title: "English-cogsci-results"
author: "George & Georgia"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
require(tidytext)
require(GGally)
```

## Load Data

First we load in the frequency data from several corpora preprocessed in CDIorigins.Rmd.
The frequencies are already normalized to counts per million tokens.

```{r load-data}
load("data/merged_word_freqs.Rdata")

aoas <- readRDS("data/english_(american)_aoa_bydefinition.rds") %>%
  filter(measure=="produces")
```

## Word frequency table

Use to pick compelling examples.

```{r, echo=F}
ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) %>%
  DT::datatable(options=list(columnDefs = list(list(visible=FALSE, targets=c(1,3))))) %>%
  DT::formatRound(columns=c('books','tv','speech','adult_books'), digits=0) %>%
  DT::formatPercentage(columns=c('prop_booky'), digits=0)
```


## Prepare data

```{r}
# Montag corpus
dd <- ch_freq_smooth %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

# Charlesworth (FB children's books)
dd_novels <- ch_freq_smooth_charles %>% filter(on_cdi==1) %>% 
  select(-ch_book_vs_speech) 

ddd <- dd %>% select(-prop_booky) %>%
  left_join(dd_novels %>% select(-prop_booky)) %>%
  mutate(Nletters = ifelse(is.na(Nletters), nchar(word), Nletters)) %>%
  left_join(aoas %>% select(-measure))

dddc <- ddd %>% mutate(adult_books = ifelse(is.na(adult_books), 0.07322, adult_books),
                       speech = ifelse(is.na(speech), 10, speech),
                       books = ifelse(is.na(books), 10, books),
                       tv = ifelse(is.na(tv), 10, tv))

summary(ddd)
```

## Correlations between word frequency distributions

```{r}
ddd %>% mutate(CHILDES = log(CHILDES),
               Montag = log(Montag),
               books = log(books),
               tv = log(tv),
               adult_books = log(adult_books)) %>%
  ggpairs(columns = c("CHILDES","Montag","books","tv","adult_books"),
        ggplot2::aes(colour=lexical_class)) + 
  theme_classic()
#ggsave("CDIword_frequency_distro_comparisons.pdf", width=9, height=9)
```


## Basic AoA regression

Significant contributions of children's books, children's speech, adult book frequencies, and number of letters.

```{r}
lm1 <- lm(aoa ~ log(books) + log(speech) + log(tv) + log(adult_books) + Nletters, data=dddc)
summary(lm1) 
# R^2 = .314
```


```{r}
car::vif(lm1)
```

But VIFs are >>1: the word frequency distributions are highly correlated, so we will use PCA to disentangle.


## PCA 

We will use all relevant frequency distributions and try to understand the composition of the principal components.

```{r, pca-3d}
pc_bsa <- prcomp(log(dddc[,c("books","speech","tv","adult_books")]))
summary(pc_bsa)
```


```{r}
pc_bsa$rotation 
ddbsa <- cbind(dddc, data.frame(pc_bsa$x))
```


```{r, plot-pca, fig.width=9, fig.height=5}
p1 <- ggplot(ddbsa, 
       aes(x = PC1, y = aoa, col = lexical_class)) + 
  geom_point() +
  geom_smooth(method = "lm") + 
  geom_text_repel(data = filter(ddbsa, PC1 < -5 | PC1 > 5), aes(label = word)) + 
  facet_wrap(~lexical_class) + theme_classic()

p2 <- ggplot(ddbsa, 
       aes(x = PC2, y = aoa, col = lexical_class)) + 
  geom_point() +
  geom_smooth(method = "lm") + 
  # geom_text_repel(data = filter(ddbsa, PC1 < -5 | PC1 > 5), aes(label = word)) + 
  facet_wrap(~lexical_class) + theme_classic()

ggarrange(p1, p2, nrow=1, common.legend = T)
```


```{r}
ggplot(ddbsa, 
       aes(x = PC1, y = PC2, col = lexical_class)) + 
  geom_point() + theme_classic() +
  facet_wrap(~lexical_class) + 
  geom_text_repel(data = filter(ddbsa, PC2< -4 | PC1 > 5), aes(label = word), max.overlaps = 50, size =3)
```

## Regression on PCA Loadings

```{r, pca-regressions}
ddbsa$lexical_class <- fct_relevel(ddbsa$lexical_class, "nouns")
summary(lm(aoa ~ PC1 + PC2 + PC3 + PC4, data=ddbsa)) 
```

Now with lexical class.

```{r}
lexclass_mod <- lm(aoa ~ (PC1 + PC2 + PC3 + PC4) * lexical_class, data=ddbsa)
summary(lexclass_mod)
```


```{r}
ddbsa$hatvals <- hatvalues(lexclass_mod)

arrange(ddbsa,desc(hatvals)) %>%
  ungroup() %>%
  select(word, lexical_class, aoa, starts_with("PC"), hatvals) %>%
  slice(1:20) %>%
  knitr::kable(digits = 2)
```


# Item- and child-level data

Use mother's education to predict children's vocabulary.

```{r}
load("data/en_ws_production.Rdata")
samp <- d_demo %>% filter(!is.na(mom_ed)) # 2773
```

produces ~ book_freq * mom_ed + speech_freq * mom_ed + (1|item) + (1|child)
produces ~ lexical_class * book_freq * mom_ed + lexical_class * speech_freq * mom_ed + (1|item) + (1|child)


```{r}
d_samp <- d_en_ws %>% filter(is.element(data_id, samp$data_id)) %>%
  left_join(samp %>% select(data_id, age, production, mom_ed, ses_group)) %>% # vocab? booky_production
  left_join(ddd %>% select(definition, lexical_class, books, tv, speech, #prop_booky, 
                           adult_books))

d_samp <- d_samp %>% mutate(age_sc = scale(age, center=T, scale=F), 
                            books_sc = scale(log(books)),
                            tv_sc = scale(log(tv)),
                            speech_sc = scale(log(speech)),
                            #prop_booky_sc = scale(log(prop_booky)),
                            mom_ed_sc = scale(as.numeric(mom_ed), center=T, scale=F))
```

ToDo: need to re-fit

```{r fit-big-models, eval=F}
m1 <- glmer(produces ~ age_sc + books_sc * mom_ed_sc + speech_sc * mom_ed_sc + (1|item_id) + (1|data_id), family=binomial, data=d_samp)

m2 <- glmer(produces ~ age_sc + lexical_class * books_sc * mom_ed_sc + lexical_class * speech_sc * mom_ed_sc + (1|item_id) + (1|data_id), family=binomial, data=d_samp)
#Model failed to converge with max|grad| = 0.0255532 (tol = 0.002, component 1)Model is nearly unidentifiable: large eigenvalue ratio
# - Rescale variables?

save(m1, m2, file="data/model_fits_scaled.Rdata")
```