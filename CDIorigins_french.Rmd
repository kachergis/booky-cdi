---
title: "Where do CDI words come from?"
author: "George & Georgia"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glue)
library(wordbankr)
require(tidyverse)
require(here)
require(ggrepel)
require(kableExtra)
require(tidyboot)
require(readr)
require(tokenizers)
require(lme4)
require(lmerTest)
require(ggeffects)
require(ggpubr)
library(stringi)
library(arm)
library(SnowballC)

lang <- "French (French)"

wb_data <- readRDS("/Users/loukatou/Documents/booky-cdi/data/french_(french).rds")

walk(list.files("*", pattern = "*.R$", full.names = TRUE), source)

```


```{r aoa functions, echo=F, message=F, warning=F}

normalize_language <- function(language) {
  language |> str_replace(" ", "_") |> str_to_lower()
}

lang_map <- read_csv("language_map.csv")

convert_lang_stemmer <- function(lang, method = "snowball") {
  lang_map |> filter(wordbank == lang) |> pull(!!method)
}


fit_bglm <- function(df, max_steps = 200) {
  model <- arm::bayesglm(cbind(num_true, num_false) ~ age,
                         family = "binomial",
                         prior.mean = .3,
                         prior.scale = c(.01),
                         prior.mean.for.intercept = 0,
                         prior.scale.for.intercept = 2.5,
                         prior.df = 1,
                         data = df,
                         maxit = max_steps)
  intercept <- model$coefficients[["(Intercept)"]]
  slope <- model$coefficients[["age"]]
  tibble(intercept = intercept, slope = slope, aoa = -intercept / slope)
}

fit_aoas_def <- function(wb_data, max_steps = 200, min_aoa = 0, max_aoa = 72) {
  aoas <- wb_data |>
    mutate(num_false = total - num_true) |>
    nest(data = -c(language, measure, definition)) |>
    mutate(aoas = map(data, fit_bglm)) |>
    dplyr::select(-data) |>
    unnest(aoas) |>
    filter(aoa >= min_aoa, aoa <= max_aoa)
}

wb_data_def <- wb_data %>% unnest(items) #%>% dplyr::select(-uni_lemma)

aoas_def <- fit_aoas_def(wb_data_def) %>% filter(measure=="produces")
#aoas_unilemma <- fit_aoas(wb_data)

```


## Goal

Can we identify words on the CDI that are more bookish, speechy, or from TV? (Controlling for difficulty, as more bookish words are probably more difficult)

Using this distributional source information, can we find features of children (e.g., mother's education) that relate to knowledge of subsets of words (e.g. bookish words)?

To start, we will look at American English, before extending to British English and French.
We will begin with unlemmatized corpora, and disregard part-of-speech classification.



```{r unilemma functions, echo=F, message=F, warning=F}
build_options <- function(language, word) {
  opts <- c(word)
  opts <- c(opts, word |> str_split("[,/]") |> unlist()) # "foo, bar", "foo/bar"
  opts <- c(opts, map(transforms, \(t) t(opts)))
  opts <- opts |> unlist() |> unique() |> str_trim()
  opts <- c(opts, stem(opts, language))
  return(unique(opts))
}
transforms <- list(
  \(s) str_replace_all(s, "(.*) \\(.*\\)", "\\1"), # foo (bar) -> foo
  \(s) str_replace_all(s, " ", "_"), # foo bar -> foo_bar
  \(s) str_replace_all(s, " ", "+"), # foo bar -> foo+bar
  \(s) str_replace_all(s, "(.+) \\1", "\\1") # (foo) bar -> bar
)

stem <- function(words, language) {
  #lang_snowball <- convert_lang_stemmer(language, "snowball")
  SnowballC::wordStem(words, language)
}
```

```{r load-orig-data, echo=F, message=F, warning=F}
# ToDo: remove book-reading corpora from CHILDES data!
# CHILDES corpus (hapaxes already removed, but 6295 word_count==2)

unis <- wb_data %>% unnest(items)%>% 
  dplyr::select(uni_lemma, lexical_class, category, definition, language) %>%
  distinct()
#Unilemmas
#uni_lemma_map <- read_csv("fr_uni_lemmas.csv")    |> 
uni_lemma_map <- unis |>
  mutate(option = pmap(list("french", definition),
                         build_options)) |>
  dplyr::select(language, definition, option, lexical_class, uni_lemma) |>
  unnest(option) |>
  mutate(option = tolower(option)) |>
  filter(!(definition=="baton") )|>
  filter(!(lexical_class=="other"& definition=="ciel"))|>
  filter(!(lexical_class=="other"& definition=="fleur"))|>
  filter(!(lexical_class=="other"& definition=="jardin")) |>
  filter(!(lexical_class=="other"& definition=="neige")) |>
filter(!(lexical_class=="other"& definition=="toboggan")) |>
  filter(!(lexical_class=="other"& definition=="piscine")) |>
  filter(!(lexical_class=="other"& definition=="étoile")) |>
  filter(!(lexical_class=="other"& definition=="caillou")) |>
  filter(!(lexical_class=="other"& definition=="pelle")) |>
  filter(!(lexical_class=="other"& definition=="pluie")) |>
  filter(!(lexical_class=="other"& definition=="soleil")) |>
  filter(!(lexical_class=="other"& definition=="balançoire")) |>
  filter(!(lexical_class=="other"& definition=="arbre")) |>
  distinct()
  
#CHILDES tokens arranging 
#child_speech <- readRDS(here("data/token_metrics_french_(french).rds")) %>%
child_speech <- readRDS("~/Desktop/utterances_fra.rds") %>%
  dplyr::select(gloss) %>%
  rename(text=gloss) %>%
  mutate(text = gsub('[[:punct:] ]+',' ',text))%>%
  mutate(text=tolower(text)) %>%
  mutate(text = gsub("ː","",text))%>%
  mutate(text = gsub("aujourd hui","aujourd'hui",text))%>%
  mutate(text = gsub("aujourd' hui","aujourd'hui",text))%>%
  mutate(text = gsub("là bas","là-bas",text))%>%
  mutate(text = gsub("moi même","moi-même",text))%>%
  mutate(text = gsub("s il te plait","s'il te plait",text))%>%
  mutate(text = gsub("s il te plaît","s'il te plait",text))%>%
  mutate(text = gsub("grand mère","grand-mère",text))%>%
  mutate(text = gsub("grand père","grand-père",text))%>%
  mutate(text = gsub('[[:digit:]]+','',text))%>%
  mutate(text = gsub("en bas","en_bas",text))%>%
  mutate(text = gsub("en haut","en_haut",text))%>%
  mutate(text = gsub("up peu","un_peu",text))%>%
  mutate(text = gsub("salle de bain", "salle+de+bain",text))%>%
  mutate(text = gsub("parce que","parce_que",text))%>%
  mutate(text = gsub("au revoir","au_revoir",text))%>%
  mutate(text = gsub("petit dejeuner","petit-dejeuner",text))%>%
  mutate(text = gsub("haricots verts","haricots-verts",text))%>%
  mutate(text = gsub("pas bon","pas-bon",text))%>%
  mutate(text = gsub("camion de pompier","camion-de-pompier",text))%>%
  mutate(text = gsub("appareil photo","appareil-photo",text))%>%
  mutate(text = gsub("bonne nuit","bonne-nuit",text))%>%
  mutate(text = gsub("courir après","courir-après",text))%>%
  mutate(text = gsub("à elle","à-elle",text))%>%
  mutate(text = gsub("à lui","à-lui",text))%>%
  mutate(text = gsub("à moi","à-moi",text))%>%
  mutate(text = gsub("le même","le-même",text))%>%
  mutate(text = gsub("la même","la-même",text))%>%
  mutate(text = gsub("petits pois","petits-pois",text))%>%
  mutate(text = gsub("raisins secs","raisins-secs",text))%>%
  mutate(text = gsub("tondeuse à gazon","tondeuse-à-gazon",text))%>%
  mutate(text = gsub("bonhomme de neige","bonhomme-de-neige",text))%>%
  mutate(text = gsub("doigt de pied","doigt-de-pied",text))%>%
  mutate(text = gsub("ce soir","ce-soir",text))%>%
  mutate(text = gsub("brosse à dent","brosse-à-dent",text))%>%
  mutate(text = gsub("machine à laver","machine-à-laver",text))%>%
  mutate(text = gsub("coup de fil","coup-de-fil",text))%>%
  mutate(text = gsub("pique nique","pique-nique",text))%>%
  mutate(text = gsub("porte monnaie","porte-monnaie",text))%>%
  mutate(text = gsub("au sujet de","au-sujet-de",text))%>%
  mutate(text = gsub("autour de","autour-de",text))%>%
  mutate(text = gsub("près de","près-de",text))%>%
  mutate(text = gsub("au sommet de","au-sommet-de",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  mutate(text = gsub("bac à sable","bac-à-sable",text))%>%
  mutate(text = gsub("serviette de table","serviette-de-table",text))%>%
  mutate(text = gsub("centre ville","centre-ville",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  separate_rows(text, sep = ' ') %>%
  mutate(text = gsub("petit-dejeuner","petit dejeuner",text))%>%
  mutate(text = gsub("haricots-verts","haricots verts",text))%>%
  mutate(text = gsub("pas-bon","pas bon",text))%>%
  mutate(text = gsub("camion-de-pompier","camion de pompier",text))%>%
  mutate(text = gsub("appareil-photo","appareil photo",text))%>%
  mutate(text = gsub("bonne-nuit","bonne nuit",text))%>%
  mutate(text = gsub("courir-après","courir après",text))%>%
  mutate(text = gsub("à-elle","à elle",text))%>%
  mutate(text = gsub("à-lui","à lui",text))%>%
  mutate(text = gsub("tondeuse-à-gazon","tondeuse à gazon",text))%>%
  mutate(text = gsub("à-moi","à moi",text))%>%
  mutate(text = gsub("petits-pois","petits pois",text))%>%
  mutate(text = gsub("raisins-secs","raisins secs",text))%>%
  mutate(text = gsub("la-même","la même",text))%>%
  mutate(text = gsub("le-même","le même",text))%>%
  mutate(text = gsub("bonhomme-de-neige","bonhomme de neige",text))%>%
  mutate(text = gsub("doigt-de-pied","doigt de pied",text))%>%
  mutate(text = gsub("ce-soir","ce soir",text))%>%
  mutate(text = gsub("brosse-à-dent","brosse à dent",text))%>%
  mutate(text = gsub("machine-à-laver","machine à laver",text))%>%
  mutate(text = gsub("coup-de-fil","coup de fil",text))%>%
  mutate(text = gsub("au-sujet-de","au sujet de",text))%>%
  mutate(text = gsub("autour-de","autour de",text))%>%
  mutate(text = gsub("près-de","près de",text))%>%
  mutate(text = gsub("au-sommet-de","au sommet de",text))%>%
  mutate(text = gsub("par-dessus","par dessus",text))%>%
  mutate(text = gsub("bac-à-sable","bac à sable",text))%>%
  mutate(text = gsub("serviette-de-table","serviette de table",text))%>%
  group_by(word = tolower(text)) %>%
  summarise(word_count = n(),
      sum_count = sum(word_count))%>%
  #mutate(word_count_norm=word_count * (1e6 / sum(word_count))) %>%
  #arrange(desc(word_count_norm)) %>%
  filter(!word_count==1) %>%
  filter(!word=="") %>%
  mutate(rank = 1:n()) %>%
  dplyr::select(word, word_count, rank) %>%
  rename(token=word) 

  
#cdi_items <- unique(wb_data$uni_lemma)

#Stem CHILDES tokens and matched to stemmed cdi definitions
child_speech_mapped <- child_speech |>
   dplyr::select(token) |>
   filter(!token=="")|>
    mutate(token_self = token,
           token_stemmed = stem(token, "french")) |>
    pivot_longer(c(token_self, token_stemmed), names_to = "src",
                 values_to = "option") |>
    #filter(!is.na(option), option != "") |>
    dplyr::select(-src) |>
    distinct() |>
    inner_join(uni_lemma_map) |>
      distinct() |>
   left_join(aoas_def) |>
    dplyr::select(-option)|>
    distinct()|>
    filter(!is.na(definition)) |>
   dplyr::select(token, aoa, definition, lexical_class)

 child_speech_matched <-unique(list(child_speech_mapped$token))

#Mark CHILDES tokens matched to cdi with 'on_cdi' column  

child_speech <- child_speech |>
  mutate(on_cdi = ifelse(as.character(token) %in% child_speech_mapped[[1]], 1, 0),
           source="child_speech")  |> distinct() |>
  left_join(child_speech_mapped) |>
  group_by(token) |>
  mutate(word_count=sum(word_count)) |>
  dplyr::select(-rank) |>
  distinct() |>
  filter(!(token=="le"& definition=="gentil/le"))|>
  filter(!(token=="le"& definition=="le/la même"))|>
  filter(!(token=="le"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="le"& definition=="le/la/les/quelles"))|>
  filter(!(token=="la"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="la"& definition=="le/la/les/quelles"))|>
  filter(!(token=="il"& definition=="Il"))|>
  filter(!(token=="ils"& definition=="Il"))|>
  filter(!(token=="les"& definition=="gentil/le"))|>
  filter(!(token=="les"& definition=="le/la même"))|>
  filter(!(token=="les"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="les"& definition=="le/la/les/quelles"))|>
  filter(!(token=="elle"& definition=="ils/elles"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="dans"& definition=="danser"))|>
  filter(!(token=="faire"& definition=="faire" & lexical_class=="function_words"))|>
  filter(!(token=="parce"& definition=="nouns"))|>
  filter(!(token=="es"))|>
  filter(!(token=="e"))|>
  filter(!(token=="lea"))|>
  filter(!(token=="ne"& lexical_class=="adjectives"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="jouer"& definition=="joue"))|> 
  filter(!(token=="joué"& definition=="joue"))|>
  filter(!(token=="jouait"& definition=="joue"))|>
  filter(!(token=="elles"& definition=="elle"))|> 
  filter(!(token=="grand"& definition=="grand"))|> 
  filter(!(token=="grands"& definition=="grand"))|> 
  filter(!(token=="aïe"& lexical_class=="nouns"))|>
  filter(!(token=="plein"& definition=="plein/ne"))|> 
  filter(!(token=="pleine"& definition=="plein/ne"))|> 
  filter(!(token=="pleins"& definition=="plein/ne"))|> 
  filter(!(token=="oreilles"& definition=="oreiller"))|> 
  filter(!(token=="oreille"& definition=="oreiller"))|> 
  filter(!(token=="casser"& definition=="cassé"))|>
  filter(!(token=="vers"& definition=="verser"))|> 
  filter(!(token=="colle"& definition=="collants"))|> 
  filter(!(token=="colle"& definition=="collant/e"))|> 
  filter(!(token=="coller"& definition=="collants"))|> 
  filter(!(token=="coller"& definition=="collant/e"))|> 
  filter(!(token=="collé"& definition=="collants"))|> 
  filter(!(token=="collés"& definition=="collants"))|>
  filter(!(token=="collée"& definition=="collants"))|> 
  filter(!(token=="collera"& definition=="collants"))|> 
  filter(!(token=="lune"& definition=="other"))|> 
  filter(!(token=="copains"& definition=="copain/e"))|> 
  filter(!(token=="copain"& definition=="copain/e"))|> 
  filter(!(token=="carottes"& definition=="carottes"))|> 
  filter(!(token=="grandes"& definition=="grand"))|> 
  filter(!(token=="grandi"& definition=="grand"))|>
  filter(!(token=="grandis"& definition=="grand"))|>
  filter(!(token=="grandir"& definition=="grand"))|>
  filter(!(token=="grandit"& definition=="grand"))|>
  filter(!(token=="montré"& definition=="montre"))|> 
  filter(!(token=="papi"& definition=="papier"))|> 
  filter(!(token=="danse"& definition=="dans"))|> 
  filter(!(token=="dansé"& definition=="dans"))|> 
  filter(!(token=="danses"& definition=="dans"))|>
  filter(!(token=="dansait"& definition=="dans"))|>
  filter(!(token=="dansant"& definition=="dans"))|>
  filter(!(token=="dansaient"& definition=="dans"))|>
  filter(!(token=="fou"& definition=="fou/lle"))|> 
  filter(!(token=="mignon"& definition=="mignon/e"))|> 
  filter(!(token=="mignons"& definition=="mignon/e"))|> 
  filter(!(token=="mignonne"& definition=="mignon/e"))|> 
  filter(!(token=="mignonnes"& definition=="mignon/e"))|> 
  filter(!(token=="porter"& definition=="porte"))|> 
  filter(!(token=="porté"& definition=="porte"))|> 
  filter(!(token=="portée"& definition=="porte"))|>
  filter(!(token=="porterai"& definition=="porte"))|>
  filter(!(token=="goûter"& lexical_class=="other"))|> 
  filter(!(token=="sec"& definition=="sec/he"))|>
  filter(!(token=="casses"& definition=="cassé"))|>
  filter(!(token=="cassais"& definition=="cassé"))|>
  filter(!(token=="casserais"& definition=="cassé"))|>
  filter(!(token=="montrerai"& definition=="montre"))|>
  filter(!(token=="montreras"& definition=="montre"))|>
  filter(!(token=="montrerais"& definition=="montre"))|>
  filter(!(token=="montrais"& definition=="montre"))|>
  filter(!(token=="jouera"& definition=="joue"))|>
  filter(!(token=="joueras"& definition=="joue"))|>
  filter(!(token=="jouerais"& definition=="joue"))|>
  filter(!(token=="jouerait"& definition=="joue"))|>
  filter(!(token=="jouée"& definition=="joue"))|>
  filter(!(token=="verse"& definition=="vers"))|>
  filter(!(token=="verses"& definition=="vers"))|>
  filter(!(token=="versé"& definition=="vers"))|>
  filter(!(token=="fermé"& definition=="ferme"))|>
  filter(!(token=="fermés"& definition=="ferme"))|>
  filter(!(token=="fermées"& definition=="ferme"))|>
  filter(!(token=="fermait"& definition=="ferme"))|>
  filter(!(token=="fermant"& definition=="ferme"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="aucune"& definition=="aucun/ne"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="cuisiner"& definition=="cuisine"))|>
  filter(!(token=="balayer"& definition=="balai"))|>
  filter(!(token=="balayé"& definition=="balai"))|>
  filter(!(token=="peter"& definition=="petit/e"))|>
  filter(!(token=="péter"& definition=="petit/e"))
  
  
t<- sum(child_speech$word_count)

child_speech <-child_speech |>
  mutate(word_count_norm=word_count * (1000000 /t))



#saveRDS(child_speech_mapped, "data/aoas_token_fr.rds")



```

```{r adult_speech, echo=F, message=F, warning=F}

##############################################
adult_speech <- readLines("~/Desktop/adult_speech_fr.txt")


dat <- map(adult_speech, function(x) {
  tibble(text = unlist(str_split(x, pattern = "\\n"))) 
})

adult_speech <- do.call(rbind.data.frame, dat) %>%
  mutate(text = gsub('[[:punct:] ]+',' ',text))%>%
  mutate(text=tolower(text)) %>%
  #mutate(text = gsub("l ","",text))%>%
  mutate(text = gsub("aujourd hui","aujourd'hui",text))%>%
  mutate(text = gsub("aujourd' hui","aujourd'hui",text))%>%
  mutate(text = gsub("là bas","là-bas",text))%>%
  mutate(text = gsub("moi même","moi-même",text))%>%
  mutate(text = gsub("s il te plait","s'il te plait",text))%>%
  mutate(text = gsub("s il te plaît","s'il te plait",text))%>%
  mutate(text = gsub("grand mère","grand-mère",text))%>%
  mutate(text = gsub("grand père","grand-père",text))%>%
  mutate(text = gsub('[[:digit:]]+','',text))%>%
  mutate(text = gsub("en bas","en_bas",text))%>%
  mutate(text = gsub("en haut","en_haut",text))%>%
  mutate(text = gsub("up peu","un_peu",text))%>%
  mutate(text = gsub("salle de bain", "salle+de+bain",text))%>%
  mutate(text = gsub("parce que","parce_que",text))%>%
  mutate(text = gsub("au revoir","au_revoir",text))%>%
  mutate(text = gsub("petit dejeuner","petit-dejeuner",text))%>%
  mutate(text = gsub("haricots verts","haricots-verts",text))%>%
  mutate(text = gsub("pas bon","pas-bon",text))%>%
  mutate(text = gsub("camion de pompier","camion-de-pompier",text))%>%
  mutate(text = gsub("appareil photo","appareil-photo",text))%>%
  mutate(text = gsub("bonne nuit","bonne-nuit",text))%>%
  mutate(text = gsub("courir après","courir-après",text))%>%
  mutate(text = gsub("à elle","à-elle",text))%>%
  mutate(text = gsub("à lui","à-lui",text))%>%
  mutate(text = gsub("à moi","à-moi",text))%>%
  mutate(text = gsub("le même","le-même",text))%>%
  mutate(text = gsub("la même","la-même",text))%>%
  mutate(text = gsub("petits pois","petits-pois",text))%>%
  mutate(text = gsub("raisins secs","raisins-secs",text))%>%
  mutate(text = gsub("tondeuse à gazon","tondeuse-à-gazon",text))%>%
  mutate(text = gsub("bonhomme de neige","bonhomme-de-neige",text))%>%
  mutate(text = gsub("doigt de pied","doigt-de-pied",text))%>%
  mutate(text = gsub("ce soir","ce-soir",text))%>%
  mutate(text = gsub("brosse à dent","brosse-à-dent",text))%>%
  mutate(text = gsub("machine à laver","machine-à-laver",text))%>%
  mutate(text = gsub("coup de fil","coup-de-fil",text))%>%
  mutate(text = gsub("pique nique","pique-nique",text))%>%
  mutate(text = gsub("porte monnaie","porte-monnaie",text))%>%
  mutate(text = gsub("au sujet de","au-sujet-de",text))%>%
  mutate(text = gsub("autour de","autour-de",text))%>%
  mutate(text = gsub("près de","près-de",text))%>%
  mutate(text = gsub("au sommet de","au-sommet-de",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  mutate(text = gsub("bac à sable","bac-à-sable",text))%>%
  mutate(text = gsub("serviette de table","serviette-de-table",text))%>%
  mutate(text = gsub("centre ville","centre-ville",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  separate_rows(text, sep = ' ') %>%
  mutate(text = gsub("petit-dejeuner","petit dejeuner",text))%>%
  mutate(text = gsub("haricots-verts","haricots verts",text))%>%
  mutate(text = gsub("pas-bon","pas bon",text))%>%
  mutate(text = gsub("camion-de-pompier","camion de pompier",text))%>%
  mutate(text = gsub("appareil-photo","appareil photo",text))%>%
  mutate(text = gsub("bonne-nuit","bonne nuit",text))%>%
  mutate(text = gsub("courir-après","courir après",text))%>%
  mutate(text = gsub("à-elle","à elle",text))%>%
  mutate(text = gsub("à-lui","à lui",text))%>%
  mutate(text = gsub("tondeuse-à-gazon","tondeuse à gazon",text))%>%
  mutate(text = gsub("à-moi","à moi",text))%>%
  mutate(text = gsub("petits-pois","petits pois",text))%>%
  mutate(text = gsub("raisins-secs","raisins secs",text))%>%
  mutate(text = gsub("la-même","la même",text))%>%
  mutate(text = gsub("le-même","le même",text))%>%
  mutate(text = gsub("bonhomme-de-neige","bonhomme de neige",text))%>%
  mutate(text = gsub("doigt-de-pied","doigt de pied",text))%>%
  mutate(text = gsub("ce-soir","ce soir",text))%>%
  mutate(text = gsub("brosse-à-dent","brosse à dent",text))%>%
  mutate(text = gsub("machine-à-laver","machine à laver",text))%>%
  mutate(text = gsub("coup-de-fil","coup de fil",text))%>%
  mutate(text = gsub("au-sujet-de","au sujet de",text))%>%
  mutate(text = gsub("autour-de","autour de",text))%>%
  mutate(text = gsub("près-de","près de",text))%>%
  mutate(text = gsub("au-sommet-de","au sommet de",text))%>%
  mutate(text = gsub("par-dessus","par dessus",text))%>%
  mutate(text = gsub("bac-à-sable","bac à sable",text))%>%
  mutate(text = gsub("serviette-de-table","serviette de table",text))%>%
  group_by(word = tolower(text)) %>%
  summarise(word_count = n(),
      sum_count = sum(word_count))%>%
  #mutate(word_count_norm=word_count * (1e6 / sum(word_count))) %>%
  #arrange(desc(word_count_norm)) %>%
  filter(!word_count==1) %>%
  filter(!word=="") %>%
  mutate(rank = 1:n()) %>%
  dplyr::select(word, word_count, rank) %>%
  rename(token=word) 

adult_speech_mapped <- adult_speech |>
   filter(!token=="")|>
    mutate(token_self = token,
           token_stemmed = stem(token, "french")) |>
    pivot_longer(c(token_self,  token_stemmed), names_to = "src",
                 values_to = "option") |>
    dplyr::select(-src) |>
    inner_join(uni_lemma_map) |>
    dplyr::select(-option)|>
    distinct()|>
    filter(!is.na(definition)) |>
    left_join(aoas_def) |>
   dplyr::select(token, aoa, definition, lexical_class)


adult_speech_matched <-unique(list(adult_speech_mapped$token))
 
adult_speech <- adult_speech |>
  mutate(on_cdi = ifelse(as.character(token) %in% adult_speech_mapped[[1]], 1, 0),
           source="adult_speech")  |> distinct() |>
  #left_join(lex_cat) |>
  #left_join(aoa_df) |> 
  left_join(adult_speech_mapped) |>
  group_by(token) |>
  mutate(word_count=sum(word_count)) |>
  dplyr::select(-rank) |>
  distinct() |>
  filter(!(token=="le"& definition=="gentil/le"))|>
  filter(!(token=="le"& definition=="le/la même"))|>
  filter(!(token=="le"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="le"& definition=="le/la/les/quelles"))|>
  filter(!(token=="la"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="la"& definition=="le/la/les/quelles"))|>
  filter(!(token=="il"& definition=="Il"))|>
  filter(!(token=="ils"& definition=="Il"))|>
  filter(!(token=="les"& definition=="gentil/le"))|>
  filter(!(token=="les"& definition=="le/la même"))|>
  filter(!(token=="les"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="les"& definition=="le/la/les/quelles"))|>
  filter(!(token=="elle"& definition=="ils/elles"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="dans"& definition=="danser"))|>
  filter(!(token=="faire"& definition=="faire" & lexical_class=="function_words"))|>
  filter(!(token=="parce"& definition=="nouns"))|>
  filter(!(token=="es"))|>
  filter(!(token=="e"))|>
  filter(!(token=="lea"))|>
  filter(!(token=="ne"& lexical_class=="adjectives"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="jouer"& definition=="joue"))|> 
  filter(!(token=="joué"& definition=="joue"))|>
  filter(!(token=="jouait"& definition=="joue"))|>
  filter(!(token=="elles"& definition=="elle"))|> 
  filter(!(token=="grand"& definition=="grand"))|> 
  filter(!(token=="grands"& definition=="grand"))|> 
  filter(!(token=="aïe"& lexical_class=="nouns"))|>
  filter(!(token=="plein"& definition=="plein/ne"))|> 
  filter(!(token=="pleine"& definition=="plein/ne"))|> 
  filter(!(token=="pleins"& definition=="plein/ne"))|> 
  filter(!(token=="oreilles"& definition=="oreiller"))|> 
  filter(!(token=="oreille"& definition=="oreiller"))|> 
  filter(!(token=="casser"& definition=="cassé"))|>
  filter(!(token=="vers"& definition=="verser"))|> 
  filter(!(token=="colle"& definition=="collants"))|> 
  filter(!(token=="colle"& definition=="collant/e"))|> 
  filter(!(token=="coller"& definition=="collants"))|> 
  filter(!(token=="coller"& definition=="collant/e"))|> 
  filter(!(token=="collé"& definition=="collants"))|> 
  filter(!(token=="collés"& definition=="collants"))|>
  filter(!(token=="collée"& definition=="collants"))|> 
  filter(!(token=="collera"& definition=="collants"))|> 
  filter(!(token=="lune"& definition=="other"))|> 
  filter(!(token=="copains"& definition=="copain/e"))|> 
  filter(!(token=="copain"& definition=="copain/e"))|> 
  filter(!(token=="carottes"& definition=="carottes"))|> 
  filter(!(token=="grandes"& definition=="grand"))|> 
  filter(!(token=="grandi"& definition=="grand"))|>
  filter(!(token=="grandis"& definition=="grand"))|>
  filter(!(token=="grandir"& definition=="grand"))|>
  filter(!(token=="grandit"& definition=="grand"))|>
  filter(!(token=="montré"& definition=="montre"))|> 
  filter(!(token=="papi"& definition=="papier"))|> 
  filter(!(token=="danse"& definition=="dans"))|> 
  filter(!(token=="dansé"& definition=="dans"))|> 
  filter(!(token=="danses"& definition=="dans"))|>
  filter(!(token=="dansait"& definition=="dans"))|>
  filter(!(token=="dansant"& definition=="dans"))|>
  filter(!(token=="dansaient"& definition=="dans"))|>
  filter(!(token=="fou"& definition=="fou/lle"))|> 
  filter(!(token=="mignon"& definition=="mignon/e"))|> 
  filter(!(token=="mignons"& definition=="mignon/e"))|> 
  filter(!(token=="mignonne"& definition=="mignon/e"))|> 
  filter(!(token=="mignonnes"& definition=="mignon/e"))|> 
  filter(!(token=="porter"& definition=="porte"))|> 
  filter(!(token=="porté"& definition=="porte"))|> 
  filter(!(token=="portée"& definition=="porte"))|>
  filter(!(token=="porterai"& definition=="porte"))|>
  filter(!(token=="goûter"& lexical_class=="other"))|> 
  filter(!(token=="sec"& definition=="sec/he"))|>
  filter(!(token=="casses"& definition=="cassé"))|>
  filter(!(token=="cassais"& definition=="cassé"))|>
  filter(!(token=="casserais"& definition=="cassé"))|>
  filter(!(token=="montrerai"& definition=="montre"))|>
  filter(!(token=="montreras"& definition=="montre"))|>
  filter(!(token=="montrerais"& definition=="montre"))|>
  filter(!(token=="montrais"& definition=="montre"))|>
  filter(!(token=="jouera"& definition=="joue"))|>
  filter(!(token=="joueras"& definition=="joue"))|>
  filter(!(token=="jouerais"& definition=="joue"))|>
  filter(!(token=="jouerait"& definition=="joue"))|>
  filter(!(token=="jouée"& definition=="joue"))|>
  filter(!(token=="verse"& definition=="vers"))|>
  filter(!(token=="verses"& definition=="vers"))|>
  filter(!(token=="versé"& definition=="vers"))|>
  filter(!(token=="fermé"& definition=="ferme"))|>
  filter(!(token=="fermés"& definition=="ferme"))|>
  filter(!(token=="fermées"& definition=="ferme"))|>
  filter(!(token=="fermait"& definition=="ferme"))|>
  filter(!(token=="fermant"& definition=="ferme"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="aucune"& definition=="aucun/ne"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="cuisiner"& definition=="cuisine"))|>
  filter(!(token=="balayer"& definition=="balai"))|>
  filter(!(token=="balayé"& definition=="balai"))|>
  filter(!(token=="peter"& definition=="petit/e"))|>
  filter(!(token=="péter"& definition=="petit/e"))


t<- sum(adult_speech$word_count)

adult_speech <-adult_speech |>
  mutate(word_count_norm=word_count * (1000000 /t))


 
```

```{r adult_book, echo=F, message=F, warning=F} 
 
adult_book <- readLines(here("~/Desktop/fr_adult_book.txt")) 
dat <- map(adult_book, function(x) {
  tibble(text = unlist(str_split(x, pattern = "\\n"))) 
})
adult_book <- do.call(rbind.data.frame, dat) %>%
  mutate(text = gsub('[[:punct:] ]+',' ',text))%>%
  #mutate(text = gsub("l ","",text))%>%
  mutate(text = gsub("aujourd hui","aujourd'hui",text))%>%
  mutate(text = gsub("aujourd' hui","aujourd'hui",text))%>%
  mutate(text = gsub("là bas","là-bas",text))%>%
  mutate(text = gsub("moi même","moi-même",text))%>%
  mutate(text = gsub("s il te plait","s'il te plait",text))%>%
  mutate(text = gsub("s il te plaît","s'il te plait",text))%>%
  mutate(text = gsub("grand mère","grand-mère",text))%>%
  mutate(text = gsub("grand père","grand-père",text))%>%
  mutate(text = gsub('[[:digit:]]+','',text))%>%
  mutate(text = gsub("en bas","en_bas",text))%>%
  mutate(text = gsub("en haut","en_haut",text))%>%
  mutate(text = gsub("up peu","un_peu",text))%>%
  mutate(text = gsub("salle de bain", "salle+de+bain",text))%>%
  mutate(text = gsub("parce que","parce_que",text))%>%
  mutate(text = gsub("au revoir","au_revoir",text))%>%
  mutate(text = gsub("petit dejeuner","petit-dejeuner",text))%>%
  mutate(text = gsub("haricots verts","haricots-verts",text))%>%
  mutate(text = gsub("pas bon","pas-bon",text))%>%
  mutate(text = gsub("camion de pompier","camion-de-pompier",text))%>%
  mutate(text = gsub("appareil photo","appareil-photo",text))%>%
  mutate(text = gsub("bonne nuit","bonne-nuit",text))%>%
  mutate(text = gsub("courir après","courir-après",text))%>%
  mutate(text = gsub("à elle","à-elle",text))%>%
  mutate(text = gsub("à lui","à-lui",text))%>%
  mutate(text = gsub("à moi","à-moi",text))%>%
  mutate(text = gsub("le même","le-même",text))%>%
  mutate(text = gsub("la même","la-même",text))%>%
  mutate(text = gsub("petits pois","petits-pois",text))%>%
  mutate(text = gsub("raisins secs","raisins-secs",text))%>%
  mutate(text = gsub("tondeuse à gazon","tondeuse-à-gazon",text))%>%
  mutate(text = gsub("bonhomme de neige","bonhomme-de-neige",text))%>%
  mutate(text = gsub("doigt de pied","doigt-de-pied",text))%>%
  mutate(text = gsub("ce soir","ce-soir",text))%>%
  mutate(text = gsub("brosse à dent","brosse-à-dent",text))%>%
  mutate(text = gsub("machine à laver","machine-à-laver",text))%>%
  mutate(text = gsub("coup de fil","coup-de-fil",text))%>%
  mutate(text = gsub("pique nique","pique-nique",text))%>%
  mutate(text = gsub("porte monnaie","porte-monnaie",text))%>%
  mutate(text = gsub("au sujet de","au-sujet-de",text))%>%
  mutate(text = gsub("autour de","autour-de",text))%>%
  mutate(text = gsub("près de","près-de",text))%>%
  mutate(text = gsub("au sommet de","au-sommet-de",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  mutate(text = gsub("bac à sable","bac-à-sable",text))%>%
  mutate(text = gsub("serviette de table","serviette-de-table",text))%>%
  mutate(text = gsub("centre ville","centre-ville",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  separate_rows(text, sep = ' ') %>%
  mutate(text = gsub("petit-dejeuner","petit dejeuner",text))%>%
  mutate(text = gsub("haricots-verts","haricots verts",text))%>%
  mutate(text = gsub("pas-bon","pas bon",text))%>%
  mutate(text = gsub("camion-de-pompier","camion de pompier",text))%>%
  mutate(text = gsub("appareil-photo","appareil photo",text))%>%
  mutate(text = gsub("bonne-nuit","bonne nuit",text))%>%
  mutate(text = gsub("courir-après","courir après",text))%>%
  mutate(text = gsub("à-elle","à elle",text))%>%
  mutate(text = gsub("à-lui","à lui",text))%>%
  mutate(text = gsub("tondeuse-à-gazon","tondeuse à gazon",text))%>%
  mutate(text = gsub("à-moi","à moi",text))%>%
  mutate(text = gsub("petits-pois","petits pois",text))%>%
  mutate(text = gsub("raisins-secs","raisins secs",text))%>%
  mutate(text = gsub("la-même","la même",text))%>%
  mutate(text = gsub("le-même","le même",text))%>%
  mutate(text = gsub("bonhomme-de-neige","bonhomme de neige",text))%>%
  mutate(text = gsub("doigt-de-pied","doigt de pied",text))%>%
  mutate(text = gsub("ce-soir","ce soir",text))%>%
  mutate(text = gsub("brosse-à-dent","brosse à dent",text))%>%
  mutate(text = gsub("machine-à-laver","machine à laver",text))%>%
  mutate(text = gsub("coup-de-fil","coup de fil",text))%>%
  mutate(text = gsub("au-sujet-de","au sujet de",text))%>%
  mutate(text = gsub("autour-de","autour de",text))%>%
  mutate(text = gsub("près-de","près de",text))%>%
  mutate(text = gsub("au-sommet-de","au sommet de",text))%>%
  mutate(text = gsub("par-dessus","par dessus",text))%>%
  mutate(text = gsub("bac-à-sable","bac à sable",text))%>%
  mutate(text = gsub("serviette-de-table","serviette de table",text))%>%
  group_by(word = tolower(text)) %>%
  summarise(word_count = n(),
      sum_count = sum(word_count))%>%
  #mutate(word_count_norm=word_count * (1e6 / sum(word_count))) %>%
  filter(!word_count==1) %>%
  #arrange(desc(word_count_norm)) %>%
  mutate(rank = 1:n()) %>%
  dplyr::select(word, word_count, rank) %>%
  rename(token=word) 

adult_book_mapped <- adult_book |>
   filter(!token=="")|>
    mutate(token_self = token,
           token_stemmed = stem(token, "french")) |>
    pivot_longer(c(token_self,  token_stemmed), names_to = "src",
                 values_to = "option") |>
    dplyr::select(-src) |>
    inner_join(uni_lemma_map) |>
    dplyr::select(-option)|>
    distinct()|>
    filter(!is.na(definition))|>
   left_join(aoas_def) |>
   dplyr::select(token, aoa, definition, lexical_class)


 adult_book_matched <-unique(list(adult_book_mapped$token))

  
 adult_book <- adult_book |>
  filter(!token=="")|>
  mutate(on_cdi = ifelse(as.character(token) %in% adult_book_mapped[[1]], 1, 0),
           source="adult_book")  |> distinct() |>
  #left_join(lex_cat) |>
  #left_join(aoa_df) |> 
  left_join(adult_book_mapped) |>
  group_by(token) |>
  mutate(word_count=sum(word_count)) |>
  dplyr::select(-rank) |>
  distinct() |>
   filter(!(token=="le"& definition=="gentil/le"))|>
  filter(!(token=="le"& definition=="le/la même"))|>
  filter(!(token=="le"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="le"& definition=="le/la/les/quelles"))|>
  filter(!(token=="la"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="la"& definition=="le/la/les/quelles"))|>
  filter(!(token=="il"& definition=="Il"))|>
  filter(!(token=="ils"& definition=="Il"))|>
  filter(!(token=="les"& definition=="gentil/le"))|>
  filter(!(token=="les"& definition=="le/la même"))|>
  filter(!(token=="les"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="les"& definition=="le/la/les/quelles"))|>
  filter(!(token=="elle"& definition=="ils/elles"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="dans"& definition=="danser"))|>
  filter(!(token=="faire"& definition=="faire" & lexical_class=="function_words"))|>
  filter(!(token=="parce"& definition=="nouns"))|>
  filter(!(token=="es"))|>
  filter(!(token=="e"))|>
  filter(!(token=="lea"))|>
  filter(!(token=="ne"& lexical_class=="adjectives"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="jouer"& definition=="joue"))|> 
  filter(!(token=="joué"& definition=="joue"))|>
  filter(!(token=="jouait"& definition=="joue"))|>
  filter(!(token=="elles"& definition=="elle"))|> 
  filter(!(token=="grand"& definition=="grand"))|> 
  filter(!(token=="grands"& definition=="grand"))|> 
  filter(!(token=="aïe"& lexical_class=="nouns"))|>
  filter(!(token=="plein"& definition=="plein/ne"))|> 
  filter(!(token=="pleine"& definition=="plein/ne"))|> 
  filter(!(token=="pleins"& definition=="plein/ne"))|> 
  filter(!(token=="oreilles"& definition=="oreiller"))|> 
  filter(!(token=="oreille"& definition=="oreiller"))|> 
  filter(!(token=="casser"& definition=="cassé"))|>
  filter(!(token=="vers"& definition=="verser"))|> 
  filter(!(token=="colle"& definition=="collants"))|> 
  filter(!(token=="colle"& definition=="collant/e"))|> 
  filter(!(token=="coller"& definition=="collants"))|> 
  filter(!(token=="coller"& definition=="collant/e"))|> 
  filter(!(token=="collé"& definition=="collants"))|> 
  filter(!(token=="collés"& definition=="collants"))|>
  filter(!(token=="collée"& definition=="collants"))|> 
  filter(!(token=="collera"& definition=="collants"))|> 
  filter(!(token=="lune"& definition=="other"))|> 
  filter(!(token=="copains"& definition=="copain/e"))|> 
  filter(!(token=="copain"& definition=="copain/e"))|> 
  filter(!(token=="carottes"& definition=="carottes"))|> 
  filter(!(token=="grandes"& definition=="grand"))|> 
  filter(!(token=="grandi"& definition=="grand"))|>
  filter(!(token=="grandis"& definition=="grand"))|>
  filter(!(token=="grandir"& definition=="grand"))|>
  filter(!(token=="grandit"& definition=="grand"))|>
  filter(!(token=="montré"& definition=="montre"))|> 
  filter(!(token=="papi"& definition=="papier"))|> 
  filter(!(token=="danse"& definition=="dans"))|> 
  filter(!(token=="dansé"& definition=="dans"))|> 
  filter(!(token=="danses"& definition=="dans"))|>
  filter(!(token=="dansait"& definition=="dans"))|>
  filter(!(token=="dansant"& definition=="dans"))|>
  filter(!(token=="dansaient"& definition=="dans"))|>
  filter(!(token=="fou"& definition=="fou/lle"))|> 
  filter(!(token=="mignon"& definition=="mignon/e"))|> 
  filter(!(token=="mignons"& definition=="mignon/e"))|> 
  filter(!(token=="mignonne"& definition=="mignon/e"))|> 
  filter(!(token=="mignonnes"& definition=="mignon/e"))|> 
  filter(!(token=="porter"& definition=="porte"))|> 
  filter(!(token=="porté"& definition=="porte"))|> 
  filter(!(token=="portée"& definition=="porte"))|>
  filter(!(token=="porterai"& definition=="porte"))|>
  filter(!(token=="goûter"& lexical_class=="other"))|> 
  filter(!(token=="sec"& definition=="sec/he"))|>
  filter(!(token=="casses"& definition=="cassé"))|>
  filter(!(token=="cassais"& definition=="cassé"))|>
  filter(!(token=="casserais"& definition=="cassé"))|>
  filter(!(token=="montrerai"& definition=="montre"))|>
  filter(!(token=="montreras"& definition=="montre"))|>
  filter(!(token=="montrerais"& definition=="montre"))|>
  filter(!(token=="montrais"& definition=="montre"))|>
  filter(!(token=="jouera"& definition=="joue"))|>
  filter(!(token=="joueras"& definition=="joue"))|>
  filter(!(token=="jouerais"& definition=="joue"))|>
  filter(!(token=="jouerait"& definition=="joue"))|>
  filter(!(token=="jouée"& definition=="joue"))|>
  filter(!(token=="verse"& definition=="vers"))|>
  filter(!(token=="verses"& definition=="vers"))|>
  filter(!(token=="versé"& definition=="vers"))|>
  filter(!(token=="fermé"& definition=="ferme"))|>
  filter(!(token=="fermés"& definition=="ferme"))|>
  filter(!(token=="fermées"& definition=="ferme"))|>
  filter(!(token=="fermait"& definition=="ferme"))|>
  filter(!(token=="fermant"& definition=="ferme"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="aucune"& definition=="aucun/ne"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="cuisiner"& definition=="cuisine"))|>
  filter(!(token=="balayer"& definition=="balai"))|>
  filter(!(token=="balayé"& definition=="balai"))|>
  filter(!(token=="peter"& definition=="petit/e"))|>
  filter(!(token=="péter"& definition=="petit/e"))


t<- sum(adult_book$word_count)

adult_book <-adult_book |>
  mutate(word_count_norm=word_count * (1000000 /t))

 
########################################################
 
```



```{r adult_tv, echo=F, message=F, warning=F} 
adult_tv <- readLines(here("~/Desktop/adult_media_fr_clean.txt")) 
dat <- map(adult_tv, function(x) {
  tibble(text = unlist(str_split(x, pattern = "\\n"))) 
})
adult_tv <- do.call(rbind.data.frame, dat) %>%
  mutate(text = gsub('[[:punct:] ]+',' ',text))%>%
  #mutate(text = gsub("l ","",text))%>%
  mutate(text = gsub("aujourd hui","aujourd'hui",text))%>%
  mutate(text = gsub("aujourd' hui","aujourd'hui",text))%>%
  mutate(text = gsub("là bas","là-bas",text))%>%
  mutate(text = gsub("moi même","moi-même",text))%>%
  mutate(text = gsub("s il te plait","s'il te plait",text))%>%
  mutate(text = gsub("s il te plaît","s'il te plait",text))%>%
  mutate(text = gsub("grand mère","grand-mère",text))%>%
  mutate(text = gsub("grand père","grand-père",text))%>%
  mutate(text = gsub('[[:digit:]]+','',text))%>%
  mutate(text = gsub("en bas","en_bas",text))%>%
  mutate(text = gsub("en haut","en_haut",text))%>%
  mutate(text = gsub("up peu","un_peu",text))%>%
  mutate(text = gsub("salle de bain", "salle+de+bain",text))%>%
  mutate(text = gsub("parce que","parce_que",text))%>%
  mutate(text = gsub("au revoir","au_revoir",text))%>%
  mutate(text = gsub("petit dejeuner","petit-dejeuner",text))%>%
  mutate(text = gsub("haricots verts","haricots-verts",text))%>%
  mutate(text = gsub("pas bon","pas-bon",text))%>%
  mutate(text = gsub("camion de pompier","camion-de-pompier",text))%>%
  mutate(text = gsub("appareil photo","appareil-photo",text))%>%
  mutate(text = gsub("bonne nuit","bonne-nuit",text))%>%
  mutate(text = gsub("courir après","courir-après",text))%>%
  mutate(text = gsub("à elle","à-elle",text))%>%
  mutate(text = gsub("à lui","à-lui",text))%>%
  mutate(text = gsub("à moi","à-moi",text))%>%
  mutate(text = gsub("le même","le-même",text))%>%
  mutate(text = gsub("la même","la-même",text))%>%
  mutate(text = gsub("petits pois","petits-pois",text))%>%
  mutate(text = gsub("raisins secs","raisins-secs",text))%>%
  mutate(text = gsub("tondeuse à gazon","tondeuse-à-gazon",text))%>%
  mutate(text = gsub("bonhomme de neige","bonhomme-de-neige",text))%>%
  mutate(text = gsub("doigt de pied","doigt-de-pied",text))%>%
  mutate(text = gsub("ce soir","ce-soir",text))%>%
  mutate(text = gsub("brosse à dent","brosse-à-dent",text))%>%
  mutate(text = gsub("machine à laver","machine-à-laver",text))%>%
  mutate(text = gsub("coup de fil","coup-de-fil",text))%>%
  mutate(text = gsub("pique nique","pique-nique",text))%>%
  mutate(text = gsub("porte monnaie","porte-monnaie",text))%>%
  mutate(text = gsub("au sujet de","au-sujet-de",text))%>%
  mutate(text = gsub("autour de","autour-de",text))%>%
  mutate(text = gsub("près de","près-de",text))%>%
  mutate(text = gsub("au sommet de","au-sommet-de",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  mutate(text = gsub("bac à sable","bac-à-sable",text))%>%
  mutate(text = gsub("serviette de table","serviette-de-table",text))%>%
  mutate(text = gsub("centre ville","centre-ville",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  separate_rows(text, sep = ' ') %>%
  mutate(text = gsub("petit-dejeuner","petit dejeuner",text))%>%
  mutate(text = gsub("haricots-verts","haricots verts",text))%>%
  mutate(text = gsub("pas-bon","pas bon",text))%>%
  mutate(text = gsub("camion-de-pompier","camion de pompier",text))%>%
  mutate(text = gsub("appareil-photo","appareil photo",text))%>%
  mutate(text = gsub("bonne-nuit","bonne nuit",text))%>%
  mutate(text = gsub("courir-après","courir après",text))%>%
  mutate(text = gsub("à-elle","à elle",text))%>%
  mutate(text = gsub("à-lui","à lui",text))%>%
  mutate(text = gsub("tondeuse-à-gazon","tondeuse à gazon",text))%>%
  mutate(text = gsub("à-moi","à moi",text))%>%
  mutate(text = gsub("petits-pois","petits pois",text))%>%
  mutate(text = gsub("raisins-secs","raisins secs",text))%>%
  mutate(text = gsub("la-même","la même",text))%>%
  mutate(text = gsub("le-même","le même",text))%>%
  mutate(text = gsub("bonhomme-de-neige","bonhomme de neige",text))%>%
  mutate(text = gsub("doigt-de-pied","doigt de pied",text))%>%
  mutate(text = gsub("ce-soir","ce soir",text))%>%
  mutate(text = gsub("brosse-à-dent","brosse à dent",text))%>%
  mutate(text = gsub("machine-à-laver","machine à laver",text))%>%
  mutate(text = gsub("coup-de-fil","coup de fil",text))%>%
  mutate(text = gsub("au-sujet-de","au sujet de",text))%>%
  mutate(text = gsub("autour-de","autour de",text))%>%
  mutate(text = gsub("près-de","près de",text))%>%
  mutate(text = gsub("au-sommet-de","au sommet de",text))%>%
  mutate(text = gsub("par-dessus","par dessus",text))%>%
  mutate(text = gsub("bac-à-sable","bac à sable",text))%>%
  mutate(text = gsub("serviette-de-table","serviette de table",text))%>%
  group_by(word = tolower(text)) %>%
  summarise(word_count = n(),
      sum_count = sum(word_count))%>%
  #mutate(word_count_norm=word_count * (1e6 / sum(word_count))) %>%
  filter(!word_count==1) %>%
  #arrange(desc(word_count_norm)) %>%
  mutate(rank = 1:n()) %>%
  dplyr::select(word, word_count, rank) %>%
  rename(token=word) 

adult_tv_mapped <- adult_tv |>
   filter(!token=="")|>
    mutate(token_self = token,
           token_stemmed = stem(token, "french")) |>
    pivot_longer(c(token_self,  token_stemmed), names_to = "src",
                 values_to = "option") |>
    dplyr::select(-src) |>
    inner_join(uni_lemma_map) |>
    dplyr::select(-option)|>
    distinct()|>
    filter(!is.na(definition))|>
    left_join(aoas_def) |>
   dplyr::select(token, aoa, definition, lexical_class)


 adult_tv_matched <-unique(list(adult_tv_mapped$token))
  
 adult_tv <- adult_tv |>
     filter(!token=="")|>
    mutate(on_cdi = ifelse(as.character(token) %in% adult_tv_matched[[1]], 1, 0),
           source="adult_tv") |> distinct() |>
    #left_join(lex_cat)|>
    #left_join(aoa_df) |>
    left_join(adult_tv_mapped) |>
    group_by(token) |>
    mutate(word_count=sum(word_count)) |>
    dplyr::select(-rank) |>
    distinct() |>
   filter(!(token=="le"& definition=="gentil/le"))|>
  filter(!(token=="le"& definition=="le/la même"))|>
  filter(!(token=="le"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="le"& definition=="le/la/les/quelles"))|>
  filter(!(token=="la"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="la"& definition=="le/la/les/quelles"))|>
  filter(!(token=="il"& definition=="Il"))|>
  filter(!(token=="ils"& definition=="Il"))|>
  filter(!(token=="les"& definition=="gentil/le"))|>
  filter(!(token=="les"& definition=="le/la même"))|>
  filter(!(token=="les"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="les"& definition=="le/la/les/quelles"))|>
  filter(!(token=="elle"& definition=="ils/elles"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="dans"& definition=="danser"))|>
  filter(!(token=="faire"& definition=="faire" & lexical_class=="function_words"))|>
  filter(!(token=="parce"& definition=="nouns"))|>
  filter(!(token=="es"))|>
  filter(!(token=="e"))|>
  filter(!(token=="lea"))|>
  filter(!(token=="ne"& lexical_class=="adjectives"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="jouer"& definition=="joue"))|> 
  filter(!(token=="joué"& definition=="joue"))|>
  filter(!(token=="jouait"& definition=="joue"))|>
  filter(!(token=="elles"& definition=="elle"))|> 
  filter(!(token=="grand"& definition=="grand"))|> 
  filter(!(token=="grands"& definition=="grand"))|> 
  filter(!(token=="aïe"& lexical_class=="nouns"))|>
  filter(!(token=="plein"& definition=="plein/ne"))|> 
  filter(!(token=="pleine"& definition=="plein/ne"))|> 
  filter(!(token=="pleins"& definition=="plein/ne"))|> 
  filter(!(token=="oreilles"& definition=="oreiller"))|> 
  filter(!(token=="oreille"& definition=="oreiller"))|> 
  filter(!(token=="casser"& definition=="cassé"))|>
  filter(!(token=="vers"& definition=="verser"))|> 
  filter(!(token=="colle"& definition=="collants"))|> 
  filter(!(token=="colle"& definition=="collant/e"))|> 
  filter(!(token=="coller"& definition=="collants"))|> 
  filter(!(token=="coller"& definition=="collant/e"))|> 
  filter(!(token=="collé"& definition=="collants"))|> 
  filter(!(token=="collés"& definition=="collants"))|>
  filter(!(token=="collée"& definition=="collants"))|> 
  filter(!(token=="collera"& definition=="collants"))|> 
  filter(!(token=="lune"& definition=="other"))|> 
  filter(!(token=="copains"& definition=="copain/e"))|> 
  filter(!(token=="copain"& definition=="copain/e"))|> 
  filter(!(token=="carottes"& definition=="carottes"))|> 
  filter(!(token=="grandes"& definition=="grand"))|> 
  filter(!(token=="grandi"& definition=="grand"))|>
  filter(!(token=="grandis"& definition=="grand"))|>
  filter(!(token=="grandir"& definition=="grand"))|>
  filter(!(token=="grandit"& definition=="grand"))|>
  filter(!(token=="montré"& definition=="montre"))|> 
  filter(!(token=="papi"& definition=="papier"))|> 
  filter(!(token=="danse"& definition=="dans"))|> 
  filter(!(token=="dansé"& definition=="dans"))|> 
  filter(!(token=="danses"& definition=="dans"))|>
  filter(!(token=="dansait"& definition=="dans"))|>
  filter(!(token=="dansant"& definition=="dans"))|>
  filter(!(token=="dansaient"& definition=="dans"))|>
  filter(!(token=="fou"& definition=="fou/lle"))|> 
  filter(!(token=="mignon"& definition=="mignon/e"))|> 
  filter(!(token=="mignons"& definition=="mignon/e"))|> 
  filter(!(token=="mignonne"& definition=="mignon/e"))|> 
  filter(!(token=="mignonnes"& definition=="mignon/e"))|> 
  filter(!(token=="porter"& definition=="porte"))|> 
  filter(!(token=="porté"& definition=="porte"))|> 
  filter(!(token=="portée"& definition=="porte"))|>
  filter(!(token=="porterai"& definition=="porte"))|>
  filter(!(token=="goûter"& lexical_class=="other"))|> 
  filter(!(token=="sec"& definition=="sec/he"))|>
  filter(!(token=="casses"& definition=="cassé"))|>
  filter(!(token=="cassais"& definition=="cassé"))|>
  filter(!(token=="casserais"& definition=="cassé"))|>
  filter(!(token=="montrerai"& definition=="montre"))|>
  filter(!(token=="montreras"& definition=="montre"))|>
  filter(!(token=="montrerais"& definition=="montre"))|>
  filter(!(token=="montrais"& definition=="montre"))|>
  filter(!(token=="jouera"& definition=="joue"))|>
  filter(!(token=="joueras"& definition=="joue"))|>
  filter(!(token=="jouerais"& definition=="joue"))|>
  filter(!(token=="jouerait"& definition=="joue"))|>
  filter(!(token=="jouée"& definition=="joue"))|>
  filter(!(token=="verse"& definition=="vers"))|>
  filter(!(token=="verses"& definition=="vers"))|>
  filter(!(token=="versé"& definition=="vers"))|>
  filter(!(token=="fermé"& definition=="ferme"))|>
  filter(!(token=="fermés"& definition=="ferme"))|>
  filter(!(token=="fermées"& definition=="ferme"))|>
  filter(!(token=="fermait"& definition=="ferme"))|>
  filter(!(token=="fermant"& definition=="ferme"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="aucune"& definition=="aucun/ne"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="cuisiner"& definition=="cuisine"))|>
  filter(!(token=="balayer"& definition=="balai"))|>
  filter(!(token=="balayé"& definition=="balai"))|>
  filter(!(token=="peter"& definition=="petit/e"))|>
  filter(!(token=="péter"& definition=="petit/e"))

   # filter(!is.na(definition)) |>
   # filter(!is.na(aoa)) 

t<- sum(adult_tv$word_count)

adult_tv<-adult_tv |>
  mutate(word_count_norm=word_count * (1000000 /t))
  
```

```{r child_book, echo=F, message=F, warning=F}

child_books <- readLines(here("~/Desktop/child_book.txt"))
                        
dat <- map(child_books, function(x) {
  tibble(text = unlist(str_split(x, pattern = "\\n"))) 
})
child_book <- do.call(rbind.data.frame, dat) %>%
  mutate(text = gsub('[[:punct:] ]+',' ',text))%>%
  #mutate(text = gsub("l ","",text))%>%
  mutate(text = gsub("aujourd hui","aujourd'hui",text))%>%
  mutate(text = gsub("aujourd' hui","aujourd'hui",text))%>%
  mutate(text = gsub("là bas","là-bas",text))%>%
  mutate(text = gsub("moi même","moi-même",text))%>%
  mutate(text = gsub("s il te plait","s'il te plait",text))%>%
  mutate(text = gsub("s il te plaît","s'il te plait",text))%>%
  mutate(text = gsub("grand mère","grand-mère",text))%>%
  mutate(text = gsub("grand père","grand-père",text))%>%
  mutate(text = gsub('[[:digit:]]+','',text))%>%
  mutate(text = gsub("en bas","en_bas",text))%>%
  mutate(text = gsub("en haut","en_haut",text))%>%
  mutate(text = gsub("up peu","un_peu",text))%>%
  mutate(text = gsub("salle de bain", "salle+de+bain",text))%>%
  mutate(text = gsub("parce que","parce_que",text))%>%
  mutate(text = gsub("au revoir","au_revoir",text))%>%
  mutate(text = gsub("petit dejeuner","petit-dejeuner",text))%>%
  mutate(text = gsub("haricots verts","haricots-verts",text))%>%
  mutate(text = gsub("pas bon","pas-bon",text))%>%
  mutate(text = gsub("camion de pompier","camion-de-pompier",text))%>%
  mutate(text = gsub("appareil photo","appareil-photo",text))%>%
  mutate(text = gsub("bonne nuit","bonne-nuit",text))%>%
  mutate(text = gsub("courir après","courir-après",text))%>%
  mutate(text = gsub("à elle","à-elle",text))%>%
  mutate(text = gsub("à lui","à-lui",text))%>%
  mutate(text = gsub("à moi","à-moi",text))%>%
  mutate(text = gsub("le même","le-même",text))%>%
  mutate(text = gsub("la même","la-même",text))%>%
  mutate(text = gsub("petits pois","petits-pois",text))%>%
  mutate(text = gsub("raisins secs","raisins-secs",text))%>%
  mutate(text = gsub("tondeuse à gazon","tondeuse-à-gazon",text))%>%
  mutate(text = gsub("bonhomme de neige","bonhomme-de-neige",text))%>%
  mutate(text = gsub("doigt de pied","doigt-de-pied",text))%>%
  mutate(text = gsub("ce soir","ce-soir",text))%>%
  mutate(text = gsub("brosse à dent","brosse-à-dent",text))%>%
  mutate(text = gsub("machine à laver","machine-à-laver",text))%>%
  mutate(text = gsub("coup de fil","coup-de-fil",text))%>%
  mutate(text = gsub("pique nique","pique-nique",text))%>%
  mutate(text = gsub("porte monnaie","porte-monnaie",text))%>%
  mutate(text = gsub("au sujet de","au-sujet-de",text))%>%
  mutate(text = gsub("autour de","autour-de",text))%>%
  mutate(text = gsub("près de","près-de",text))%>%
  mutate(text = gsub("au sommet de","au-sommet-de",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  mutate(text = gsub("bac à sable","bac-à-sable",text))%>%
  mutate(text = gsub("serviette de table","serviette-de-table",text))%>%
  mutate(text = gsub("centre ville","centre-ville",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  separate_rows(text, sep = ' ') %>%
  mutate(text = gsub("petit-dejeuner","petit dejeuner",text))%>%
  mutate(text = gsub("haricots-verts","haricots verts",text))%>%
  mutate(text = gsub("pas-bon","pas bon",text))%>%
  mutate(text = gsub("camion-de-pompier","camion de pompier",text))%>%
  mutate(text = gsub("appareil-photo","appareil photo",text))%>%
  mutate(text = gsub("bonne-nuit","bonne nuit",text))%>%
  mutate(text = gsub("courir-après","courir après",text))%>%
  mutate(text = gsub("à-elle","à elle",text))%>%
  mutate(text = gsub("à-lui","à lui",text))%>%
  mutate(text = gsub("tondeuse-à-gazon","tondeuse à gazon",text))%>%
  mutate(text = gsub("à-moi","à moi",text))%>%
  mutate(text = gsub("petits-pois","petits pois",text))%>%
  mutate(text = gsub("raisins-secs","raisins secs",text))%>%
  mutate(text = gsub("la-même","la même",text))%>%
  mutate(text = gsub("le-même","le même",text))%>%
  mutate(text = gsub("bonhomme-de-neige","bonhomme de neige",text))%>%
  mutate(text = gsub("doigt-de-pied","doigt de pied",text))%>%
  mutate(text = gsub("ce-soir","ce soir",text))%>%
  mutate(text = gsub("brosse-à-dent","brosse à dent",text))%>%
  mutate(text = gsub("machine-à-laver","machine à laver",text))%>%
  mutate(text = gsub("coup-de-fil","coup de fil",text))%>%
  mutate(text = gsub("au-sujet-de","au sujet de",text))%>%
  mutate(text = gsub("autour-de","autour de",text))%>%
  mutate(text = gsub("près-de","près de",text))%>%
  mutate(text = gsub("au-sommet-de","au sommet de",text))%>%
  mutate(text = gsub("par-dessus","par dessus",text))%>%
  mutate(text = gsub("bac-à-sable","bac à sable",text))%>%
  mutate(text = gsub("serviette-de-table","serviette de table",text))%>%
  group_by(word = tolower(text)) %>%
  summarise(word_count = n(),
      sum_count = sum(word_count))%>%
  #mutate(word_count_norm=word_count * (1e6 / sum(word_count))) %>%
   filter(!word_count==1) %>%
  #arrange(desc(word_count_norm)) %>%
  mutate(rank = 1:n()) %>%
  dplyr::select(word, word_count, rank) %>%
  rename(token=word)                        
                      
child_book_mapped <- child_book |>
   filter(!token=="")|>
    mutate(token_self = token,
           token_stemmed = stem(token, "french")) |>
    pivot_longer(c(token_self,  token_stemmed), names_to = "src",
                 values_to = "option") |>
    dplyr::select(-src) |>
    left_join(uni_lemma_map) |>
    dplyr::select(-option)|>
    distinct()|>
    filter(!is.na(definition))|>
    left_join(aoas_def) |>
    dplyr::select(token, aoa, definition, lexical_class)

child_book_matched <-unique(list(child_book_mapped$token))
  
child_book <- child_book |>
    filter(!token=="")|>
    mutate(on_cdi = ifelse(as.character(token) %in% child_book_matched[[1]], 1, 0),
           source="child_book") |> distinct() |>
    #left_join(lex_cat)|>
    #left_join(aoa_df) |>
    left_join(child_book_mapped) |>
    group_by(token) |>
    mutate(word_count=sum(word_count)) |>
    dplyr::select(-rank) |>
    distinct() |>
  filter(!(token=="le"& definition=="gentil/le"))|>
  filter(!(token=="le"& definition=="le/la même"))|>
  filter(!(token=="le"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="le"& definition=="le/la/les/quelles"))|>
  filter(!(token=="la"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="la"& definition=="le/la/les/quelles"))|>
  filter(!(token=="il"& definition=="Il"))|>
  filter(!(token=="ils"& definition=="Il"))|>
  filter(!(token=="les"& definition=="gentil/le"))|>
  filter(!(token=="les"& definition=="le/la même"))|>
  filter(!(token=="les"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="les"& definition=="le/la/les/quelles"))|>
  filter(!(token=="elle"& definition=="ils/elles"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="dans"& definition=="danser"))|>
  filter(!(token=="faire"& definition=="faire" & lexical_class=="function_words"))|>
  filter(!(token=="parce"& definition=="nouns"))|>
  filter(!(token=="es"))|>
  filter(!(token=="e"))|>
  filter(!(token=="lea"))|>
  filter(!(token=="ne"& lexical_class=="adjectives"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="jouer"& definition=="joue"))|> 
  filter(!(token=="joué"& definition=="joue"))|>
  filter(!(token=="jouait"& definition=="joue"))|>
  filter(!(token=="elles"& definition=="elle"))|> 
  filter(!(token=="grand"& definition=="grand"))|> 
  filter(!(token=="grands"& definition=="grand"))|> 
  filter(!(token=="aïe"& lexical_class=="nouns"))|>
  filter(!(token=="plein"& definition=="plein/ne"))|> 
  filter(!(token=="pleine"& definition=="plein/ne"))|> 
  filter(!(token=="pleins"& definition=="plein/ne"))|> 
  filter(!(token=="oreilles"& definition=="oreiller"))|> 
  filter(!(token=="oreille"& definition=="oreiller"))|> 
  filter(!(token=="casser"& definition=="cassé"))|>
  filter(!(token=="vers"& definition=="verser"))|> 
  filter(!(token=="colle"& definition=="collants"))|> 
  filter(!(token=="colle"& definition=="collant/e"))|> 
  filter(!(token=="coller"& definition=="collants"))|> 
  filter(!(token=="coller"& definition=="collant/e"))|> 
  filter(!(token=="collé"& definition=="collants"))|> 
  filter(!(token=="collés"& definition=="collants"))|>
  filter(!(token=="collée"& definition=="collants"))|> 
  filter(!(token=="collera"& definition=="collants"))|> 
  filter(!(token=="lune"& definition=="other"))|> 
  filter(!(token=="copains"& definition=="copain/e"))|> 
  filter(!(token=="copain"& definition=="copain/e"))|> 
  filter(!(token=="carottes"& definition=="carottes"))|> 
  filter(!(token=="grandes"& definition=="grand"))|> 
  filter(!(token=="grandi"& definition=="grand"))|>
  filter(!(token=="grandis"& definition=="grand"))|>
  filter(!(token=="grandir"& definition=="grand"))|>
  filter(!(token=="grandit"& definition=="grand"))|>
  filter(!(token=="montré"& definition=="montre"))|> 
  filter(!(token=="papi"& definition=="papier"))|> 
  filter(!(token=="danse"& definition=="dans"))|> 
  filter(!(token=="dansé"& definition=="dans"))|> 
  filter(!(token=="danses"& definition=="dans"))|>
  filter(!(token=="dansait"& definition=="dans"))|>
  filter(!(token=="dansant"& definition=="dans"))|>
  filter(!(token=="dansaient"& definition=="dans"))|>
  filter(!(token=="fou"& definition=="fou/lle"))|> 
  filter(!(token=="mignon"& definition=="mignon/e"))|> 
  filter(!(token=="mignons"& definition=="mignon/e"))|> 
  filter(!(token=="mignonne"& definition=="mignon/e"))|> 
  filter(!(token=="mignonnes"& definition=="mignon/e"))|> 
  filter(!(token=="porter"& definition=="porte"))|> 
  filter(!(token=="porté"& definition=="porte"))|> 
  filter(!(token=="portée"& definition=="porte"))|>
  filter(!(token=="porterai"& definition=="porte"))|>
  filter(!(token=="goûter"& lexical_class=="other"))|> 
  filter(!(token=="sec"& definition=="sec/he"))|>
  filter(!(token=="casses"& definition=="cassé"))|>
  filter(!(token=="cassais"& definition=="cassé"))|>
  filter(!(token=="casserais"& definition=="cassé"))|>
  filter(!(token=="montrerai"& definition=="montre"))|>
  filter(!(token=="montreras"& definition=="montre"))|>
  filter(!(token=="montrerais"& definition=="montre"))|>
  filter(!(token=="montrais"& definition=="montre"))|>
  filter(!(token=="jouera"& definition=="joue"))|>
  filter(!(token=="joueras"& definition=="joue"))|>
  filter(!(token=="jouerais"& definition=="joue"))|>
  filter(!(token=="jouerait"& definition=="joue"))|>
  filter(!(token=="jouée"& definition=="joue"))|>
  filter(!(token=="verse"& definition=="vers"))|>
  filter(!(token=="verses"& definition=="vers"))|>
  filter(!(token=="versé"& definition=="vers"))|>
  filter(!(token=="fermé"& definition=="ferme"))|>
  filter(!(token=="fermés"& definition=="ferme"))|>
  filter(!(token=="fermées"& definition=="ferme"))|>
  filter(!(token=="fermait"& definition=="ferme"))|>
  filter(!(token=="fermant"& definition=="ferme"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="aucune"& definition=="aucun/ne"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="cuisiner"& definition=="cuisine"))|>
  filter(!(token=="balayer"& definition=="balai"))|>
  filter(!(token=="balayé"& definition=="balai"))|>
  filter(!(token=="peter"& definition=="petit/e"))|>
  filter(!(token=="péter"& definition=="petit/e"))

    #filter(!is.na(definition)) |>
    #filter(!is.na(aoa)) 

t<- sum(child_book$word_count)

child_book <-child_book |>
  mutate(word_count_norm=word_count * (1000000 /t))

```

```{r child_tv, echo=F, message=F, warning=F}

child_tv <- readLines(here("~/Desktop/child_media_fr_copy_clean1.txt"))
                        
dat <- map(child_tv, function(x) {
  tibble(text = unlist(str_split(x, pattern = "\\n"))) 
})
child_tv <- do.call(rbind.data.frame, dat) %>%
  mutate(text = gsub('[[:punct:] ]+',' ',text))%>%
  #mutate(text = gsub('!','',text))%>%
  mutate(text = gsub("aujourd hui","aujourd'hui",text))%>%
  #mutate(text = gsub("l ","",text))%>%
  mutate(text = gsub("aujourd' hui","aujourd'hui",text))%>%
  mutate(text = gsub("là bas","là-bas",text))%>%
  mutate(text = gsub("moi même","moi-même",text))%>%
  mutate(text = gsub("s il te plait","s'il te plait",text))%>%
  mutate(text = gsub("s il te plaît","s'il te plait",text))%>%
  mutate(text = gsub("grand mère","grand-mère",text))%>%
  mutate(text = gsub("grand père","grand-père",text))%>%
  mutate(text = gsub('[[:digit:]]+','',text))%>%
  mutate(text = gsub("en bas","en_bas",text))%>%
  mutate(text = gsub("en haut","en_haut",text))%>%
  mutate(text = gsub("up peu","un_peu",text))%>%
  mutate(text = gsub("salle de bain", "salle+de+bain",text))%>%
  mutate(text = gsub("parce que","parce_que",text))%>%
  mutate(text = gsub("au revoir","au_revoir",text))%>%
  mutate(text = gsub("petit dejeuner","petit-dejeuner",text))%>%
  mutate(text = gsub("haricots verts","haricots-verts",text))%>%
  mutate(text = gsub("pas bon","pas-bon",text))%>%
  mutate(text = gsub("camion de pompier","camion-de-pompier",text))%>%
  mutate(text = gsub("appareil photo","appareil-photo",text))%>%
  mutate(text = gsub("bonne nuit","bonne-nuit",text))%>%
  mutate(text = gsub("courir après","courir-après",text))%>%
  mutate(text = gsub("à elle","à-elle",text))%>%
  mutate(text = gsub("à lui","à-lui",text))%>%
  mutate(text = gsub("à moi","à-moi",text))%>%
  mutate(text = gsub("le même","le-même",text))%>%
  mutate(text = gsub("la même","la-même",text))%>%
  mutate(text = gsub("petits pois","petits-pois",text))%>%
  mutate(text = gsub("raisins secs","raisins-secs",text))%>%
  mutate(text = gsub("tondeuse à gazon","tondeuse-à-gazon",text))%>%
  mutate(text = gsub("bonhomme de neige","bonhomme-de-neige",text))%>%
  mutate(text = gsub("doigt de pied","doigt-de-pied",text))%>%
  mutate(text = gsub("ce soir","ce-soir",text))%>%
  mutate(text = gsub("brosse à dent","brosse-à-dent",text))%>%
  mutate(text = gsub("machine à laver","machine-à-laver",text))%>%
  mutate(text = gsub("coup de fil","coup-de-fil",text))%>%
  mutate(text = gsub("pique nique","pique-nique",text))%>%
  mutate(text = gsub("porte monnaie","porte-monnaie",text))%>%
  mutate(text = gsub("au sujet de","au-sujet-de",text))%>%
  mutate(text = gsub("autour de","autour-de",text))%>%
  mutate(text = gsub("près de","près-de",text))%>%
  mutate(text = gsub("au sommet de","au-sommet-de",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  mutate(text = gsub("bac à sable","bac-à-sable",text))%>%
  mutate(text = gsub("serviette de table","serviette-de-table",text))%>%
  mutate(text = gsub("centre ville","centre-ville",text))%>%
  mutate(text = gsub("par dessus","par-dessus",text))%>%
  separate_rows(text, sep = ' ') %>%
  mutate(text = gsub("petit-dejeuner","petit dejeuner",text))%>%
  mutate(text = gsub("haricots-verts","haricots verts",text))%>%
  mutate(text = gsub("pas-bon","pas bon",text))%>%
  mutate(text = gsub("camion-de-pompier","camion de pompier",text))%>%
  mutate(text = gsub("appareil-photo","appareil photo",text))%>%
  mutate(text = gsub("bonne-nuit","bonne nuit",text))%>%
  mutate(text = gsub("courir-après","courir après",text))%>%
  mutate(text = gsub("à-elle","à elle",text))%>%
  mutate(text = gsub("à-lui","à lui",text))%>%
  mutate(text = gsub("tondeuse-à-gazon","tondeuse à gazon",text))%>%
  mutate(text = gsub("à-moi","à moi",text))%>%
  mutate(text = gsub("petits-pois","petits pois",text))%>%
  mutate(text = gsub("raisins-secs","raisins secs",text))%>%
  mutate(text = gsub("la-même","la même",text))%>%
  mutate(text = gsub("le-même","le même",text))%>%
  mutate(text = gsub("bonhomme-de-neige","bonhomme de neige",text))%>%
  mutate(text = gsub("doigt-de-pied","doigt de pied",text))%>%
  mutate(text = gsub("ce-soir","ce soir",text))%>%
  mutate(text = gsub("brosse-à-dent","brosse à dent",text))%>%
  mutate(text = gsub("machine-à-laver","machine à laver",text))%>%
  mutate(text = gsub("coup-de-fil","coup de fil",text))%>%
  mutate(text = gsub("au-sujet-de","au sujet de",text))%>%
  mutate(text = gsub("autour-de","autour de",text))%>%
  mutate(text = gsub("près-de","près de",text))%>%
  mutate(text = gsub("au-sommet-de","au sommet de",text))%>%
  mutate(text = gsub("par-dessus","par dessus",text))%>%
  mutate(text = gsub("bac-à-sable","bac à sable",text))%>%
  mutate(text = gsub("serviette-de-table","serviette de table",text))%>%
  group_by(word = tolower(text)) %>%
  summarise(word_count = n(),
      sum_count = sum(word_count))%>%
  mutate(word_count_norm=word_count * (1e6 / sum(word_count))) %>%
   filter(!word_count==1) %>%
  arrange(desc(word_count_norm)) %>%
  mutate(rank = 1:n()) %>%
  dplyr::select(word, word_count, word_count_norm, rank) %>%
  rename(token=word)                        
                      
child_tv_mapped <- child_tv |>
   filter(!token=="")|>
    mutate(token_self = token,
           token_stemmed = stem(token, "french")) |>
    pivot_longer(c(token_self,  token_stemmed), names_to = "src",
                 values_to = "option") |>
    dplyr::select(-src) |>
    left_join(uni_lemma_map) |>
    dplyr::select(-option)|>
    distinct()|>
    filter(!is.na(definition))|>
    left_join(aoas_def) |>
   dplyr::select(token, aoa, definition, lexical_class)

child_tv_matched <-unique(list(child_tv_mapped$token))
  
child_tv <- child_tv |>
    filter(!token=="")|>
    mutate(on_cdi = ifelse(as.character(token) %in% child_tv_matched[[1]], 1, 0),
           source="child_tv")|> distinct() |>
    #left_join(lex_cat)|>
    #left_join(aoa_df) |>
    left_join(child_tv_mapped) |>
    group_by(token) |>
    mutate(word_count=sum(word_count)) |>
    dplyr::select(-rank) |>
    distinct() |>
  filter(!(token=="le"& definition=="gentil/le"))|>
  filter(!(token=="le"& definition=="le/la même"))|>
  filter(!(token=="le"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="le"& definition=="le/la/les/quelles"))|>
  filter(!(token=="la"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="la"& definition=="le/la/les/quelles"))|>
  filter(!(token=="il"& definition=="Il"))|>
  filter(!(token=="ils"& definition=="Il"))|>
  filter(!(token=="les"& definition=="gentil/le"))|>
  filter(!(token=="les"& definition=="le/la même"))|>
  filter(!(token=="les"& definition=="le/la/les/quel(les)"))|>
  filter(!(token=="les"& definition=="le/la/les/quelles"))|>
  filter(!(token=="elle"& definition=="ils/elles"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="dans"& definition=="danser"))|>
  filter(!(token=="faire"& definition=="faire" & lexical_class=="function_words"))|>
  filter(!(token=="parce"& definition=="nouns"))|>
  filter(!(token=="es"))|>
  filter(!(token=="e"))|>
  filter(!(token=="lea"))|>
  filter(!(token=="ne"& lexical_class=="adjectives"))|>
  filter(!(token=="ce"& definition=="doux/ce"))|>
  filter(!(token=="jouer"& definition=="joue"))|> 
  filter(!(token=="joué"& definition=="joue"))|>
  filter(!(token=="jouait"& definition=="joue"))|>
  filter(!(token=="elles"& definition=="elle"))|> 
  filter(!(token=="grand"& definition=="grand"))|> 
  filter(!(token=="grands"& definition=="grand"))|> 
  filter(!(token=="aïe"& lexical_class=="nouns"))|>
  filter(!(token=="plein"& definition=="plein/ne"))|> 
  filter(!(token=="pleine"& definition=="plein/ne"))|> 
  filter(!(token=="pleins"& definition=="plein/ne"))|> 
  filter(!(token=="oreilles"& definition=="oreiller"))|> 
  filter(!(token=="oreille"& definition=="oreiller"))|> 
  filter(!(token=="casser"& definition=="cassé"))|>
  filter(!(token=="vers"& definition=="verser"))|> 
  filter(!(token=="colle"& definition=="collants"))|> 
  filter(!(token=="colle"& definition=="collant/e"))|> 
  filter(!(token=="coller"& definition=="collants"))|> 
  filter(!(token=="coller"& definition=="collant/e"))|> 
  filter(!(token=="collé"& definition=="collants"))|> 
  filter(!(token=="collés"& definition=="collants"))|>
  filter(!(token=="collée"& definition=="collants"))|> 
  filter(!(token=="collera"& definition=="collants"))|> 
  filter(!(token=="lune"& definition=="other"))|> 
  filter(!(token=="copains"& definition=="copain/e"))|> 
  filter(!(token=="copain"& definition=="copain/e"))|> 
  filter(!(token=="carottes"& definition=="carottes"))|> 
  filter(!(token=="grandes"& definition=="grand"))|> 
  filter(!(token=="grandi"& definition=="grand"))|>
  filter(!(token=="grandis"& definition=="grand"))|>
  filter(!(token=="grandir"& definition=="grand"))|>
  filter(!(token=="grandit"& definition=="grand"))|>
  filter(!(token=="montré"& definition=="montre"))|> 
  filter(!(token=="papi"& definition=="papier"))|> 
  filter(!(token=="danse"& definition=="dans"))|> 
  filter(!(token=="dansé"& definition=="dans"))|> 
  filter(!(token=="danses"& definition=="dans"))|>
  filter(!(token=="dansait"& definition=="dans"))|>
  filter(!(token=="dansant"& definition=="dans"))|>
  filter(!(token=="dansaient"& definition=="dans"))|>
  filter(!(token=="fou"& definition=="fou/lle"))|> 
  filter(!(token=="mignon"& definition=="mignon/e"))|> 
  filter(!(token=="mignons"& definition=="mignon/e"))|> 
  filter(!(token=="mignonne"& definition=="mignon/e"))|> 
  filter(!(token=="mignonnes"& definition=="mignon/e"))|> 
  filter(!(token=="porter"& definition=="porte"))|> 
  filter(!(token=="porté"& definition=="porte"))|> 
  filter(!(token=="portée"& definition=="porte"))|>
  filter(!(token=="porterai"& definition=="porte"))|>
  filter(!(token=="goûter"& lexical_class=="other"))|> 
  filter(!(token=="sec"& definition=="sec/he"))|>
  filter(!(token=="casses"& definition=="cassé"))|>
  filter(!(token=="cassais"& definition=="cassé"))|>
  filter(!(token=="casserais"& definition=="cassé"))|>
  filter(!(token=="montrerai"& definition=="montre"))|>
  filter(!(token=="montreras"& definition=="montre"))|>
  filter(!(token=="montrerais"& definition=="montre"))|>
  filter(!(token=="montrais"& definition=="montre"))|>
  filter(!(token=="jouera"& definition=="joue"))|>
  filter(!(token=="joueras"& definition=="joue"))|>
  filter(!(token=="jouerais"& definition=="joue"))|>
  filter(!(token=="jouerait"& definition=="joue"))|>
  filter(!(token=="jouée"& definition=="joue"))|>
  filter(!(token=="verse"& definition=="vers"))|>
  filter(!(token=="verses"& definition=="vers"))|>
  filter(!(token=="versé"& definition=="vers"))|>
  filter(!(token=="fermé"& definition=="ferme"))|>
  filter(!(token=="fermés"& definition=="ferme"))|>
  filter(!(token=="fermées"& definition=="ferme"))|>
  filter(!(token=="fermait"& definition=="ferme"))|>
  filter(!(token=="fermant"& definition=="ferme"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="aucune"& definition=="aucun/ne"))|>
  filter(!(token=="aucun"& definition=="aucun/ne"))|>
  filter(!(token=="cuisiner"& definition=="cuisine"))|>
  filter(!(token=="balayer"& definition=="balai"))|>
  filter(!(token=="balayé"& definition=="balai"))|>
  filter(!(token=="peter"& definition=="petit/e"))|>
  filter(!(token=="péter"& definition=="petit/e"))

    #filter(!is.na(definition)) |>
    #filter(!is.na(aoa)) 

t<- sum(child_tv$word_count)

child_tv <-child_tv |>
  mutate(word_count_norm=word_count * (1000000 /t))


coverage<- do.call(c, list(unique(child_speech_mapped$definition), child_book_mapped$definition, adult_speech_mapped$definition, adult_book_mapped$definition))

```
Our data sources:

- Adult speech: `r nrow(adult_speech)` types, `r sum(adult_speech$word_count)` tokens from the corpus collection TCOF (`r sum(adult_speech$on_cdi)` CDI words)
- Children speech: `r nrow(child_speech)` types, `r sum(child_speech$word_count)` tokens from the CHILDES corpus (`r sum(child_speech$on_cdi)` CDI words)
- Adult books: `r nrow(adult_book)` types, `r sum(adult_book$word_count)` tokens from the Gutemberg books corpus "le roi du Klondike"
- Children's books: `r nrow(child_book)` types, `r sum(child_book$word_count)` tokens from the children stories online corpus (`r sum(child_book$on_cdi)` CDI words)


## Normalize Word Frequencies

We will calculate keyness scores for each word, i.e. the ratio of normalized frequency in focus corpus to normalized frequency in a reference corpus.
For now, we will use the subset of words that are found in all four corpora (N=4229), but we may consider following Dawson et al. (2021) and adding a constant (e.g., 10) to all normalized frequencies in every corpus in order to not eliminate the bulk of words that do not appear in the smaller corpora (which are the child-directed corpora).
<!-- "Given the problem of calculating ratios for words occurring in the focus corpus, but not at all in the reference corpus, we added a constant of 10 to all normalised frequencies before calculating keyness. We selected this value as the constant because it focuses the keyword analysis on the lower end of the frequency spectrum (Kilgarriff, 2009), which we considered to be important when identifying the words that children were unlikely to encounter in everyday conversation, but which they would experience through regular exposure to book language." --> 


### Child-directed Speech vs. Books

```{r keyness-childes-vs-montag, echo=F}
# CHILDES and Stories books
ch_freq_long <- rbind(child_speech, child_book, child_tv, adult_speech, adult_book, adult_tv)  %>%
  dplyr::select(token, source, definition, on_cdi, word_count_norm, word_count, lexical_class, aoa)  

# wide - no smoothing, only keep subset of words in all corpora
ch_freq <- ch_freq_long %>% dplyr::select(-word_count_norm) %>%
  pivot_wider(names_from = source, values_from = word_count) %>%
  #filter(!is.na(`child_book`), !is.na(child_speech), !is.na(child_tv)) %>%
  mutate(ch_book_vs_speech = `child_book` / child_speech,
         prop_booky = `child_book` / (`child_book` + child_speech)) 

ch_freq_smooth <- ch_freq_long %>% dplyr::select(-word_count) %>%
  mutate(word_count_norm_smooth = word_count_norm + 10) %>%
  dplyr::select(-word_count_norm) %>%
  pivot_wider(names_from = source, values_from = word_count_norm_smooth) %>% 
  #filter(!is.na(`child_book`), !is.na(child_speech), !is.na(child_tv)) %>%
  mutate(ch_book_vs_speech = `child_book` / child_speech,
         prop_booky = `child_book` / (`child_book` + child_speech))

# Laplace smoothing, alpha=10, keep all words in any corpus
#ch_freq_smooth <- ch_freq_long%>% 
#  mutate(word_count_norm_smooth = word_count_norm + 10) %>%
#  dplyr::select(-word_count_norm) %>%
#  pivot_wider(names_from = source, values_from = word_count_norm_smooth) %>% 
#  #filter(!is.na(`child_book`), !is.na(child_speech), !is.na(child_tv)) %>%
#  mutate(ch_book_vs_speech = `child_book` / child_speech,
#         prop_booky = `child_book` / (`child_book` + child_speech))
```


### Most Child Booky Words

```{r}
#ch_freq %>% arrange(desc(ch_book_vs_speech)) %>% head(10) %>% kable(digits=2)
#ch_freq_smooth %>% arrange(desc(ch_book_vs_speech)) %>% head(10) %>% kable(digits=2)

#p1 <- ch_freq %>% mutate(Word = ifelse(on_cdi==1, "CDI", "Non-CDI")) %>%
#  ggplot(aes(x=child_speech, y=`child_book`, color=Word)) + 
#  geom_point(alpha=.3) + theme_classic() + geom_smooth(method='lm') +
#  scale_x_log10() + scale_y_log10() + ggtitle("Raw Frequencies (intersection)")

#p2 <- ch_freq_smooth %>% mutate(Word = ifelse(on_cdi==1, "CDI", "Non-CDI")) %>%
#  ggplot(aes(x=child_speech, y=`child_book`, color=Word)) + 
#  geom_point(alpha=.3) + theme_classic() + geom_smooth(method='lm') +
#  scale_x_log10() + scale_y_log10() + ggtitle("Smoothed Frequencies (union)")

#ggpubr::ggarrange(p1, p2, nrow=1, common.legend = T)
```


```{r}
#cor(subset(ch_freq, on_cdi==1)$child_speech, subset(ch_freq, on_cdi==1)$`child_book`) # .62
#cor(subset(ch_freq, on_cdi==0)$child_speech, subset(ch_freq, on_cdi==0)$`child_book`) # .44
```

### Which CDI words are more booky/speechy?

```{r, fig.width=10, fig.height=5}
#p1 <- ch_freq_smooth %>% filter(on_cdi==1) %>%
#  ggplot(aes(x=child_speech, y=`child_book`)) + 
#  geom_point(alpha=.3) + theme_classic() + 
#  scale_x_log10() + scale_y_log10() +
#  geom_text_repel(aes(label=token)) +
#  geom_abline(intercept = 0, slope = 1, linetype="dashed") +
#  ggtitle("CHILDES vs. Stories corpus")
#p1
```



```{r}
library(spacyr)
library(udpipe)

ud_model <- udpipe_download_model(language = "french")
ud_model <- udpipe_load_model(ud_model$file_model)

#child_speech_mapped <- child_speech_mapped %>% filter(measure=="produces")

#mappings <- child_book_mapped %>% 
#  dplyr::select(token, uni_lemma) %>% 
#  rbind(child_speech_mapped %>% dplyr::select(token, uni_lemma)) %>% 
#  distinct() %>% 
#  full_join(uni_lemmas) %>%
#  dplyr::select(token, lexical_class, uni_lemma) %>% 
#  distinct()

ch_freq_ <- ch_freq
x_<-((ch_freq_ %>% filter(is.na(lexical_class))))

x__ <- udpipe_annotate(ud_model, x = x_$token, doc_id = x_$token)
x <- as.data.frame(x__) %>%
  mutate(lexical_class1=ifelse(upos=="VERB", "verbs", upos),
         lexical_class1=ifelse(upos=="NOUN", "nouns", lexical_class1),
         lexical_class1=ifelse(upos=="AUX", "verbs", lexical_class1),
         lexical_class1=ifelse(upos=="ADJ", "adjectives", lexical_class1),
         lexical_class1=ifelse(upos=="ADP", "function_words", lexical_class1),
         lexical_class1=ifelse(upos=="DET", "function_words", lexical_class1),
         lexical_class1=ifelse(upos=="CCONJ", "function_words", lexical_class1),
          lexical_class1=ifelse(upos=="INTJ", "function_words", lexical_class1),
          lexical_class1=ifelse(upos=="ADV", "adverbs", lexical_class1),
          lexical_class1=ifelse(upos=="NUM", "function_words", lexical_class1),
          lexical_class1=ifelse(upos=="PROPN", "nouns", lexical_class1),
          lexical_class1=ifelse(upos=="PART", "function_words", lexical_class1),
          lexical_class1=ifelse(upos=="CCONJ", "function_words", lexical_class1),
          lexical_class1=ifelse(upos=="PRON", "function_words", lexical_class1),
          lexical_class1=ifelse(upos=="SCONJ", "function_words", lexical_class1)) %>%
        dplyr::select(doc_id, lexical_class1) %>%
        filter(!is.na(lexical_class1)) %>%
        rename("token"="doc_id")

x1 <- x[!duplicated(x$token),]

ch_freq_fr <- ch_freq_ %>% left_join(x1) %>% 
  mutate(lexical_class=ifelse(is.na(lexical_class), lexical_class1, lexical_class )) %>%
  dplyr::select(-lexical_class1) %>%
  distinct() 

ch_freq_smooth_fr<- ch_freq_smooth %>% left_join(x1) %>% 
  mutate(lexical_class=ifelse(is.na(lexical_class), lexical_class1, lexical_class )) %>%
  dplyr::select(-lexical_class1) %>%
  distinct() 

# holds even if we filter out many low frequency words
#ch_key <- ch_freq_new %>% filter(child_speech>1, `child_book`>1) %>%
#  group_by(on_cdi, lexical_class) %>%
#  tidyboot_mean(ch_book_vs_speech)

#ch_key_smooth <- ch_freq_smooth_new %>% 
#  group_by(on_cdi, lexical_class) %>%
#  tidyboot_mean(ch_book_vs_speech)

#p1 <- ch_key %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean, color=lexical_class)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper), position = position_jitter(w=0.1)) +
#  theme_classic() + ylab("Children's Book Keyness") + xlab("")

#p2 <- ch_key_smooth %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean, color=lexical_class)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper), position = position_jitter(w=0.1)) +
#  theme_classic() + ylab("Children's Book Keyness") + xlab("")

#ggpubr::ggarrange(p1, p2, nrow=1, common.legend = T)

#ch_freq_smooth1<- ch_freq_smooth_new |>
#  group_by(token, prop_booky, on_cdi, child_speech, child_book,ch_book_vs_speech) |>
#    summarise(
#      lexical_category = list(lexical_class)) |>
#  distinct()



save(ch_freq_fr, ch_freq_smooth_fr, file="~/Desktop/word_freqs_fr_complete3.Rdata")


```

```{r}
#ch_booky <- ch_freq_new %>% group_by(on_cdi, lexical_class) %>%
#  tidyboot_mean(prop_booky)

#ch_booky_smooth <- ch_freq_smooth_new %>% group_by(on_cdi, lexical_class) %>%
#  tidyboot_mean(prop_booky)

#p1 <- ch_booky %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean, color=lexical_class)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper), position = position_jitter(w=0.1)) +
#  theme_classic() + ylab("Proportion of Book Occurrences") + xlab("")

#p2 <- ch_booky_smooth %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean, color=lexical_class)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper), position = position_jitter(w=0.1)) +
#  theme_classic() + ylab("Proportion of Book Occurrences") + xlab("")

#ggpubr::ggarrange(p1, p2, nrow=1, common.legend = T)
```

## Predicting AoA

Which corpus best predicts words' AoA?
Which corpora contribute unique variance to predicting AoA?
(Does the book corpus interact with SES, since we expect high-SES parents read more to their children?)

```{r}
#dd <- ch_freq_smooth %>% filter(on_cdi==1) %>% 
#  dplyr::select(-ch_book_vs_speech) %>% left_join(mappings) %>% left_join(aoas_unilemma)
  
#dd_produces <- dd %>% filter(measure=="produces")
#summary(lm(aoa ~ prop_booky, data=dd_produces)) # r^2 = .05; more booky = harder (significant)

#dd_understands <- dd %>% filter(measure=="understands")
#summary(lm(aoa ~ prop_booky, data=dd_understands)) # r^2 = .04; more booky = harder (significant)

# with the 2 sources, only books are significant
#summary(lm(aoa ~ child_book + child_speech , data=dd_produces)) 
```


## Predict Booky Vocabulary per child by Mother Education

(also try ses_group)

```{r}
# let's take the 200 bookiest CDI items and look at relation of subscore to mom_ed
#booky_items <- dd %>% arrange(desc(prop_booky)) %>% head(200)

```


## Also try same regressions controlling for Google books frequency (reference corpus) 


### Comparing to Adult Speech

For our reference corpus, we will use adult speech (movie subtitles), as this is the target language distribution that children will eventually learn.
(Although we could use child-directed speech, as that is what is given to children, or Google book frequency, which could be considered the epitome of an 'educated' distribution.)

```{r keyness-subtlex, echo=F, eval=F}
#all_freq_long <- rbind(child_speech, adult_speech, child_books, adult_books) %>%
#  dplyr::select(word, source, on_cdi, word_count_norm) #%>%
#  filter(!is.element(word, c("d","m","s","t", "xxx"))) 

# wide - no smoothing, only keep subset of words in all corpora
#all_freq <- all_freq_long %>%
#  pivot_wider(names_from = source, values_from = word_count_norm) %>%
#  filter(!is.na(`Children's Books`), !is.na(CHILDES), !is.na(SUBTLEX), !is.na(`Adult Books`)) #%>%
#  mutate(child_vs_adult_speech = CHILDES / SUBTLEX, 
#         child_book_vs_adult_speech = `Children's Books` / SUBTLEX,
#         adult_books_vs_speech = `Adult Books` / SUBTLEX)
         #ch_book_ad_book_ratio = `Children's Books` / `Adult Books`,
         #ch_book_ad_book_ratio = `Children's Books` / CHILDES) # are children's books like adult books, or like children's speech?

# Laplace smoothing, alpha=1, keep all 123060 words in any corpus
#all_freq_smooth <- all_freq_long %>% 
#  mutate(word_count_norm_smooth = word_count_norm + 1) %>%
#  pivot_wider(id_cols = c(word, on_cdi), names_from = source, values_from = word_count_norm_smooth, values_fill=1) %>% 
#  filter(!is.na(`Children's Books`), !is.na(CHILDES), !is.na(SUBTLEX), !is.na(`Adult Books`)) #%>%
#  mutate(child_vs_adult_speech = CHILDES / SUBTLEX, 
#         child_book_vs_adult_speech = `Children's Books` / SUBTLEX,
#         adult_books_vs_speech = `Adult Books` / SUBTLEX)
```

Non-SUBTLEX adult speech (Charlesworth corpus):

```{r keyness-charlesworth, echo=F, eval=F}
#all_freq_long <- rbind(child_speech, adult_speech2, child_books, adult_books) %>%
#  select(word, source, on_cdi, word_count_norm) %>%
#  filter(!is.element(word, c("d","m","s","t", "xxx", "child's own name"))) 

# wide - no smoothing, only keep subset of 3977 words in all corpora
#all_freq <- all_freq_long %>%
#  pivot_wider(names_from = source, values_from = word_count_norm) %>%
#  filter(!is.na(`Children's Books`), !is.na(CHILDES), !is.na(speech), !is.na(`Adult Books`)) #%>%
#  mutate(child_vs_adult_speech = CHILDES / speech, 
#         child_book_vs_adult_speech = `Children's Books` / speech,
#         adult_books_vs_speech = `Adult Books` / speech)

# Laplace smoothing, alpha=10, keep all 115597 words in any corpus
#all_freq_smooth <- all_freq_long %>% 
#  mutate(word_count_norm_smooth = word_count_norm + 10) %>%
#  pivot_wider(id_cols = c(word, on_cdi), names_from = source, values_from = #word_count_norm_smooth, values_fill=10) %>% 
#  filter(!is.na(`Children's Books`), !is.na(CHILDES), !is.na(speech), !is.na(`Adult Books`)) #%>%
#  mutate(child_vs_adult_speech = CHILDES / speech, 
#         child_book_vs_adult_speech = `Children's Books` / speech,
#         adult_books_vs_speech = `Adult Books` / speech)
```

## Child-directed speech vs. Adult speech

We first examine the keyness of words in child-directed speech vs. that in adult speech.
Here are the 10 words most over-represented in child-directed speech compared to adult movies:

```{r, echo=F, eval=F}
# top 10
#all_freq %>% arrange(desc(child_vs_adult_speech)) %>% head(10) %>% kable(digits=2)
#all_freq_smooth %>% arrange(desc(child_vs_adult_speech)) %>% head(10) %>% kable(digits=2)
```


Here are the 10 words most under-represented in CHILDES, compared to adult movies:

```{r, echo=F, eval=F}
#all_freq %>% arrange(desc(child_vs_adult_speech)) %>% tail(10) %>% kable(digits=2)
#all_freq_smooth %>% arrange(desc(child_vs_adult_speech)) %>% tail(10) %>% kable(digits=2)
```


Below we show the average keyness of child-directed speech and children's books for CDI vs. non-CDI words, using adult speech as the reference corpus.
This is based on the words common to all corpora.

```{r, keyness-cdi-confint, echo=F, eval=F, fig.width=8, fig.height=3.5}
#ch_book <- all_freq %>% group_by(on_cdi) %>%
#  tidyboot_mean(child_book_vs_adult_speech)

#ch_speech <- all_freq %>% group_by(on_cdi) %>%
#  tidyboot_mean(child_vs_adult_speech)

#ad_speech <- all_freq %>% group_by(on_cdi) %>%
#  tidyboot_mean(adult_books_vs_speech)

#p1 <- ch_book %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
#  theme_classic() + ylab("Children's Books Keyness") #+ ylim(0,20)

#p2 <- ch_speech %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
#  theme_classic() + ylab("Child-directed Speech Keyness") #+ ylim(0,6)

#p3 <- ad_speech %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
#  theme_classic() + ylab("Adult Books Keyness") + ylim(0,5)

#ggpubr::ggarrange(p1, p2, p3, nrow=1)
```

Now we do the same for the union of all words across the corpora (Laplace-smoothed).

```{r, keyness-cdi-confint-smooth, echo=F, eval=F, fig.width=8, fig.height=3.5}
#ch_book <- all_freq_smooth %>% group_by(on_cdi) %>%
#  tidyboot_mean(child_book_vs_adult_speech)

#ch_speech <- all_freq_smooth %>% group_by(on_cdi) %>%
#  tidyboot_mean(child_vs_adult_speech)

#ad_speech <- all_freq_smooth %>% group_by(on_cdi) %>%
#  tidyboot_mean(adult_books_vs_speech)

#p1 <- ch_book %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
#  theme_classic() + ylab("Children's Books Keyness") #+ ylim(0,20)

#p2 <- ch_speech %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
#  theme_classic() + ylab("Child-directed Speech Keyness") #+ ylim(0,6)

#p3 <- ad_speech %>% mutate(CDI = ifelse(on_cdi==1, "CDI Words", "non-CDI Words")) %>%
#  ggplot(aes(x=CDI, y=mean)) + 
#  geom_pointrange(aes(ymin=ci_lower, ymax=ci_upper)) +
#  theme_classic() + ylab("Adult Books Keyness") #+ ylim(0,5)

#ggpubr::ggarrange(p1, p2, p3, nrow=1)
```


## Other Ideas

- Does the mean IRT difficulty of CDI/non-CDI words systematically vary with the source?
- Use adult books as reference corpus for keyness metric (instad of subtitles)
